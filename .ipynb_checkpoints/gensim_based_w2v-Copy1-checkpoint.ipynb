{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T16:47:00.336806Z",
     "start_time": "2021-11-14T16:46:57.820961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing word2vec_inner...\n",
      "CYTHON IMPORTED SUCCESSFULLY!! (Normal)\n",
      "cbound : False\n",
      "tbound : False\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "#import wiki_old as w # old wiki\n",
    "import wiki as w # changed wiki to include '[]'\n",
    " \n",
    "#from gensim.models import word2vec # for orignal w2v\n",
    "from localgensim.gensim2.models import word2vec #remmember to change flags in word2vec.py  161-162\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models.fasttext import FastText\n",
    "#from gensim.models.word2vec import Word2Vec # not in use\n",
    "#from localgensim.gensim2.models.word2vec import Word2Vec # not in use\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#WIKIXML = '/mnt/nfs/resdata0/manni/wiki/papwiki-20180720-pages-articles-multistream.xml.bz2'\n",
    "#WIKIXML = '/mnt/nfs/resdata0/manni/wiki/enwiki-20180920-pages-articles-multistream.xml.bz2'\n",
    "#WIKIXML = '/mnt/nfs/resdata0/manni/wiki/enwiki-20200601-pages-articles-multistream1.xml-p1p30303.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T16:47:00.345294Z",
     "start_time": "2021-11-14T16:47:00.340048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manni/ner-s2s/word_embedding/wiki.py\n",
      "/home/manni/ner-s2s/word_embedding/localgensim/gensim2/models/word2vec.py\n"
     ]
    }
   ],
   "source": [
    "print(w.__file__)\n",
    "print(word2vec.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T16:47:00.415247Z",
     "start_time": "2021-11-14T16:47:00.347433Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T16:47:00.465047Z",
     "start_time": "2021-11-14T16:47:00.417936Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO)\n",
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:25:05.774307Z",
     "start_time": "2020-10-25T13:25:05.608721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Training model %s', 'word2vec')\n",
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=10, size=300, sample=0, \n",
    "                 workers=1, iter=1, min_count=1)\n",
    "\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:48:45.834994Z",
     "start_time": "2020-10-25T13:47:14.780675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conll corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"conll_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:52:12.732111Z",
     "start_time": "2020-10-25T13:48:45.838043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=1, iter=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_spx2g.txt'\n",
    "emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# skip if sentence made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T16:26:15.962934Z",
     "start_time": "2021-11-11T23:53:14.340627Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# loc = 'num'|'lr'|'ent'\n",
    "# pos = True|False\n",
    "# download latest wiki dump\n",
    "#w.download_wiki_dump('en', WIKIXML)\n",
    "\n",
    "# parse wiki dump\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',lower=True) # Orignal\n",
    "wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='DEP',lower=True,pos=False,loc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T16:26:17.695239Z",
     "start_time": "2021-11-12T16:26:15.966492Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sv.save(wiki_sentences,\"wiki_sentences_pos_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_pos\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_loc\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences\") # orignal\n",
    "sv.save(wiki_sentences,\"wiki_sentences_dep2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phrase mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:50:46.815932Z",
     "start_time": "2021-05-29T18:50:46.810808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T22:24:31.858045Z",
     "start_time": "2021-05-29T18:51:21.445049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences, min_count=100, threshold=1)\n",
    "frozen_phrases = phrases.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:41:54.697897Z",
     "start_time": "2021-05-30T11:41:38.608475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(phrases,\"gensim_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:06:13.397797Z",
     "start_time": "2020-03-16T22:06:13.394080Z"
    }
   },
   "source": [
    "# Train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-14T16:47:10.355082Z",
     "start_time": "2021-11-14T16:47:08.760292Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentences = sv.load(\"wiki_sentences_no\")\n",
    "#temp_sens are cased!!\n",
    "#sentences = sv.load(\"temp_sens\")\n",
    " \n",
    "#sentences = sv.load(\"wiki_sentences\") #Normal sentences using wiki_old.py\n",
    "\n",
    "#Wiki_Sentences_SP are cased\n",
    "#sentences = sv.load(\"Wiki_Sentences_SP\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_loc\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_sp\") #New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_pos\") # not to be used\n",
    "#sentences = sv.load(\"Wiki_sentences_pos_sample\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent\") # New\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent_sample\") # New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_dep\") #New\n",
    "sentences = sv.load(\"wiki_sentences_dep2\") #New\n",
    "\n",
    "#wiki english sample Cased \n",
    "#sentences = sv.load(\"Wiki_sentences_sp_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:49:32.564070Z",
     "start_time": "2021-11-26T12:49:32.558782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length of token: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-26T12:50:05.678Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-26 12:50:01,630] Training model spxM100DEPw1\n",
      "[2021-11-26 12:50:01,634] collecting all words and their counts\n",
      "[2021-11-26 12:50:15,115] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "[2021-11-26 13:00:22,847] PROGRESS: at sentence #10000, processed 36927724 words, keeping 2260161 word types\n",
      "[2021-11-26 13:10:01,351] PROGRESS: at sentence #20000, processed 69938672 words, keeping 3833001 word types\n",
      "[2021-11-26 13:16:45,284] PROGRESS: at sentence #30000, processed 95962065 words, keeping 4866422 word types\n",
      "[2021-11-26 13:22:56,991] PROGRESS: at sentence #40000, processed 119159777 words, keeping 5826941 word types\n",
      "[2021-11-26 13:27:17,557] PROGRESS: at sentence #50000, processed 135767226 words, keeping 6479807 word types\n",
      "[2021-11-26 13:29:17,969] PROGRESS: at sentence #60000, processed 146008964 words, keeping 6686450 word types\n",
      "[2021-11-26 13:30:59,819] PROGRESS: at sentence #70000, processed 154396156 words, keeping 6837255 word types\n",
      "[2021-11-26 13:32:40,487] PROGRESS: at sentence #80000, processed 162682567 words, keeping 6989166 word types\n",
      "[2021-11-26 13:37:16,471] PROGRESS: at sentence #90000, processed 180557614 words, keeping 7612327 word types\n",
      "[2021-11-26 13:43:45,071] PROGRESS: at sentence #100000, processed 202780345 words, keeping 8604172 word types\n",
      "[2021-11-26 13:48:51,143] PROGRESS: at sentence #110000, processed 222171268 words, keeping 9366450 word types\n",
      "[2021-11-26 13:53:39,209] PROGRESS: at sentence #120000, processed 240304760 words, keeping 10030781 word types\n",
      "[2021-11-26 13:58:03,886] PROGRESS: at sentence #130000, processed 257640604 words, keeping 10596749 word types\n",
      "[2021-11-26 14:03:05,960] PROGRESS: at sentence #140000, processed 275896660 words, keeping 11220854 word types\n",
      "[2021-11-26 14:07:49,510] PROGRESS: at sentence #150000, processed 292349973 words, keeping 11820580 word types\n",
      "[2021-11-26 14:12:34,308] PROGRESS: at sentence #160000, processed 308910345 words, keeping 12398709 word types\n",
      "[2021-11-26 14:16:42,756] PROGRESS: at sentence #170000, processed 324441627 words, keeping 12923498 word types\n",
      "[2021-11-26 14:20:48,860] PROGRESS: at sentence #180000, processed 339364336 words, keeping 13417400 word types\n",
      "[2021-11-26 14:24:33,040] PROGRESS: at sentence #190000, processed 353160267 words, keeping 13874095 word types\n",
      "[2021-11-26 14:28:40,021] PROGRESS: at sentence #200000, processed 366654110 words, keeping 14342411 word types\n",
      "[2021-11-26 14:32:24,422] PROGRESS: at sentence #210000, processed 379795738 words, keeping 14777021 word types\n",
      "[2021-11-26 14:36:05,404] PROGRESS: at sentence #220000, processed 393188074 words, keeping 15178120 word types\n",
      "[2021-11-26 14:39:49,648] PROGRESS: at sentence #230000, processed 406113501 words, keeping 15598900 word types\n",
      "[2021-11-26 14:43:34,388] PROGRESS: at sentence #240000, processed 419080638 words, keeping 16013979 word types\n",
      "[2021-11-26 14:47:11,002] PROGRESS: at sentence #250000, processed 431843236 words, keeping 16418379 word types\n"
     ]
    }
   ],
   "source": [
    "logging.info('Training model %s', 'spxM100DEPw1')\n",
    "model = word2vec.Word2Vec(sentences, cbound=False, tbound=False, bound_type='lr', window=1, sg=1, hs=0, negative=5, size=300, sample=1e-3, workers=40, iter=5, min_count=100)\n",
    "#model = word2vec.Word2Vec(sentences, window=1, sg=1, hs=0, negative=5, size=300, sample=1e-3, workers=40, iter=5, min_count=100)\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-26T12:50:14.971Z"
    }
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_filtered_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc1_epoch5_300_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2R_mc1_epoch5_300_filtered.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2w2v_mc1_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_con1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2S_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2B_mc100_epoch5_300_sub3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2TB_mc100_epoch5_300_LR.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2POS_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2DEP_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LRM3_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LOC_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_loc.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_pos.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_ent_w10.txt'\n",
    "emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_dep2_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_ent_static_w3.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-26T12:50:17.311Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-26T12:50:17.580Z"
    }
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-26T12:50:18.163Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab.pop('[', None)\n",
    "vocab.pop(']', None)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-26T12:50:19.026Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab, position=0):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = dict()\n",
    "for word,obj in model.wv.vocab.items():\n",
    "    word_ = word.split('#')\n",
    "    if len(word_) < 2:\n",
    "        vocab_[word] = obj\n",
    "        continue \n",
    "    if int(word_[-1]) > 5:\n",
    "        continue\n",
    "    vocab_[word]=obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-15T16:04:50.575Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-15T16:05:20.903Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T05:27:29.780749Z",
     "start_time": "2021-06-15T05:27:29.698465Z"
    }
   },
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:47.172484Z",
     "start_time": "2021-02-25T06:15:45.664705Z"
    }
   },
   "outputs": [],
   "source": [
    "#Wiki_Phrase_Vocab are cased phrases => length 2\n",
    "phrase_vocab = sv.load(\"Wiki_Phrase_Vocab\")\n",
    "#wiki_phrase_vocab are lower cased phrases => length 2\n",
    "#phrase_vocab = sv.load(\"wiki_phrase_vocab\")\n",
    "#bi_phrase are only bi-grams\n",
    "#phrase_vocab = sv.load(\"bi_phrase\")\n",
    "#Phrase vocab filtered with first letter capital from Wikipedia\n",
    "#phrase_vocab = sv.load(\"filtered_phrase_vocab_freq\")\n",
    "#phrase_vocab = sv.load(\"filtered_phrase_vocab_freq_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:47.193022Z",
     "start_time": "2021-02-25T06:15:47.177032Z"
    }
   },
   "outputs": [],
   "source": [
    "len(phrase_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:51.532986Z",
     "start_time": "2021-02-25T06:15:47.194688Z"
    }
   },
   "outputs": [],
   "source": [
    "common_vocab = set(model.wv.vocab) & set(phrase_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:51.541444Z",
     "start_time": "2021-02-25T06:15:51.536009Z"
    }
   },
   "outputs": [],
   "source": [
    "len(common_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:26:24.941776Z",
     "start_time": "2021-02-25T06:15:51.543819Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save X-trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(common_vocab), 300))\n",
    "    for word in tqdm(common_vocab, position=0, leave=True):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-12T12:25:27.820Z"
    }
   },
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T14:10:48.114Z"
    }
   },
   "outputs": [],
   "source": [
    "len(phrase_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T15:45:36.718717Z",
     "start_time": "2020-11-22T15:45:36.714446Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T15:46:25.556163Z",
     "start_time": "2020-11-22T15:45:37.403827Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_phrase_vocab_freq = defaultdict(int)\n",
    "for sent in tqdm(sentences, position=0, leave=True):\n",
    "    lensen = len(sent)\n",
    "    i = 0\n",
    "    try:\n",
    "        while i<lensen:\n",
    "            if sent[i]=='[':\n",
    "                i+=1\n",
    "                while sent[i]!=']':\n",
    "                    if sent[i][0].isupper() and sent[i][1:].islower():\n",
    "                        filtered_phrase_vocab_freq[sent[i]]+=1\n",
    "                    i+=1\n",
    "            else:\n",
    "                i+=1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T15:46:25.662163Z",
     "start_time": "2020-11-22T15:46:25.560551Z"
    }
   },
   "outputs": [],
   "source": [
    "sv.save(filtered_phrase_vocab_freq,\"filtered_phrase_vocab_freq_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save(filtered_phrase_vocab_freq,\"filtered_phrase_vocab_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_phrase_vocab_10 = set()\n",
    "for word,freq in filtered_phrase_vocab_freq.items():\n",
    "    if freq >=10:\n",
    "        filtered_phrase_vocab_10.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_phrase_vocab_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_emb_file_10 = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_r_mc10_epoch5_300_filtered.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(filt_emb_file_10, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(filtered_phrase_vocab_10), 300))\n",
    "    for word in tqdm(filtered_phrase_vocab_10,position=0, leave=True):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:24:15.811039Z",
     "start_time": "2020-07-31T14:16:15.627353Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:24:15.816736Z",
     "start_time": "2020-07-31T14:24:15.812984Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total trainting time:\",model.total_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:24:40.090027Z",
     "start_time": "2020-07-31T14:24:15.818151Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"/mnt/nfs/resdata0/manni/wiki/no_sp_r_m1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:17:22.908996Z",
     "start_time": "2020-06-19T10:35:51.695Z"
    }
   },
   "outputs": [],
   "source": [
    "min100vocab = sv.load(\"en_vocab_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:17:22.910337Z",
     "start_time": "2020-06-19T10:35:55.889Z"
    }
   },
   "outputs": [],
   "source": [
    "new_emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_skip2_r_mc100_epoch5_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:17:22.911527Z",
     "start_time": "2020-06-19T10:35:57.226Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(new_emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(min100vocab), 300))\n",
    "    for word in tqdm(min100vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ngam testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T20:10:55.241574Z",
     "start_time": "2021-02-27T20:08:21.513741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams = sv.load(\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T21:46:11.984024Z",
     "start_time": "2021-02-27T21:44:40.578809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngram_counts = dict()\n",
    "for k,ngs in ngrams.items():\n",
    "    new_dict = dict()\n",
    "    for p in ngs:\n",
    "        for w in p:\n",
    "            w = w.lower()\n",
    "            if w in new_dict:\n",
    "                new_dict[w] += 1\n",
    "            else:\n",
    "                new_dict[w] = 1\n",
    "    ngram_counts[k]=new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T22:09:21.874134Z",
     "start_time": "2021-02-27T22:09:18.533627Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# merging on lowercase\n",
    "ng_counts = dict()\n",
    "for _k,ngs in ngram_counts.items():\n",
    "    k = _k.lower()\n",
    "    if k in ng_counts:\n",
    "        for word,count in ngs.items():\n",
    "            if word in ng_counts[k]:\n",
    "                ng_counts[k][word]+=count\n",
    "            else:\n",
    "                ng_counts[k][word]=count\n",
    "    else:\n",
    "        ng_counts[k] = ngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T22:09:33.055414Z",
     "start_time": "2021-02-27T22:09:25.213606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(ng_counts,\"ngram_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T22:10:00.242755Z",
     "start_time": "2021-02-27T22:10:00.236981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(ngram_counts))\n",
    "print(len(ng_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T10:38:08.600385Z",
     "start_time": "2020-06-21T10:38:04.780273Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "min10vocab = sv.load(\"en_vocab_min10_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T20:38:35.678667Z",
     "start_time": "2020-07-05T20:37:39.162620Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"/mnt/nfs/resdata0/manni/wiki/skip4_m1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T10:39:54.872501Z",
     "start_time": "2020-06-21T10:39:54.867737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_sp_r_mc10_epoch5_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T10:45:21.624290Z",
     "start_time": "2020-06-21T10:39:54.875449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(new_emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(min10vocab), 300))\n",
    "    for word in tqdm(min10vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SPhrase test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    ">>>\n",
    ">>> texts = [['human', 'interface', 'computer']]\n",
    ">>> dct = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T21:36:19.157874Z",
     "start_time": "2020-06-13T21:36:18.682474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"wiki_sentences_sp_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T21:36:19.660635Z",
     "start_time": "2020-06-13T21:36:19.655291Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T09:27:49.908959Z",
     "start_time": "2020-06-14T09:18:46.079637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=40, iter=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T09:29:54.494935Z",
     "start_time": "2020-06-14T09:27:49.911300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open('/mnt/nfs/resdata0/manni/wiki/en_wiki_sp_test_300.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
