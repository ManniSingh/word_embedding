{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3615c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T04:16:27.822543Z",
     "start_time": "2022-09-03T04:16:26.888795Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc861d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T02:45:58.736447Z",
     "start_time": "2022-09-06T02:45:58.733366Z"
    }
   },
   "outputs": [],
   "source": [
    "glove = '/home/manni/embs/gloveW2V.6B.300d.txt'\n",
    "w2v = '/home/manni/embs/w2v.bin.gz'\n",
    "wiki2v = '/home/manni/embs/enwiki_20180420_300d.txt.bz2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d300d722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T04:50:42.200250Z",
     "start_time": "2022-09-03T04:47:33.538927Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(glove, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686d3459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T05:07:28.132813Z",
     "start_time": "2022-09-03T05:04:27.965132Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(w2v, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d3518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T03:30:54.721721Z",
     "start_time": "2022-09-06T02:46:26.849131Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki_model = KeyedVectors.load_word2vec_format(wiki2v, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8658b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:52:59.801518Z",
     "start_time": "2022-09-06T10:52:50.331181Z"
    }
   },
   "outputs": [],
   "source": [
    "words = wiki_model.most_similar('bank',topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0100f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:53:05.579984Z",
     "start_time": "2022-09-06T10:53:05.365775Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki_model.most_similar('bank',topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f3e11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f93435fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:54:49.109053Z",
     "start_time": "2022-09-06T10:54:49.099681Z"
    }
   },
   "outputs": [],
   "source": [
    "def total_overlap(word,model,topn=100):\n",
    "    overlap_matrix = np.zeros((topn,topn),dtype=np.int8)\n",
    "    words = model.most_similar(word,topn=topn)\n",
    "    words = set([i[0] for i in words])\n",
    "    nns = dict()\n",
    "    for word in words:\n",
    "        _words = model.most_similar(word,topn=5)\n",
    "        _words = set([i[0] for i in _words])\n",
    "        nns[word]=_words\n",
    "    for i,(k,a) in enumerate(nns.items()):\n",
    "        for j,(kk,b) in enumerate(nns.items()):\n",
    "            if k == kk:\n",
    "                overlap_matrix[i][j]=1\n",
    "                continue\n",
    "            overlap = len(a & b)\n",
    "            overlap_matrix[i][j]=overlap\n",
    "    return nns,overlap_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41c1c0f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:56:53.394146Z",
     "start_time": "2022-09-06T10:56:33.802453Z"
    }
   },
   "outputs": [],
   "source": [
    "nns,ovm = total_overlap('bank',glove_model,topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22597047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T11:04:30.598682Z",
     "start_time": "2022-09-05T11:04:30.593698Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_closer(n):\n",
    "    print(voc[n])\n",
    "    print(':::::')\n",
    "    nn = set()\n",
    "    for i,val in enumerate(ovm[n]>0):\n",
    "        if val:\n",
    "            print(i,voc[i])\n",
    "            nn.add(voc[i])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c412a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T11:07:31.737434Z",
     "start_time": "2022-09-05T11:07:31.730802Z"
    }
   },
   "outputs": [],
   "source": [
    "a = print_closer(3)\n",
    "b = print_closer(15)\n",
    "print(a&b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5725ca37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:57:04.205686Z",
     "start_time": "2022-09-06T10:57:03.054048Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = dict()\n",
    "voc = list(nns.keys())\n",
    "for v in voc:\n",
    "    clusters[v]=-1\n",
    "c = 0\n",
    "for i in range(len(ovm)):\n",
    "    for j in range(len(ovm)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        vec = ovm[i]\n",
    "        vec = vec>0\n",
    "        vec_ = ovm[j]\n",
    "        vec_= vec_>0\n",
    "        list_a = [voc[i_] for i_,x in enumerate(vec) if x]\n",
    "        list_b = [voc[i_] for i_,x in enumerate(vec_) if x]\n",
    "        overlap = len(set(list_a) & set(list_b))\n",
    "        if overlap>3:\n",
    "            #print(i,':',voc[i],j,':',voc[j])\n",
    "            #if i == 10 or j == 10:\n",
    "            #    print(i,j,clusters[voc[i]],clusters[voc[j]])\n",
    "            if clusters[voc[i]]>-1 and clusters[voc[j]]>-1:\n",
    "                c_ = min(clusters[voc[i]],clusters[voc[j]])\n",
    "                clusters[voc[i]]=c_\n",
    "                clusters[voc[j]]=c_\n",
    "            elif clusters[voc[i]]>-1:\n",
    "                clusters[voc[j]]=clusters[voc[i]]\n",
    "            elif clusters[voc[j]]>-1:\n",
    "                clusters[voc[i]]=clusters[voc[j]]\n",
    "            else:\n",
    "                clusters[voc[i]]=c\n",
    "                clusters[voc[j]]=c\n",
    "                c+=1\n",
    "            #if i == 10 or j == 10:\n",
    "            #    print(i,j,clusters[voc[i]],clusters[voc[j]])\n",
    "        #if np.array_equal(vec,vec_):\n",
    "        #    print(i,':',voc[i],j,':',voc[j])\n",
    "    #print('***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d755e4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'central': -1,\n",
       " 'hsbc': 0,\n",
       " 'markets': -1,\n",
       " 'branch': -1,\n",
       " 'securities': -1,\n",
       " 'money': 1,\n",
       " 'lenders': 1,\n",
       " 'palestinian': -1,\n",
       " 'assets': 1,\n",
       " 'commercial': -1,\n",
       " 'deposits': 1,\n",
       " 'amro': 3,\n",
       " 'settlements': -1,\n",
       " 'institution': -1,\n",
       " 'subsidiary': -1,\n",
       " 'financing': 1,\n",
       " 'banco': -1,\n",
       " 'finance': -1,\n",
       " 'banks': 1,\n",
       " 'settlement': -1,\n",
       " 'swiss': -1,\n",
       " 'institutions': -1,\n",
       " 'ramallah': -1,\n",
       " 'occupied': -1,\n",
       " 'month': -1,\n",
       " 'currency': -1,\n",
       " 'debt': 1,\n",
       " 'earlier': -1,\n",
       " 'stake': -1,\n",
       " 'mortgage': 1,\n",
       " 'raised': -1,\n",
       " 'banking': 1,\n",
       " 'rates': -1,\n",
       " 'liquidity': 1,\n",
       " 'interbk': -1,\n",
       " 'ecb': -1,\n",
       " 'treasury': -1,\n",
       " 'dollars': -1,\n",
       " 'fed': -1,\n",
       " 'cash': 1,\n",
       " 'investments': 1,\n",
       " 'government': -1,\n",
       " 'banker': -1,\n",
       " 'citibank': 0,\n",
       " 'raise': -1,\n",
       " 'imf': -1,\n",
       " 'offices': -1,\n",
       " 'brokerage': -1,\n",
       " 'monetary': -1,\n",
       " 'the': -1,\n",
       " 'deposit': -1,\n",
       " 'asset': 1,\n",
       " 'fargo': -1,\n",
       " 'sector': -1,\n",
       " 'borrowing': -1,\n",
       " 'economist': -1,\n",
       " 'bankers': 1,\n",
       " 'dresdner': -1,\n",
       " 'barclays': -1,\n",
       " 'accounts': -1,\n",
       " 'withdrawals': -1,\n",
       " 'investment': 1,\n",
       " 'lender': 1,\n",
       " 'deutsche': -1,\n",
       " 'holdings': -1,\n",
       " 'branches': -1,\n",
       " 'loan': 1,\n",
       " 'funds': 1,\n",
       " 'its': -1,\n",
       " 'capital': 1,\n",
       " 'interest': -1,\n",
       " 'israel': 6,\n",
       " 'loans': 1,\n",
       " 'lending': 1,\n",
       " 'west': -1,\n",
       " 'boj': -1,\n",
       " 'insurance': -1,\n",
       " 'rate': -1,\n",
       " 'israeli': 6,\n",
       " 'investors': -1,\n",
       " 'abn': 3,\n",
       " 'account': -1,\n",
       " 'overnight': -1,\n",
       " 'suisse': -1,\n",
       " 'ubs': 0,\n",
       " 'reserve': -1,\n",
       " 'chase': -1,\n",
       " 'fund': 1,\n",
       " 'trust': 1,\n",
       " 'nablus': -1,\n",
       " 'financial': -1,\n",
       " 'palestinians': 6,\n",
       " 'savings': -1,\n",
       " 'exchange': -1,\n",
       " 'credit': 1,\n",
       " 'jpmorgan': 0,\n",
       " 'gaza': -1,\n",
       " 'citigroup': 0,\n",
       " 'sachs': 0,\n",
       " 'ltd.': -1}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b62aa363",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_ = dict()\n",
    "for i,c in enumerate(clusters.values()):\n",
    "    if c == -1:\n",
    "        continue\n",
    "    if c in clusters_:\n",
    "        clusters_[c].append(voc[i])\n",
    "    else:\n",
    "        clusters_[c] = [voc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a0f0949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['hsbc',\n",
       "  'amro',\n",
       "  'citibank',\n",
       "  'fargo',\n",
       "  'economist',\n",
       "  'dresdner',\n",
       "  'barclays',\n",
       "  'deutsche',\n",
       "  'abn',\n",
       "  'suisse',\n",
       "  'ubs',\n",
       "  'chase',\n",
       "  'jpmorgan',\n",
       "  'citigroup',\n",
       "  'sachs'],\n",
       " 1: ['securities',\n",
       "  'money',\n",
       "  'lenders',\n",
       "  'assets',\n",
       "  'deposits',\n",
       "  'financing',\n",
       "  'finance',\n",
       "  'banks',\n",
       "  'institutions',\n",
       "  'debt',\n",
       "  'mortgage',\n",
       "  'banking',\n",
       "  'rates',\n",
       "  'liquidity',\n",
       "  'fed',\n",
       "  'cash',\n",
       "  'investments',\n",
       "  'raise',\n",
       "  'deposit',\n",
       "  'asset',\n",
       "  'borrowing',\n",
       "  'bankers',\n",
       "  'accounts',\n",
       "  'investment',\n",
       "  'lender',\n",
       "  'holdings',\n",
       "  'loan',\n",
       "  'funds',\n",
       "  'capital',\n",
       "  'interest',\n",
       "  'loans',\n",
       "  'lending',\n",
       "  'rate',\n",
       "  'reserve',\n",
       "  'fund',\n",
       "  'trust',\n",
       "  'financial',\n",
       "  'credit'],\n",
       " 3: ['palestinian', 'ramallah', 'israel', 'israeli', 'palestinians', 'gaza']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2c6e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:57:08.838657Z",
     "start_time": "2022-09-06T10:57:08.835017Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_matrix(model,vocab):\n",
    "    matrix = list()\n",
    "    for word in vocab:\n",
    "        vec = model.get_vector(word)\n",
    "        matrix.append(vec)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c396b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72fe17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:57:09.590216Z",
     "start_time": "2022-09-06T10:57:09.586470Z"
    }
   },
   "outputs": [],
   "source": [
    "matrix = get_matrix(glove_model,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02783965",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:57:10.202341Z",
     "start_time": "2022-09-06T10:57:10.199456Z"
    }
   },
   "outputs": [],
   "source": [
    "matrix = np.asarray(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d38942",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9dec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T08:57:26.230716Z",
     "start_time": "2022-09-04T08:57:26.227804Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91eec4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T08:57:53.105922Z",
     "start_time": "2022-09-04T08:57:53.101094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.4,min_samples=2,metric='cosine').fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd163cfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T08:57:54.152284Z",
     "start_time": "2022-09-04T08:57:54.147759Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = db.labels_\n",
    "clusters = dict()\n",
    "for i,c in enumerate(labels):\n",
    "    if c == -1:\n",
    "        continue\n",
    "    if c in clusters:\n",
    "        clusters[c].append(vocab[i])\n",
    "    else:\n",
    "        clusters[c] = [vocab[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9b4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T08:57:54.422562Z",
     "start_time": "2022-09-04T08:57:54.416438Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f4809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T13:36:38.213641Z",
     "start_time": "2022-09-03T13:36:38.206152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe464db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T08:34:40.641537Z",
     "start_time": "2022-09-04T08:34:40.633434Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = [[1, 2], [2, 2], [2, 3],[8, 7], [8, 8], [25, 80]]\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9627e0",
   "metadata": {},
   "source": [
    "# wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "42953c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dde97549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3cf22afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff8a16c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47c9e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a dull unattractive unpleasant girl or woman'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('frump.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fdbf8e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'informal term for a man'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.03').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b551adb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42ae361c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4c0da744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertical_bank.n.01')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank')[9].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f6ad014",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(wn.all_synsets('n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57bfd509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82115"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "977ba745",
   "metadata": {},
   "outputs": [],
   "source": [
    "_vocab = list(wn.all_synsets('v'))\n",
    "vocab = vocab+_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9cb1f000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('breathe.v.01'),\n",
       " Synset('respire.v.02'),\n",
       " Synset('respire.v.01'),\n",
       " Synset('choke.v.01'),\n",
       " Synset('hyperventilate.v.02'),\n",
       " Synset('hyperventilate.v.01'),\n",
       " Synset('aspirate.v.03'),\n",
       " Synset('burp.v.01'),\n",
       " Synset('force_out.v.08'),\n",
       " Synset('hiccup.v.01')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6d679afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95882"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925515b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5aa44188",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = set()\n",
    "for ss in vocab:\n",
    "    word = ss.name().split('.')[0]\n",
    "    if '_' in word:\n",
    "        continue\n",
    "    vocab_.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "01e0081f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44400"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "70fb97bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinky',\n",
       " 'ummah',\n",
       " 'volcanism',\n",
       " 'sigmoidoscope',\n",
       " 'newsagent',\n",
       " 'watts',\n",
       " 'isaac',\n",
       " 'needlebush',\n",
       " 'procyonid',\n",
       " 'decode']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9ce3ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniques(input_list):\n",
    "    uniques = set()\n",
    "    for k,v in Counter(input_list).items():\n",
    "        if v>1:\n",
    "            continue\n",
    "        uniques.add(k)\n",
    "    return uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "389498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "word_desc = dict()\n",
    "for word in vocab_:\n",
    "    synsets = wn.synsets(word)\n",
    "    for synset in synsets:\n",
    "        txt = synset.definition().lower()\n",
    "        txt = re.sub(r'\\W+', ' ', txt)\n",
    "        words = [w for w in txt.split() if w not in stops]\n",
    "        #txts = synset.examples()\n",
    "        #txt = ' '.join(txts)\n",
    "        #txt = re.sub(r'\\W+', ' ', txt)\n",
    "        words_ = [w for w in txt.split() if w not in stops]\n",
    "        words = words + words_\n",
    "        hypo = synset.hyponyms()\n",
    "        hypo_words = [ss.name().split('.')[0] for ss in hypo]\n",
    "        hyper = synset.hypernyms()\n",
    "        hyper_words = [ss.name().split('.')[0] for ss in hyper]\n",
    "        _words = words+hypo_words+hyper_words\n",
    "        words = list()\n",
    "        for w in _words:\n",
    "            words.extend(w.split('_'))\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "        words = set(words)\n",
    "        #add to dict\n",
    "        if word in word_desc:\n",
    "            word_desc[word].append(words)\n",
    "        else:\n",
    "            word_desc[word]=[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6bee40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2desc = dict()\n",
    "for k,v in word_desc.items():\n",
    "    word2desc[k] = list()\n",
    "    all_words = [item for sublist in v for item in sublist]\n",
    "    uniques = get_uniques(all_words)\n",
    "    for words in v:\n",
    "        words = set(words)\n",
    "        word2desc[k].append(words&uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fd3fa1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'animal',\n",
       "  'basenji',\n",
       "  'breed',\n",
       "  'canine',\n",
       "  'canis',\n",
       "  'common',\n",
       "  'corgi',\n",
       "  'cur',\n",
       "  'dalmatian',\n",
       "  'descended',\n",
       "  'dog',\n",
       "  'domestic',\n",
       "  'domesticated',\n",
       "  'genus',\n",
       "  'great',\n",
       "  'griffon',\n",
       "  'hairless',\n",
       "  'hunting',\n",
       "  'lapdog',\n",
       "  'leonberg',\n",
       "  'many',\n",
       "  'member',\n",
       "  'mexican',\n",
       "  'newfoundland',\n",
       "  'occurs',\n",
       "  'pooch',\n",
       "  'poodle',\n",
       "  'prehistoric',\n",
       "  'probably',\n",
       "  'pug',\n",
       "  'puppy',\n",
       "  'pyrenees',\n",
       "  'since',\n",
       "  'spitz',\n",
       "  'time',\n",
       "  'toy',\n",
       "  'wolf',\n",
       "  'working'},\n",
       " {'dull', 'girl', 'unattractive', 'unpleasant', 'woman'},\n",
       " {'chap', 'informal', 'term'},\n",
       " {'morally', 'perisher', 'reprehensible', 'someone', 'villain'},\n",
       " {'beef',\n",
       "  'bread',\n",
       "  'minced',\n",
       "  'often',\n",
       "  'pork',\n",
       "  'roll',\n",
       "  'sausage',\n",
       "  'served',\n",
       "  'smoked',\n",
       "  'smooth',\n",
       "  'textured',\n",
       "  'usually',\n",
       "  'vienna'},\n",
       " {'backward',\n",
       "  'fit',\n",
       "  'forward',\n",
       "  'hinged',\n",
       "  'move',\n",
       "  'moving',\n",
       "  'notch',\n",
       "  'prevent',\n",
       "  'ratchet',\n",
       "  'wheel'},\n",
       " {'fireplace', 'log', 'metal', 'support'},\n",
       " {'down', 'go', 'hound', 'intent', 'pursue', 'quest', 'run', 'tree'}]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2desc['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eafe510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "72d5f63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the data\n"
     ]
    }
   ],
   "source": [
    "sv.save(word2desc,\"word2desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "678e7f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40968"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e5252fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinky',\n",
       " 'ummah',\n",
       " 'volcanism',\n",
       " 'sigmoidoscope',\n",
       " 'newsagent',\n",
       " 'watts',\n",
       " 'isaac',\n",
       " 'needlebush',\n",
       " 'procyonid',\n",
       " 'shredder']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "54059d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the dog barked all night']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6beb0d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "03120e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('rally.n.02')\n",
      "the feat of mustering strength for a renewed effort\n",
      "the feat of mustering strength for a renewed effort\n",
      "{'effort', 'mustering', 'renewed', 'strength', 'feat'}\n",
      "[Synset('rally.n.01'), Synset('rally.n.02'), Synset('rally.n.03'), Synset('rally.n.04'), Synset('rally.n.05'), Synset('beat_up.v.02'), Synset('call_up.v.04'), Synset('muster.v.01'), Synset('rally.v.04'), Synset('tease.v.02')]\n",
      "\n",
      "Synset('recovery.n.03')\n",
      "the act of regaining or saving something lost (or in danger of becoming lost)\n",
      "the act of regaining or saving something lost or in danger of becoming lost \n",
      "{'danger', 'something', 'saving', 'act', 'becoming', 'lost', 'regaining'}\n",
      "[Synset('recovery.n.01'), Synset('convalescence.n.01'), Synset('recovery.n.03')]\n",
      "\n",
      "Synset('running_away.n.01')\n",
      "the act of leaving (without permission) the place you are expected to be\n",
      "the act of leaving without permission the place you are expected to be\n",
      "{'act', 'without', 'permission', 'place', 'leaving', 'expected'}\n",
      "[Synset('running_away.n.01')]\n",
      "\n",
      "Synset('stunt.n.01')\n",
      "a difficult or unusual or dangerous feat; usually done to gain attention\n",
      "a difficult or unusual or dangerous feat usually done to gain attention\n",
      "{'done', 'gain', 'unusual', 'difficult', 'usually', 'attention', 'dangerous', 'feat'}\n",
      "[Synset('stunt.n.01'), Synset('stunt.n.02'), Synset('stunt.v.01'), Synset('stunt.v.02')]\n",
      "\n",
      "Synset('touch.n.05')\n",
      "the act of putting two things together with no space between them\n",
      "the act of putting two things together with no space between them\n",
      "{'together', 'act', 'putting', 'things', 'space', 'two'}\n",
      "[Synset('touch.n.01'), Synset('touch.n.02'), Synset('touch.n.03'), Synset('touch.n.04'), Synset('touch.n.05'), Synset('touch.n.06'), Synset('contact.n.08'), Synset('touch.n.08'), Synset('touch.n.09'), Synset('touch.n.10'), Synset('touch.n.11'), Synset('touch.n.12'), Synset('touch.v.01'), Synset('touch.v.02'), Synset('touch.v.03'), Synset('refer.v.02'), Synset('touch.v.05'), Synset('affect.v.01'), Synset('touch.v.07'), Synset('touch.v.08'), Synset('reach.v.06'), Synset('equal.v.02'), Synset('touch.v.11'), Synset('allude.v.01'), Synset('touch.v.13'), Synset('partake.v.03'), Synset('tint.v.01')]\n",
      "\n",
      "Synset('tour_de_force.n.01')\n",
      "a masterly or brilliant feat\n",
      "a masterly or brilliant feat\n",
      "{'brilliant', 'feat', 'masterly'}\n",
      "[Synset('tour_de_force.n.01')]\n",
      "\n",
      "Synset('performance.n.04')\n",
      "any recognized accomplishment\n",
      "any recognized accomplishment\n",
      "{'recognized', 'accomplishment'}\n",
      "[Synset('performance.n.01'), Synset('performance.n.02'), Synset('performance.n.03'), Synset('performance.n.04'), Synset('operation.n.08')]\n",
      "\n",
      "Synset('overachievement.n.01')\n",
      "better than expected performance (better than might have been predicted from intelligence tests)\n",
      "better than expected performance better than might have been predicted from intelligence tests \n",
      "{'better', 'might', 'performance', 'predicted', 'intelligence', 'tests', 'expected'}\n",
      "[Synset('overachievement.n.01')]\n",
      "\n",
      "Synset('underachievement.n.01')\n",
      "poorer than expected performance (poorer than might have been predicted from intelligence tests)\n",
      "poorer than expected performance poorer than might have been predicted from intelligence tests \n",
      "{'might', 'performance', 'predicted', 'intelligence', 'tests', 'poorer', 'expected'}\n",
      "[Synset('underachievement.n.01')]\n",
      "\n",
      "Synset('record.n.04')\n",
      "the sum of recognized accomplishments\n",
      "the sum of recognized accomplishments\n",
      "{'recognized', 'sum', 'accomplishments'}\n",
      "[Synset('record.n.01'), Synset('phonograph_record.n.01'), Synset('record.n.03'), Synset('record.n.04'), Synset('record.n.05'), Synset('record.n.06'), Synset('record.n.07'), Synset('criminal_record.n.01'), Synset('record.v.01'), Synset('record.v.02'), Synset('read.v.08'), Synset('record.v.04'), Synset('commemorate.v.03')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in vocab_[100:110]:\n",
    "    print(i)\n",
    "    txt = i.definition().lower()\n",
    "    print(txt)\n",
    "    txt = re.sub(r'\\W+', ' ', txt)\n",
    "    print(txt)\n",
    "    words = [word for word in txt.split() if word not in stops]\n",
    "    print(set(words))\n",
    "    word = i.name().split('.')[0]\n",
    "    print(wn.synsets(word))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fb890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
