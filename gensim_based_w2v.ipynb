{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T12:18:15.797739Z",
     "start_time": "2022-09-26T12:18:15.793381Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "#import wiki_old as w # old wiki\n",
    "import wiki as w \n",
    " \n",
    "#from gensim.models import word2vec # for orignal w2v\n",
    "from localgensim.gensim2.models import word2vec \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models.fasttext import FastText\n",
    "#from gensim.models.word2vec import Word2Vec # not in use\n",
    "#from localgensim.gensim2.models.word2vec import Word2Vec # not in use\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream.xml.bz2'\n",
    "#WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream1.xml-p1p41242.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T12:17:56.993211Z",
     "start_time": "2022-09-26T12:17:56.989327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manni/ner-s2s/word_embedding/wiki.py\n",
      "/home/manni/ner-s2s/word_embedding/localgensim/gensim2/models/word2vec.py\n"
     ]
    }
   ],
   "source": [
    "print(w.__file__)\n",
    "print(word2vec.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T12:17:57.008377Z",
     "start_time": "2022-09-26T12:17:56.994983Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T12:17:57.020209Z",
     "start_time": "2022-09-26T12:17:57.010721Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO)\n",
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:25:05.774307Z",
     "start_time": "2020-10-25T13:25:05.608721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Training model %s', 'word2vec')\n",
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=10, size=300, sample=0, \n",
    "                 workers=1, iter=1, min_count=1)\n",
    "\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:48:45.834994Z",
     "start_time": "2020-10-25T13:47:14.780675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conll corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"conll_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:52:12.732111Z",
     "start_time": "2020-10-25T13:48:45.838043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=1, iter=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_spx2g.txt'\n",
    "emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip if sentence made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T17:03:27.374988Z",
     "start_time": "2022-09-26T12:18:19.512134Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 13:18:19,513] Parsing wiki corpus Altered...\n",
      "[2022-09-26 13:18:29,741] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "[2022-09-26 13:21:46,444] adding document #30000 to Dictionary(933211 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:22:37,323] adding document #40000 to Dictionary(1068453 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:23:14,893] adding document #50000 to Dictionary(1150408 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:23:37,156] adding document #60000 to Dictionary(1171155 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:23:57,062] adding document #70000 to Dictionary(1189915 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:24:16,209] adding document #80000 to Dictionary(1205627 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:25:03,343] adding document #90000 to Dictionary(1305340 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:25:51,903] adding document #100000 to Dictionary(1419477 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:26:35,492] adding document #110000 to Dictionary(1512304 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:27:18,115] adding document #120000 to Dictionary(1604555 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:27:57,746] adding document #130000 to Dictionary(1677746 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:28:40,402] adding document #140000 to Dictionary(1768322 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:29:17,981] adding document #150000 to Dictionary(1855202 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:30:00,279] adding document #160000 to Dictionary(1937990 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:30:41,096] discarding 6792 tokens: [('thinngartsaigh', 1), ('thuilm', 1), ('thòraidh', 1), ('trosdam', 1), ('ulluva', 1), ('urray', 1), ('vacsay', 1), ('vaich', 1), ('àrnol', 1), ('blagoevgradska', 1)]...\n",
      "[2022-09-26 13:30:41,098] keeping 2000000 tokens which were in no less than 0 and no more than 170000 (=100.0%) documents\n",
      "[2022-09-26 13:30:45,696] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:30:45,727] adding document #170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:31:24,518] discarding 61346 tokens: [('essedones', 1), ('gelonos', 1), ('jaxamatae', 1), ('kimmerian', 1), ('maeotae', 1), ('maiotian', 1), ('melanchlaenae', 1), ('melanchlaini', 1), ('melanchlainoi', 1), ('mæotis', 1)]...\n",
      "[2022-09-26 13:31:24,521] keeping 2000000 tokens which were in no less than 0 and no more than 180000 (=100.0%) documents\n",
      "[2022-09-26 13:31:28,044] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:31:28,097] adding document #180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:32:05,763] discarding 60671 tokens: [('noxilizer', 1), ('peroxyacetic', 1), ('sterilizerdry', 1), ('ōū', 1), ('奥羽', 1), ('archbishoppe', 1), ('befel', 1), ('broughte', 1), ('coulde', 1), ('distresfull', 1)]...\n",
      "[2022-09-26 13:32:05,765] keeping 2000000 tokens which were in no less than 0 and no more than 190000 (=100.0%) documents\n",
      "[2022-09-26 13:32:10,839] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:32:10,959] adding document #190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:32:50,761] discarding 66424 tokens: [('wessoum', 1), ('weunquesh', 1), ('acateno', 1), ('agujereado', 1), ('agustìn', 1), ('ahuacatlán', 1), ('aijojuca', 1), ('ajalpan', 1), ('alcerreca', 1), ('aljojuca', 1)]...\n",
      "[2022-09-26 13:32:50,763] keeping 2000000 tokens which were in no less than 0 and no more than 200000 (=100.0%) documents\n",
      "[2022-09-26 13:32:54,313] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:32:54,368] adding document #200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:33:31,868] discarding 56349 tokens: [('cuelcahen', 1), ('cuelgas', 1), ('didla', 1), ('dshä', 1), ('dshäⁿ', 1), ('enimes', 1), ('gokíyaa', 1), ('goschish', 1), ('goãgahîî', 1), ('hleh', 1)]...\n",
      "[2022-09-26 13:33:31,870] keeping 2000000 tokens which were in no less than 0 and no more than 210000 (=100.0%) documents\n",
      "[2022-09-26 13:33:35,426] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:33:35,479] adding document #210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:34:13,790] discarding 60387 tokens: [('febendazole', 1), ('giadins', 1), ('giardiavax', 1), ('ornidazole', 1), ('secnidazole', 1), ('blutturm', 1), ('flemig', 1), ('hamgate', 1), ('hamtor', 1), ('historicistic', 1)]...\n",
      "[2022-09-26 13:34:13,792] keeping 2000000 tokens which were in no less than 0 and no more than 220000 (=100.0%) documents\n",
      "[2022-09-26 13:34:18,730] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:34:18,808] adding document #220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:34:55,977] discarding 60875 tokens: [('refabrication', 1), ('yamhd', 1), ('cavoukian', 1), ('edroff', 1), ('methorst', 1), ('oppal', 1), ('sidoo', 1), ('chillura', 1), ('altenhoff', 1), ('carpimus', 1)]...\n",
      "[2022-09-26 13:34:55,979] keeping 2000000 tokens which were in no less than 0 and no more than 230000 (=100.0%) documents\n",
      "[2022-09-26 13:35:01,255] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:35:01,335] adding document #230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:35:40,296] discarding 53825 tokens: [('hussburg', 1), ('husul', 1), ('postelnici', 1), ('rabczony', 1), ('burnas', 1), ('conservturris', 1), ('electroturris', 1), ('haralambios', 1), ('holavnik', 1), ('manufactura', 1)]...\n",
      "[2022-09-26 13:35:40,298] keeping 2000000 tokens which were in no less than 0 and no more than 240000 (=100.0%) documents\n",
      "[2022-09-26 13:35:45,709] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:35:45,790] adding document #240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:36:25,447] discarding 58375 tokens: [('congelăm', 1), ('criogenia', 1), ('cursă', 1), ('eutanasiați', 1), ('frigidere', 1), ('georgică', 1), ('japonezii', 1), ('kerezsi', 1), ('liniștiţi', 1), ('medeleanu', 1)]...\n",
      "[2022-09-26 13:36:25,449] keeping 2000000 tokens which were in no less than 0 and no more than 250000 (=100.0%) documents\n",
      "[2022-09-26 13:36:30,446] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:36:30,526] adding document #250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:37:07,723] discarding 53816 tokens: [('қхаqhaанк', 1), ('әділ', 1), ('әділadalадал', 1), ('assemblyseats', 1), ('gagaryedo', 1), ('samgyun', 1), ('가가례도인연합', 1), ('가자코리아', 1), ('가자환경당', 1), ('거지당', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 13:37:07,725] keeping 2000000 tokens which were in no less than 0 and no more than 260000 (=100.0%) documents\n",
      "[2022-09-26 13:37:12,596] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:37:12,677] adding document #260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:37:58,978] discarding 58966 tokens: [('redswoosh', 1), ('sharesleuth', 1), ('unikrn', 1), ('macadma', 1), ('circumsision', 1), ('isikirari', 1), ('lmuran', 1), ('loibonok', 1), ('manyattas', 1), ('nkop', 1)]...\n",
      "[2022-09-26 13:37:58,979] keeping 2000000 tokens which were in no less than 0 and no more than 270000 (=100.0%) documents\n",
      "[2022-09-26 13:38:02,659] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:38:02,717] adding document #270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:38:39,613] discarding 56478 tokens: [('udiyya', 1), ('vanod', 1), ('ܡܠܟܝܐ', 1), ('balananda', 1), ('ducouri', 1), ('harikamini', 1), ('harimohini', 1), ('harimoti', 1), ('mahasay', 1), ('mahavtar', 1)]...\n",
      "[2022-09-26 13:38:39,617] keeping 2000000 tokens which were in no less than 0 and no more than 280000 (=100.0%) documents\n",
      "[2022-09-26 13:38:44,659] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:38:44,740] adding document #280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:39:20,950] discarding 62518 tokens: [('percichthyids', 1), ('percilia', 1), ('wujalwujalensis', 1), ('baimuradov', 1), ('batoeva', 1), ('bazhenova', 1), ('belik', 1), ('brilyova', 1), ('caixeta', 1), ('chereshkevich', 1)]...\n",
      "[2022-09-26 13:39:20,952] keeping 2000000 tokens which were in no less than 0 and no more than 290000 (=100.0%) documents\n",
      "[2022-09-26 13:39:25,894] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:39:25,978] adding document #290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:39:59,829] discarding 59115 tokens: [('fpai', 1), ('rasmee', 1), ('tinkitam', 1), ('zirsanga', 1), ('alanz', 1), ('altaanz', 1), ('feidhmí', 1), ('iraal', 1), ('sajals', 1), ('wicaksono', 1)]...\n",
      "[2022-09-26 13:39:59,831] keeping 2000000 tokens which were in no less than 0 and no more than 300000 (=100.0%) documents\n",
      "[2022-09-26 13:40:03,441] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:40:03,500] adding document #300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:40:39,968] discarding 61247 tokens: [('hyponotic', 1), ('jaspera', 1), ('prententiousness', 1), ('belateldly', 1), ('freytagh', 1), ('helfereich', 1), ('hellferich', 1), ('hugenzwerg', 1), ('monarchismus', 1), ('mush#0', 1)]...\n",
      "[2022-09-26 13:40:39,970] keeping 2000000 tokens which were in no less than 0 and no more than 310000 (=100.0%) documents\n",
      "[2022-09-26 13:40:43,590] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:40:43,650] adding document #310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:41:19,513] discarding 52241 tokens: [('câ', 1), ('hlàng', 1), ('hlúqbùr', 1), ('hnjie', 1), ('hnyûmò', 1), ('hrã', 1), ('knaŋku', 1), ('koŋũ', 1), ('kuekue', 1), ('kébanè', 1)]...\n",
      "[2022-09-26 13:41:19,514] keeping 2000000 tokens which were in no less than 0 and no more than 320000 (=100.0%) documents\n",
      "[2022-09-26 13:41:24,475] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:41:24,555] adding document #320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:41:58,467] discarding 48974 tokens: [('gajendravarada', 1), ('govardhanadhari', 1), ('kailashanatha', 1), ('ksetrapala', 1), ('lalitacaritra', 1), ('mahawara', 1), ('nrtya', 1), ('pindas', 1), ('rshibha', 1), ('sarvatobhadra', 1)]...\n",
      "[2022-09-26 13:41:58,469] keeping 2000000 tokens which were in no less than 0 and no more than 330000 (=100.0%) documents\n",
      "[2022-09-26 13:42:03,515] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:42:03,598] adding document #330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:42:37,871] discarding 45827 tokens: [('ökonomiegebäude', 1), ('falelie', 1), ('empre', 1), ('craatlar', 1), ('kanadoğlu', 1), ('kavaf', 1), ('akroyds', 1), ('misantrophe', 1), ('thwacked', 1), ('undervelier', 1)]...\n",
      "[2022-09-26 13:42:37,873] keeping 2000000 tokens which were in no less than 0 and no more than 340000 (=100.0%) documents\n",
      "[2022-09-26 13:42:41,981] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:42:42,040] adding document #340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:43:14,224] discarding 44873 tokens: [('ostads', 1), ('panjgaah', 1), ('peygham', 1), ('peyvand', 1), ('peyvande', 1), ('pirniakan', 1), ('pournazeri', 1), ('raheleh', 1), ('ranghaye', 1), ('rezaeenia', 1)]...\n",
      "[2022-09-26 13:43:14,226] keeping 2000000 tokens which were in no less than 0 and no more than 350000 (=100.0%) documents\n",
      "[2022-09-26 13:43:18,853] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:43:18,915] adding document #350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:43:51,046] discarding 45720 tokens: [('neuralation', 1), ('neurocele', 1), ('abietinic', 1), ('depackage', 1), ('factorium', 1), ('kotlyakova', 1), ('lyubomlsky', 1), ('pjat', 1), ('pripech', 1), ('aktivnye', 1)]...\n",
      "[2022-09-26 13:43:51,048] keeping 2000000 tokens which were in no less than 0 and no more than 360000 (=100.0%) documents\n",
      "[2022-09-26 13:43:54,693] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:43:54,752] adding document #360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:44:28,832] discarding 50100 tokens: [('karunakar', 1), ('khuntia', 1), ('krutibas', 1), ('mahakud', 1), ('mohakud', 1), ('mohanti', 1), ('parsuram', 1), ('pindiki', 1), ('puhan', 1), ('rajasmita', 1)]...\n",
      "[2022-09-26 13:44:28,834] keeping 2000000 tokens which were in no less than 0 and no more than 370000 (=100.0%) documents\n",
      "[2022-09-26 13:44:33,801] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:44:33,890] adding document #370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:45:10,682] discarding 44217 tokens: [('bartholinbygningen', 1), ('besenbacher', 1), ('biomedicin', 1), ('bogtårnet', 1), ('bronzealdervænget', 1), ('cheminova', 1), ('deptcompsci', 1), ('emdrup', 1), ('fuglsangs', 1), ('hejredal', 1)]...\n",
      "[2022-09-26 13:45:10,684] keeping 2000000 tokens which were in no less than 0 and no more than 380000 (=100.0%) documents\n",
      "[2022-09-26 13:45:14,837] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:45:14,896] adding document #380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 13:45:46,956] discarding 46703 tokens: [('豊津上野', 1), ('近鉄八田', 1), ('近鉄名古屋', 1), ('近鉄四日市', 1), ('近鉄富田', 1), ('近鉄弥富', 1), ('近鉄蟹江', 1), ('近鉄長島', 1), ('長太ノ浦', 1), ('阿倉川', 1)]...\n",
      "[2022-09-26 13:45:46,958] keeping 2000000 tokens which were in no less than 0 and no more than 390000 (=100.0%) documents\n",
      "[2022-09-26 13:45:52,087] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:45:52,181] adding document #390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:46:24,767] discarding 52783 tokens: [('bivalavia', 1), ('branchipolynoe', 1), ('chaceon', 1), ('childressi', 1), ('comarge', 1), ('cordesia', 1), ('diapirbt', 1), ('galatheids', 1), ('holothurid', 1), ('leplac', 1)]...\n",
      "[2022-09-26 13:46:24,769] keeping 2000000 tokens which were in no less than 0 and no more than 400000 (=100.0%) documents\n",
      "[2022-09-26 13:46:28,425] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:46:28,487] adding document #400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:47:02,084] discarding 51820 tokens: [('pilkie', 1), ('zawerbny', 1), ('autoritatea', 1), ('fetian', 1), ('kashaf', 1), ('matakadem', 1), ('كشاف', 1), ('كن', 1), ('مستعدا', 1), ('erneleye', 1)]...\n",
      "[2022-09-26 13:47:02,086] keeping 2000000 tokens which were in no less than 0 and no more than 410000 (=100.0%) documents\n",
      "[2022-09-26 13:47:06,385] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:47:06,462] adding document #410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:47:38,781] discarding 46086 tokens: [('clasters', 1), ('pioneeringmadeeasy', 1), ('pioneeringprojects', 1), ('roundturn', 1), ('katsufrakis', 1), ('petratishkovna', 1), ('bauerberger', 1), ('filipeto', 1), ('lucieta', 1), ('nashb', 1)]...\n",
      "[2022-09-26 13:47:38,783] keeping 2000000 tokens which were in no less than 0 and no more than 420000 (=100.0%) documents\n",
      "[2022-09-26 13:47:42,438] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:47:42,499] adding document #420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:48:15,543] discarding 50536 tokens: [('drowsiness§', 1), ('greatniece', 1), ('bolamba', 1), ('fransoise', 1), ('poppg', 1), ('mfes', 1), ('vetverify', 1), ('adizua', 1), ('ajanapu', 1), ('ajanupu', 1)]...\n",
      "[2022-09-26 13:48:15,544] keeping 2000000 tokens which were in no less than 0 and no more than 430000 (=100.0%) documents\n",
      "[2022-09-26 13:48:20,496] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:48:20,583] adding document #430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:48:52,812] discarding 47665 tokens: [('chestain', 1), ('coullde', 1), ('cullerid', 1), ('curchey', 1), ('famyliar', 1), ('fatche', 1), ('feates', 1), ('foyst', 1), ('frindly', 1), ('hornebooke', 1)]...\n",
      "[2022-09-26 13:48:52,814] keeping 2000000 tokens which were in no less than 0 and no more than 440000 (=100.0%) documents\n",
      "[2022-09-26 13:48:56,474] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:48:56,536] adding document #440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:49:29,622] discarding 50850 tokens: [('buttermuffin', 1), ('soooo', 1), ('abikzer', 1), ('dennice', 1), ('godburn', 1), ('lutzke', 1), ('lyricized', 1), ('ciachir', 1), ('pashalyks', 1), ('reguaranteed', 1)]...\n",
      "[2022-09-26 13:49:29,625] keeping 2000000 tokens which were in no less than 0 and no more than 450000 (=100.0%) documents\n",
      "[2022-09-26 13:49:33,301] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:49:33,366] adding document #450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:50:04,811] discarding 40434 tokens: [('duroi', 1), ('kohso', 1), ('prazska', 1), ('sfyjw', 1), ('soldateque', 1), ('sphinxeries', 1), ('troubairitz', 1), ('jõega', 1), ('metsoja', 1), ('pdxdj', 1)]...\n",
      "[2022-09-26 13:50:04,813] keeping 2000000 tokens which were in no less than 0 and no more than 460000 (=100.0%) documents\n",
      "[2022-09-26 13:50:08,531] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:50:08,592] adding document #460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:50:40,680] discarding 49139 tokens: [('гьабулеб', 1), ('гьез', 1), ('гьитӏинав', 1), ('иш', 1), ('киве', 1), ('кӏкӏ', 1), ('лълъ', 1), ('лъугьа', 1), ('нух', 1), ('тӏаса', 1)]...\n",
      "[2022-09-26 13:50:40,682] keeping 2000000 tokens which were in no less than 0 and no more than 470000 (=100.0%) documents\n",
      "[2022-09-26 13:50:45,825] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:50:45,916] adding document #470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:51:16,703] discarding 44298 tokens: [('machinegum', 1), ('ipges', 1), ('iridosmine', 1), ('pentaamminechlororhodium', 1), ('platidises', 1), ('platiniridium', 1), ('platinoids', 1), ('ppges', 1), ('chosangyo', 1), ('chungryeolsa', 1)]...\n",
      "[2022-09-26 13:51:16,705] keeping 2000000 tokens which were in no less than 0 and no more than 480000 (=100.0%) documents\n",
      "[2022-09-26 13:51:20,484] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:51:20,546] adding document #480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:51:52,640] discarding 47635 tokens: [('next_to_top', 1), ('precede#1', 1), ('founatin', 1), ('hkclubqvstatue', 1), ('hkhsbc', 1), ('亞歷山大行', 1), ('歷山大廈', 1), ('聖佐治大廈', 1), ('peaktram', 1), ('guanguang', 1)]...\n",
      "[2022-09-26 13:51:52,642] keeping 2000000 tokens which were in no less than 0 and no more than 490000 (=100.0%) documents\n",
      "[2022-09-26 13:51:57,622] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:51:57,712] adding document #490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:52:30,972] discarding 46790 tokens: [('hallawu', 1), ('hénoc', 1), ('hēnōk', 1), ('issaverdens', 1), ('kôkabêl', 1), ('kʷəllū', 1), ('məndābē', 1), ('panopolitanus', 1), ('qenastr', 1), ('qenoch', 1)]...\n",
      "[2022-09-26 13:52:30,974] keeping 2000000 tokens which were in no less than 0 and no more than 500000 (=100.0%) documents\n",
      "[2022-09-26 13:52:34,694] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:52:34,759] adding document #500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:53:07,825] discarding 46647 tokens: [('xxxfamilyxxx', 1), ('plotter#1§', 1), ('vonla', 1), ('santacons', 1), ('santapalooza', 1), ('santarchy', 1), ('abrudului', 1), ('agățat', 1), ('aninei', 1), ('arieş', 1)]...\n",
      "[2022-09-26 13:53:07,827] keeping 2000000 tokens which were in no less than 0 and no more than 510000 (=100.0%) documents\n",
      "[2022-09-26 13:53:12,898] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 13:53:12,989] adding document #510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:53:45,098] discarding 43686 tokens: [('csrkt', 1), ('tramworks', 1), ('achinkinkan', 1), ('anyuyskiy', 1), ('arakamchechen', 1), ('avtatkuul', 1), ('avtonomnyken', 1), ('chengkuul', 1), ('chukotkaken', 1), ('chukotsnab', 1)]...\n",
      "[2022-09-26 13:53:45,100] keeping 2000000 tokens which were in no less than 0 and no more than 520000 (=100.0%) documents\n",
      "[2022-09-26 13:53:48,820] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:53:48,883] adding document #520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:54:19,154] discarding 50134 tokens: [('feejeean', 1), ('katubalevu', 1), ('lakeban', 1), ('lakebans', 1), ('nasaqalau', 1), ('navutoka', 1), ('oneata', 1), ('sikava', 1), ('tikoibau', 1), ('waitui', 1)]...\n",
      "[2022-09-26 13:54:19,156] keeping 2000000 tokens which were in no less than 0 and no more than 530000 (=100.0%) documents\n",
      "[2022-09-26 13:54:22,869] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:54:22,932] adding document #530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:54:54,922] discarding 43859 tokens: [('stellte', 1), ('taylorismus', 1), ('teilautonomer', 1), ('capobianchi', 1), ('carutti', 1), ('catini', 1), ('emboule', 1), ('fakyh', 1), ('ganadu', 1), ('louni', 1)]...\n",
      "[2022-09-26 13:54:54,924] keeping 2000000 tokens which were in no less than 0 and no more than 540000 (=100.0%) documents\n",
      "[2022-09-26 13:55:00,140] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:55:00,231] adding document #540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:55:29,221] discarding 45358 tokens: [('allozelotes', 1), ('amazoromus', 1), ('anagraphis', 1), ('anagrina', 1), ('aneplasa', 1), ('aphantaulax', 1), ('apodrassodes', 1), ('apodrassus', 1), ('apopyllus', 1), ('arauchemus', 1)]...\n",
      "[2022-09-26 13:55:29,222] keeping 2000000 tokens which were in no less than 0 and no more than 550000 (=100.0%) documents\n",
      "[2022-09-26 13:55:32,911] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:55:32,974] adding document #550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:56:03,768] discarding 41357 tokens: [('民友連', 1), ('vaucouleur', 1), ('arnáu', 1), ('comtelsa', 1), ('corresponsales', 1), ('efeagro', 1), ('efecom', 1), ('efefuturo', 1), ('efemotor', 1), ('efesalud', 1)]...\n",
      "[2022-09-26 13:56:03,770] keeping 2000000 tokens which were in no less than 0 and no more than 560000 (=100.0%) documents\n",
      "[2022-09-26 13:56:08,982] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:56:09,071] adding document #560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:56:38,777] discarding 38877 tokens: [('sczcepanik', 1), ('siemiona', 1), ('sirmax', 1), ('sklęczki', 1), ('sklęczkowska', 1), ('stodółki', 1), ('strase', 1), ('szosowa', 1), ('tetralna', 1), ('troczewski', 1)]...\n",
      "[2022-09-26 13:56:38,778] keeping 2000000 tokens which were in no less than 0 and no more than 570000 (=100.0%) documents\n",
      "[2022-09-26 13:56:42,506] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:56:42,569] adding document #570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:57:13,370] discarding 46268 tokens: [('widokiem', 1), ('łaskawej', 1), ('łowiczu', 1), ('amnsterdam', 1), ('chodakow', 1), ('rozlazlow', 1), ('sochaczewianin', 1), ('sochaczewie', 1), ('sochaczewskiej', 1), ('tułowskie', 1)]...\n",
      "[2022-09-26 13:57:13,372] keeping 2000000 tokens which were in no less than 0 and no more than 580000 (=100.0%) documents\n",
      "[2022-09-26 13:57:18,913] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:57:19,005] adding document #580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:57:48,697] discarding 44535 tokens: [('scharlachrennen', 1), ('septemiacum', 1), ('shiganshina', 1), ('stadtmauermuseum', 1), ('taschenfeind', 1), ('teufelstanz', 1), ('unkeuschheit', 1), ('vorfahrin', 1), ('zhvs', 1), ('ciapetti', 1)]...\n",
      "[2022-09-26 13:57:48,699] keeping 2000000 tokens which were in no less than 0 and no more than 590000 (=100.0%) documents\n",
      "[2022-09-26 13:57:52,444] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:57:52,508] adding document #590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:58:24,998] discarding 43728 tokens: [('missending', 1), ('thisai', 1), ('pokhodsk', 1), ('fachhochschulstudiengang', 1), ('fachhochschulstudiengänge', 1), ('fachhochschulstudiengängen', 1), ('gesundheitsberufe', 1), ('viennaer', 1), ('bayyana', 1), ('huercal', 1)]...\n",
      "[2022-09-26 13:58:25,000] keeping 2000000 tokens which were in no less than 0 and no more than 600000 (=100.0%) documents\n",
      "[2022-09-26 13:58:30,165] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:58:30,255] adding document #600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:58:59,000] discarding 41822 tokens: [('西湖鶴山나들목', 1), ('西金海나들목', 1), ('西釜山料金所', 1), ('西釜山나들목', 1), ('西靈巖料金所', 1), ('西靈巖나들목', 1), ('西馬山나들목', 1), ('辰橋나들목', 1), ('進永休憩所', 1), ('進禮나들목', 1)]...\n",
      "[2022-09-26 13:58:59,002] keeping 2000000 tokens which were in no less than 0 and no more than 610000 (=100.0%) documents\n",
      "[2022-09-26 13:59:04,107] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:59:04,196] adding document #610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:59:33,865] discarding 43883 tokens: [('hamick', 1), ('hamickcover', 1), ('drowningwhether', 1), ('outake', 1), ('biscaluz', 1), ('manczareks', 1), ('posae', 1), ('chooseveg', 1), ('hoofdorp', 1), ('sybilvoorschoten', 1)]...\n",
      "[2022-09-26 13:59:33,867] keeping 2000000 tokens which were in no less than 0 and no more than 620000 (=100.0%) documents\n",
      "[2022-09-26 13:59:37,579] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 13:59:37,642] adding document #620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:00:07,449] discarding 42730 tokens: [('parliamenttehran', 1), ('wilayi', 1), ('صد', 1), ('mctutor', 1), ('ontalauna', 1), ('lscb', 1), ('allworlhy', 1), ('burfordcommunitycentrearena', 1), ('burfordgrocerystore', 1), ('burfordontarioroadsign', 1)]...\n",
      "[2022-09-26 14:00:07,451] keeping 2000000 tokens which were in no less than 0 and no more than 630000 (=100.0%) documents\n",
      "[2022-09-26 14:00:11,207] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:00:11,271] adding document #630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:00:41,295] discarding 40389 tokens: [('conechy', 1), ('raheeja', 1), ('eventsquarterly', 1), ('stowce', 1), ('talgang', 1), ('wellesdene', 1), ('wigwell', 1), ('tsann', 1), ('abiant', 1), ('basketballhockey', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:00:41,296] keeping 2000000 tokens which were in no less than 0 and no more than 640000 (=100.0%) documents\n",
      "[2022-09-26 14:00:45,093] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:00:45,156] adding document #640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:01:11,717] discarding 51590 tokens: [('avmaktslave', 1), ('fortapelse', 1), ('sorgens', 1), ('svunnet', 1), ('fluers', 1), ('plissés', 1), ('seasted', 1), ('sursonyachtletoile', 1), ('counselers', 1), ('diseños', 1)]...\n",
      "[2022-09-26 14:01:11,718] keeping 2000000 tokens which were in no less than 0 and no more than 650000 (=100.0%) documents\n",
      "[2022-09-26 14:01:15,631] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:01:15,696] adding document #650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:01:44,145] discarding 40228 tokens: [('gitutyun', 1), ('grakanutyun', 1), ('indjeyans', 1), ('muradpekyan', 1), ('nergaght', 1), ('nurijanian', 1), ('shaginian', 1), ('touryan', 1), ('vahanyan', 1), ('vardanank', 1)]...\n",
      "[2022-09-26 14:01:44,146] keeping 2000000 tokens which were in no less than 0 and no more than 660000 (=100.0%) documents\n",
      "[2022-09-26 14:01:47,904] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:01:47,968] adding document #660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:02:17,518] discarding 39225 tokens: [('seiedy', 1), ('spermophorodrilus', 1), ('tetragonurus', 1), ('zicsi', 1), ('zophoscolex', 1), ('örley', 1), ('itschory', 1), ('itsiursk', 1), ('itsyursk', 1), ('kartasheva', 1)]...\n",
      "[2022-09-26 14:02:17,519] keeping 2000000 tokens which were in no less than 0 and no more than 670000 (=100.0%) documents\n",
      "[2022-09-26 14:02:21,675] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:02:21,737] adding document #670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:02:50,676] discarding 38985 tokens: [('lapidazione', 1), ('breitenbrunner', 1), ('harangozós', 1), ('hohenbüchler', 1), ('kaiserstein', 1), ('kelheimerstein', 1), ('khovanchina', 1), ('nibelungenring', 1), ('opernringhof', 1), ('plattenstein', 1)]...\n",
      "[2022-09-26 14:02:50,679] keeping 2000000 tokens which were in no less than 0 and no more than 680000 (=100.0%) documents\n",
      "[2022-09-26 14:02:56,325] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:02:56,414] adding document #680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:03:27,706] discarding 58852 tokens: [('ardenghesca', 1), ('nobilta', 1), ('fasco', 1), ('turone', 1), ('dėpartement', 1), ('impedit', 1), ('wasserhövel', 1), ('seizure#2§', 1), ('asmis', 1), ('canonic#3', 1)]...\n",
      "[2022-09-26 14:03:27,708] keeping 2000000 tokens which were in no less than 0 and no more than 690000 (=100.0%) documents\n",
      "[2022-09-26 14:03:32,725] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:03:32,817] adding document #690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:04:03,524] discarding 40425 tokens: [('cfiosh', 1), ('cihcm', 1), ('cmareng', 1), ('cmiia', 1), ('cmiosh', 1), ('cmktr', 1), ('cradp', 1), ('csyp', 1), ('executry', 1), ('fcieh', 1)]...\n",
      "[2022-09-26 14:04:03,526] keeping 2000000 tokens which were in no less than 0 and no more than 700000 (=100.0%) documents\n",
      "[2022-09-26 14:04:08,882] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:04:08,971] adding document #700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:04:38,748] discarding 42192 tokens: [('nīshābūr', 1), ('nīshāpūr', 1), ('pirasteh', 1), ('pirouzeh', 1), ('pâyandeh', 1), ('pâyetaxt', 1), ('pâyi', 1), ('qadamgâh', 1), ('rivand', 1), ('rivâs', 1)]...\n",
      "[2022-09-26 14:04:38,751] keeping 2000000 tokens which were in no less than 0 and no more than 710000 (=100.0%) documents\n",
      "[2022-09-26 14:04:44,245] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:04:44,333] adding document #710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:05:17,819] discarding 40273 tokens: [('misdaadnet', 1), ('muziekkiosk', 1), ('nostalgienet', 1), ('schlagertv', 1), ('uitzendinggemist', 1), ('vicetv', 1), ('weerkanaal', 1), ('ibwiri', 1), ('ugarrowwa', 1), ('allowanced', 1)]...\n",
      "[2022-09-26 14:05:17,821] keeping 2000000 tokens which were in no less than 0 and no more than 720000 (=100.0%) documents\n",
      "[2022-09-26 14:05:22,931] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:05:23,020] adding document #720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:05:55,186] discarding 41024 tokens: [('waldkatzenbach', 1), ('anthempress', 1), ('bogomdannoye', 1), ('doukbhobors', 1), ('doukhbors', 1), ('doukhobortsy', 1), ('doukhobory', 1), ('doukhobours', 1), ('edinolichniki', 1), ('evgrafova', 1)]...\n",
      "[2022-09-26 14:05:55,188] keeping 2000000 tokens which were in no less than 0 and no more than 730000 (=100.0%) documents\n",
      "[2022-09-26 14:06:00,361] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:06:00,447] adding document #730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:06:29,077] discarding 38258 tokens: [('gentlemanhog', 1), ('jimwoodring', 1), ('værverk', 1), ('weersomstandigheden', 1), ('feigensons', 1), ('keeptain', 1), ('redpop', 1), ('treesweet', 1), ('jimland', 1), ('musichis', 1)]...\n",
      "[2022-09-26 14:06:29,079] keeping 2000000 tokens which were in no less than 0 and no more than 740000 (=100.0%) documents\n",
      "[2022-09-26 14:06:32,974] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:06:33,035] adding document #740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:07:03,072] discarding 42291 tokens: [('hacheham', 1), ('hornshay', 1), ('jflames', 1), ('massakkah', 1), ('adplumbum', 1), ('denamite', 1), ('nekromaneia', 1), ('weoroscipe', 1), ('alanced', 1), ('btfd', 1)]...\n",
      "[2022-09-26 14:07:03,073] keeping 2000000 tokens which were in no less than 0 and no more than 750000 (=100.0%) documents\n",
      "[2022-09-26 14:07:08,650] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:07:08,741] adding document #750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:07:38,433] discarding 37375 tokens: [('thataka', 1), ('presaude', 1), ('sawahu', 1), ('ω²α', 1), ('akāmadeva', 1), ('balirāja', 1), ('bhaumagupta', 1), ('bhīmārjunadeva', 1), ('bhūmigupta', 1), ('gautamavajra', 1)]...\n",
      "[2022-09-26 14:07:38,436] keeping 2000000 tokens which were in no less than 0 and no more than 760000 (=100.0%) documents\n",
      "[2022-09-26 14:07:43,821] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:07:43,909] adding document #760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:08:12,563] discarding 38524 tokens: [('weiertrass', 1), ('tourtres', 1), ('slimdowns', 1), ('angehand', 1), ('lagneys', 1), ('puelevé', 1), ('ruddelat', 1), ('trotabas', 1), ('shyin', 1), ('bénéden', 1)]...\n",
      "[2022-09-26 14:08:12,565] keeping 2000000 tokens which were in no less than 0 and no more than 770000 (=100.0%) documents\n",
      "[2022-09-26 14:08:18,215] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:08:18,303] adding document #770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:08:47,502] discarding 43545 tokens: [('lekkers', 1), ('lepula', 1), ('letela', 1), ('lhenge', 1), ('luleke', 1), ('makhiya', 1), ('makhume', 1), ('makumembirhi', 1), ('makumemune', 1), ('makumenharhu', 1)]...\n",
      "[2022-09-26 14:08:47,503] keeping 2000000 tokens which were in no less than 0 and no more than 780000 (=100.0%) documents\n",
      "[2022-09-26 14:08:51,371] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:08:51,433] adding document #780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:09:22,361] discarding 40534 tokens: [('äsäyźe', 1), ('äsäyźeng', 1), ('äsäyźä', 1), ('äsäyźän', 1), ('äsäyźär', 1), ('äsäyźärgä', 1), ('äsäyźärźe', 1), ('äsäyźärźeng', 1), ('äsäyźärźä', 1), ('äsäyźärźän', 1)]...\n",
      "[2022-09-26 14:09:22,363] keeping 2000000 tokens which were in no less than 0 and no more than 790000 (=100.0%) documents\n",
      "[2022-09-26 14:09:26,299] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:09:26,361] adding document #790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:09:54,595] discarding 38287 tokens: [('maskouf', 1), ('nerantziá', 1), ('nerántzi', 1), ('nerënxa', 1), ('pomeranssi', 1), ('pomeransskal', 1), ('simach', 1), ('κιτρομηλο', 1), ('cenrtury', 1), ('djinhangir', 1)]...\n",
      "[2022-09-26 14:09:54,597] keeping 2000000 tokens which were in no less than 0 and no more than 800000 (=100.0%) documents\n",
      "[2022-09-26 14:09:59,915] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:10:00,001] adding document #800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:10:29,305] discarding 42477 tokens: [('γερακίου', 1), ('γιάννενα', 1), ('γιάννινα', 1), ('γιαννιτσών', 1), ('γιούχτας', 1), ('γκυζιακός', 1), ('γλυφάδας', 1), ('γουμέρου', 1), ('γραβιάς', 1), ('γρατινής', 1)]...\n",
      "[2022-09-26 14:10:29,307] keeping 2000000 tokens which were in no less than 0 and no more than 810000 (=100.0%) documents\n",
      "[2022-09-26 14:10:34,896] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:10:34,985] adding document #810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:11:02,941] discarding 39617 tokens: [('torsjöån', 1), ('torsmovasseln', 1), ('torvsjöån', 1), ('torvån', 1), ('torån', 1), ('toskbäcken', 1), ('tranebergsälven', 1), ('trankvillsån', 1), ('trinnan', 1), ('trollbosjöån', 1)]...\n",
      "[2022-09-26 14:11:02,942] keeping 2000000 tokens which were in no less than 0 and no more than 820000 (=100.0%) documents\n",
      "[2022-09-26 14:11:06,793] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:11:06,853] adding document #820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:11:36,484] discarding 40080 tokens: [('gåxsjönoret', 1), ('görjeån', 1), ('görslövsån', 1), ('götån', 1), ('habbestorpebäcken', 1), ('haddängsån', 1), ('hagaån', 1), ('hagbyån', 1), ('haggeån', 1), ('hagälven', 1)]...\n",
      "[2022-09-26 14:11:36,485] keeping 2000000 tokens which were in no less than 0 and no more than 830000 (=100.0%) documents\n",
      "[2022-09-26 14:11:40,292] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:11:40,355] adding document #830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:12:11,590] discarding 42259 tokens: [('labuje', 1), ('lukoya', 1), ('mangasaba', 1), ('mangbangu', 1), ('nangume', 1), ('ojul', 1), ('pincywa', 1), ('zembio', 1), ('affabee', 1), ('dragonwalk', 1)]...\n",
      "[2022-09-26 14:12:11,592] keeping 2000000 tokens which were in no less than 0 and no more than 840000 (=100.0%) documents\n",
      "[2022-09-26 14:12:15,522] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:12:15,583] adding document #840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:12:40,834] discarding 39247 tokens: [('aylettes', 1), ('hemiobols', 1), ('systam', 1), ('σίγλος', 1), ('中国造币', 1), ('钱币鉴赏', 1), ('giacometto', 1), ('microbomb', 1), ('nicolasin', 1), ('aviathar', 1)]...\n",
      "[2022-09-26 14:12:40,836] keeping 2000000 tokens which were in no less than 0 and no more than 850000 (=100.0%) documents\n",
      "[2022-09-26 14:12:44,868] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:12:44,930] adding document #850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:13:12,130] discarding 36274 tokens: [('myestny', 1), ('odernizirovannyi', 1), ('odernizirovannyy', 1), ('omercheskiy', 1), ('palubnyy', 1), ('renirovochnyy', 1), ('sarabeyev', 1), ('shturmovaya', 1), ('sukhoi_su', 1), ('sviryidov', 1)]...\n",
      "[2022-09-26 14:13:12,132] keeping 2000000 tokens which were in no less than 0 and no more than 860000 (=100.0%) documents\n",
      "[2022-09-26 14:13:16,093] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:13:16,154] adding document #860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:13:47,236] discarding 43922 tokens: [('λίγο', 1), ('λοϊζο', 1), ('λούλου', 1), ('μinos', 1), ('μάνο', 1), ('μαγιακόβσκη', 1), ('μακρόνησο', 1), ('μικρούτσικο', 1), ('μικρούτσικος', 1), ('μικρούτσικου', 1)]...\n",
      "[2022-09-26 14:13:47,238] keeping 2000000 tokens which were in no less than 0 and no more than 870000 (=100.0%) documents\n",
      "[2022-09-26 14:13:51,100] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:13:51,164] adding document #870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:14:21,362] discarding 36050 tokens: [('ariprand', 1), ('brunnlehner', 1), ('ernstand', 1), ('ligrary', 1), ('psauna', 1), ('datatheft', 1), ('congaline', 1), ('fedounoum', 1), ('hypasounds', 1), ('inckle', 1)]...\n",
      "[2022-09-26 14:14:21,364] keeping 2000000 tokens which were in no less than 0 and no more than 880000 (=100.0%) documents\n",
      "[2022-09-26 14:14:26,596] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:14:26,683] adding document #880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:14:56,908] discarding 37515 tokens: [('𑀏𑀕', 1), ('𑀘𑀢', 1), ('𑀡𑀯', 1), ('𑀢𑀑', 1), ('𑀤𑀲', 1), ('𑀦𑀯', 1), ('𑀲𑀇', 1), ('𑀲𑀢', 1), ('𗍫', 1), ('𗏁', 1)]...\n",
      "[2022-09-26 14:14:56,911] keeping 2000000 tokens which were in no less than 0 and no more than 890000 (=100.0%) documents\n",
      "[2022-09-26 14:15:02,206] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:15:02,295] adding document #890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:15:29,408] discarding 39990 tokens: [('يەتتە', 1), ('ٺ', 1), ('ٻار', 1), ('ٻہ', 1), ('ٽي', 1), ('پانزده', 1), ('پنځلس', 1), ('پنځه', 1), ('پنچ', 1), ('څلور', 1)]...\n",
      "[2022-09-26 14:15:29,409] keeping 2000000 tokens which were in no less than 0 and no more than 900000 (=100.0%) documents\n",
      "[2022-09-26 14:15:33,223] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:15:33,284] adding document #900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:16:02,051] discarding 35334 tokens: [('ȶʰan', 1), ('ɑltə', 1), ('ɑltɨ', 1), ('ɑltɯ', 1), ('ɑltʃi', 1), ('ɓelexep', 1), ('ɓuai', 1), ('ɔˀn', 1), ('ɕaɯ', 1), ('ɕet', 1)]...\n",
      "[2022-09-26 14:16:02,053] keeping 2000000 tokens which were in no less than 0 and no more than 910000 (=100.0%) documents\n",
      "[2022-09-26 14:16:07,512] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:16:07,601] adding document #910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:16:37,837] discarding 35753 tokens: [('tawaniyuq', 1), ('ta¹¹', 1), ('taō', 1), ('taɫimän', 1), ('taɫɫimat', 1), ('taˈfuːl', 1), ('taˈnɨj', 1), ('tbawz', 1), ('tchiglitun', 1), ('tchinze', 1)]...\n",
      "[2022-09-26 14:16:37,840] keeping 2000000 tokens which were in no less than 0 and no more than 920000 (=100.0%) documents\n",
      "[2022-09-26 14:16:43,240] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:16:43,328] adding document #920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:17:12,069] discarding 42396 tokens: [('ləɯ', 1), ('ləʔ', 1), ('ləˈkʰɔ', 1), ('ləˈŋâj', 1), ('lɛiŋ', 1), ('lɛjŋ', 1), ('lɛmaːʔ', 1), ('lɛnga', 1), ('lɛyi', 1), ('lɛŋ', 1)]...\n",
      "[2022-09-26 14:17:12,070] keeping 2000000 tokens which were in no less than 0 and no more than 930000 (=100.0%) documents\n",
      "[2022-09-26 14:17:15,850] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:17:15,912] adding document #930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:17:43,421] discarding 38867 tokens: [('dʑirɢoon', 1), ('dʑiɪp', 1), ('dʑʱa', 1), ('dʷɑzdʌ', 1), ('dˤiv', 1), ('dеsętĭ', 1), ('dḗz', 1), ('dḱm', 1), ('dṓdeka', 1), ('dọs', 1)]...\n",
      "[2022-09-26 14:17:43,423] keeping 2000000 tokens which were in no less than 0 and no more than 940000 (=100.0%) documents\n",
      "[2022-09-26 14:17:48,676] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:17:48,767] adding document #940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:18:18,631] discarding 54265 tokens: [('quirinsweiler', 1), ('schweizerlaendel', 1), ('sotzeling', 1), ('stuckange', 1), ('terwen', 1), ('untermuhlthal', 1), ('wölferdingen', 1), ('linkenheil', 1), ('ajanci', 1), ('bomawa', 1)]...\n",
      "[2022-09-26 14:18:18,633] keeping 2000000 tokens which were in no less than 0 and no more than 950000 (=100.0%) documents\n",
      "[2022-09-26 14:18:24,242] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:18:24,336] adding document #950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:18:51,998] discarding 42591 tokens: [('he田和', 1), ('huan環', 1), ('jing靜', 1), ('ji呂伋', 1), ('ji積', 1), ('lufu祿甫', 1), ('pan潘', 1), ('ren壬', 1), ('shangren商人', 1), ('shang呂尚', 1)]...\n",
      "[2022-09-26 14:18:52,000] keeping 2000000 tokens which were in no less than 0 and no more than 960000 (=100.0%) documents\n",
      "[2022-09-26 14:18:57,751] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:18:57,840] adding document #960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:19:28,534] discarding 44941 tokens: [('стремя', 1), ('стремянные', 1), ('ayubkhel', 1), ('dramaticules', 1), ('attaqué', 1), ('berglandschaft', 1), ('johntwachtman', 1), ('profounds', 1), ('unword', 1), ('whitenights', 1)]...\n",
      "[2022-09-26 14:19:28,537] keeping 2000000 tokens which were in no less than 0 and no more than 970000 (=100.0%) documents\n",
      "[2022-09-26 14:19:33,238] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:19:33,300] adding document #970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:20:00,062] discarding 38016 tokens: [('arambruru', 1), ('chispistas', 1), ('cuaretta', 1), ('duschatsky', 1), ('ingallinella', 1), ('penelonistas', 1), ('robopsychologists', 1), ('robospychology', 1), ('dispassionately#0', 1), ('konsertföreningen', 1)]...\n",
      "[2022-09-26 14:20:00,064] keeping 2000000 tokens which were in no less than 0 and no more than 980000 (=100.0%) documents\n",
      "[2022-09-26 14:20:04,137] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:20:04,200] adding document #980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:20:32,839] discarding 44121 tokens: [('kųų', 1), ('kǫǫʼ', 1), ('kǫʼ', 1), ('kʼis', 1), ('kʼisé', 1), ('lipiyanes', 1), ('láh', 1), ('ląh', 1), ('mááʼ', 1), ('nadeicha', 1)]...\n",
      "[2022-09-26 14:20:32,841] keeping 2000000 tokens which were in no less than 0 and no more than 990000 (=100.0%) documents\n",
      "[2022-09-26 14:20:38,222] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:20:38,313] adding document #990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:21:03,874] discarding 49025 tokens: [('ogini', 1), ('oosíyo', 1), ('sagonige', 1), ('sagonigei', 1), ('saquu', 1), ('saàgʷu', 1), ('soneladu', 1), ('syllabicy', 1), ('talidu', 1), ('talineiga', 1)]...\n",
      "[2022-09-26 14:21:03,876] keeping 2000000 tokens which were in no less than 0 and no more than 1000000 (=100.0%) documents\n",
      "[2022-09-26 14:21:07,819] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:21:07,883] adding document #1000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:21:38,996] discarding 45806 tokens: [('tunghsin', 1), ('tungmin', 1), ('xīnfù', 1), ('xīnguāng', 1), ('xīxīn', 1), ('yùchéng', 1), ('yüch', 1), ('zhōngnán', 1), ('zhōngyán', 1), ('hangchengyuan', 1)]...\n",
      "[2022-09-26 14:21:38,998] keeping 2000000 tokens which were in no less than 0 and no more than 1010000 (=100.0%) documents\n",
      "[2022-09-26 14:21:44,376] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:21:44,470] adding document #1010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:22:15,310] discarding 39513 tokens: [('camminetti', 1), ('paleogravel', 1), ('psychnews', 1), ('corporeagraphic', 1), ('dmytriieva', 1), ('honeybuzzard', 1), ('katsavos', 1), ('kérchy', 1), ('tatcheva', 1), ('consimilarity', 1)]...\n",
      "[2022-09-26 14:22:15,312] keeping 2000000 tokens which were in no less than 0 and no more than 1020000 (=100.0%) documents\n",
      "[2022-09-26 14:22:20,531] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:22:20,593] adding document #1020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:22:46,292] discarding 34446 tokens: [('конькобежцы', 1), ('кошеверова', 1), ('лирический', 1), ('пригласить', 1), ('разрешите', 1), ('славутянка', 1), ('стриганов', 1), ('танце', 1), ('туяна', 1), ('фигурный', 1)]...\n",
      "[2022-09-26 14:22:46,294] keeping 2000000 tokens which were in no less than 0 and no more than 1030000 (=100.0%) documents\n",
      "[2022-09-26 14:22:50,175] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:22:50,237] adding document #1030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:23:17,835] discarding 36120 tokens: [('giflez', 1), ('giflés', 1), ('gillimer', 1), ('girfles', 1), ('girflez', 1), ('girlflet', 1), ('gohort', 1), ('gorhaut', 1), ('gornemans', 1), ('gosenain', 1)]...\n",
      "[2022-09-26 14:23:17,837] keeping 2000000 tokens which were in no less than 0 and no more than 1040000 (=100.0%) documents\n",
      "[2022-09-26 14:23:22,655] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:23:22,724] adding document #1040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:23:50,512] discarding 34162 tokens: [('rangiahuta', 1), ('tūpoho', 1), ('vostinar', 1), ('funkitude', 1), ('funknology', 1), ('graddie', 1), ('mercernary', 1), ('sippiana', 1), ('skinji', 1), ('firstvision', 1)]...\n",
      "[2022-09-26 14:23:50,515] keeping 2000000 tokens which were in no less than 0 and no more than 1050000 (=100.0%) documents\n",
      "[2022-09-26 14:23:55,792] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:23:55,882] adding document #1050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:24:22,308] discarding 36005 tokens: [('pennsans', 1), ('pensande', 1), ('pensanns', 1), ('pensaunce', 1), ('polwithen', 1), ('portmynster', 1), ('quicumq', 1), ('raffidy', 1), ('tredarvah', 1), ('burzliwe', 1)]...\n",
      "[2022-09-26 14:24:22,311] keeping 2000000 tokens which were in no less than 0 and no more than 1060000 (=100.0%) documents\n",
      "[2022-09-26 14:24:27,625] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:24:27,715] adding document #1060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:24:53,912] discarding 33543 tokens: [('図日本文化史大系', 1), ('白川郡', 1), ('kitaiwate', 1), ('minamiiwate', 1), ('minamikunohe', 1), ('上閉伊郡', 1), ('下閉伊郡', 1), ('中閉伊郡', 1), ('九戸郡', 1), ('北九戸郡', 1)]...\n",
      "[2022-09-26 14:24:53,914] keeping 2000000 tokens which were in no less than 0 and no more than 1070000 (=100.0%) documents\n",
      "[2022-09-26 14:24:59,251] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:24:59,342] adding document #1070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:25:26,284] discarding 38026 tokens: [('kazada', 1), ('mosandi', 1), ('teice', 1), ('yarışçı', 1), ('pharroh', 1), ('sonifly', 1), ('bodycostume', 1), ('dillonites', 1), ('blochius', 1), ('bolcaensis', 1)]...\n",
      "[2022-09-26 14:25:26,287] keeping 2000000 tokens which were in no less than 0 and no more than 1080000 (=100.0%) documents\n",
      "[2022-09-26 14:25:31,543] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:25:31,633] adding document #1080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:25:58,841] discarding 38251 tokens: [('showandtellmucic', 1), ('chaoism', 1), ('eismagie', 1), ('nesán', 1), ('seangan', 1), ('bizspace', 1), ('fontigny', 1), ('mediterraneanhe', 1), ('heteronormalized', 1), ('iranrevolution', 1)]...\n",
      "[2022-09-26 14:25:58,842] keeping 2000000 tokens which were in no less than 0 and no more than 1090000 (=100.0%) documents\n",
      "[2022-09-26 14:26:04,094] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:26:04,186] adding document #1090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:26:29,364] discarding 35593 tokens: [('warrenite', 1), ('hkej', 1), ('newsjournal', 1), ('cotolerant', 1), ('taueki', 1), ('hkibc', 1), ('nabela', 1), ('qoser', 1), ('大自然大不同', 1), ('anglicitis', 1)]...\n",
      "[2022-09-26 14:26:29,366] keeping 2000000 tokens which were in no less than 0 and no more than 1100000 (=100.0%) documents\n",
      "[2022-09-26 14:26:34,742] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:26:34,832] adding document #1100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:27:02,816] discarding 35578 tokens: [('schattenreichs', 1), ('schwarwel', 1), ('schäumende', 1), ('stimmungshits', 1), ('wäste', 1), ('zeidverschwändung', 1), ('zuschauercomet', 1), ('überdimensionales', 1), ('equestrian#1§', 1), ('tsonjin', 1)]...\n",
      "[2022-09-26 14:27:02,818] keeping 2000000 tokens which were in no less than 0 and no more than 1110000 (=100.0%) documents\n",
      "[2022-09-26 14:27:06,690] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:27:06,749] adding document #1110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:27:33,483] discarding 34116 tokens: [('ravendra', 1), ('sabheral', 1), ('tifacs', 1), ('pierfelice', 1), ('亞洲電視', 1), ('麗的呼聲', 1), ('mcscotty', 1), ('fungerburg', 1), ('marineheim', 1), ('obsersalzberg', 1)]...\n",
      "[2022-09-26 14:27:33,485] keeping 2000000 tokens which were in no less than 0 and no more than 1120000 (=100.0%) documents\n",
      "[2022-09-26 14:27:39,056] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:27:39,141] adding document #1120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:28:07,790] discarding 35752 tokens: [('شحات', 1), ('صبراتة', 1), ('مسلاتة', 1), ('gratčinja', 1), ('гратчиња', 1), ('afovoany', 1), ('ambenja', 1), ('ambohimahamasina', 1), ('antsirambazaha', 1), ('faradofay', 1)]...\n",
      "[2022-09-26 14:28:07,792] keeping 2000000 tokens which were in no less than 0 and no more than 1130000 (=100.0%) documents\n",
      "[2022-09-26 14:28:11,705] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:28:11,766] adding document #1130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:28:40,041] discarding 37279 tokens: [('karolińska', 1), ('karolińskiej', 1), ('králiček', 1), ('linaa', 1), ('lucolane', 1), ('meißnischen', 1), ('nazw', 1), ('neriuani', 1), ('niewłaściwie', 1), ('nowszym', 1)]...\n",
      "[2022-09-26 14:28:40,043] keeping 2000000 tokens which were in no less than 0 and no more than 1140000 (=100.0%) documents\n",
      "[2022-09-26 14:28:45,313] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:28:45,398] adding document #1140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:29:10,989] discarding 36085 tokens: [('laitali', 1), ('centavia', 1), ('awsi', 1), ('gabillema', 1), ('smoothback', 1), ('akyrz', 1), ('molimerx', 1), ('wherner', 1), ('banruo', 1), ('gaoxiao', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:29:10,991] keeping 2000000 tokens which were in no less than 0 and no more than 1150000 (=100.0%) documents\n",
      "[2022-09-26 14:29:15,175] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:29:15,244] adding document #1150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:29:42,612] discarding 41197 tokens: [('bomball', 1), ('cybersec', 1), ('interyear', 1), ('madr', 1), ('veccs', 1), ('yoft', 1), ('drycry', 1), ('feedbin', 1), ('kaatha', 1), ('karuppaayi', 1)]...\n",
      "[2022-09-26 14:29:42,614] keeping 2000000 tokens which were in no less than 0 and no more than 1160000 (=100.0%) documents\n",
      "[2022-09-26 14:29:47,888] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:29:47,975] adding document #1160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:30:15,022] discarding 33949 tokens: [('akinya', 1), ('holoships', 1), ('tantors', 1), ('awilum', 1), ('bottero', 1), ('herrenschmidt', 1), ('zabbal', 1), ('milburnie', 1), ('trianglemtb', 1), ('maezano', 1)]...\n",
      "[2022-09-26 14:30:15,024] keeping 2000000 tokens which were in no less than 0 and no more than 1170000 (=100.0%) documents\n",
      "[2022-09-26 14:30:20,353] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:30:20,445] adding document #1170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:30:47,688] discarding 38947 tokens: [('aciliorum', 1), ('magnentium', 1), ('modificationsentitled', 1), ('tirrania', 1), ('kuhlthau', 1), ('cfnt', 1), ('chwalisz', 1), ('dølvik', 1), ('feps', 1), ('machnig', 1)]...\n",
      "[2022-09-26 14:30:47,689] keeping 2000000 tokens which were in no less than 0 and no more than 1180000 (=100.0%) documents\n",
      "[2022-09-26 14:30:51,564] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:30:51,630] adding document #1180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:31:19,460] discarding 42605 tokens: [('obluchenskoye', 1), ('obluchensky', 1), ('zutary', 1), ('glowber', 1), ('barcinonensi', 1), ('razafindratsira', 1), ('chadong', 1), ('chadongch', 1), ('chinhŭng', 1), ('chŏndong', 1)]...\n",
      "[2022-09-26 14:31:19,462] keeping 2000000 tokens which were in no less than 0 and no more than 1190000 (=100.0%) documents\n",
      "[2022-09-26 14:31:25,087] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:31:25,180] adding document #1190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:31:55,082] discarding 33968 tokens: [('topcontent', 1), ('citylincoln', 1), ('starkwether', 1), ('zerviah', 1), ('ambowodé', 1), ('longoudé', 1), ('odame', 1), ('heikegani', 1), ('helmkrab', 1), ('balix', 1)]...\n",
      "[2022-09-26 14:31:55,084] keeping 2000000 tokens which were in no less than 0 and no more than 1200000 (=100.0%) documents\n",
      "[2022-09-26 14:32:00,462] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:32:00,555] adding document #1200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:32:27,716] discarding 35461 tokens: [('chicanology', 1), ('litlefield', 1), ('rowmanlittlefield', 1), ('tvnewser', 1), ('mindelwitz', 1), ('harrasment', 1), ('melodaires', 1), ('wigens', 1), ('xentos', 1), ('westerlywordmatters', 1)]...\n",
      "[2022-09-26 14:32:27,718] keeping 2000000 tokens which were in no less than 0 and no more than 1210000 (=100.0%) documents\n",
      "[2022-09-26 14:32:31,636] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:32:31,701] adding document #1210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:32:59,772] discarding 36112 tokens: [('huelyn', 1), ('drukplaten', 1), ('dubiez', 1), ('sunnymoon', 1), ('supertracks', 1), ('ledgards', 1), ('youhill', 1), ('abigailbai', 1), ('bhonker', 1), ('khamasa', 1)]...\n",
      "[2022-09-26 14:32:59,774] keeping 2000000 tokens which were in no less than 0 and no more than 1220000 (=100.0%) documents\n",
      "[2022-09-26 14:33:05,257] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:33:05,351] adding document #1220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:33:33,386] discarding 32269 tokens: [('gameza', 1), ('rickyamadour', 1), ('liistro', 1), ('ecomm', 1), ('onesynthetic', 1), ('riulf', 1), ('soapery', 1), ('clariano', 1), ('karamokoba', 1), ('metallarius', 1)]...\n",
      "[2022-09-26 14:33:33,388] keeping 2000000 tokens which were in no less than 0 and no more than 1230000 (=100.0%) documents\n",
      "[2022-09-26 14:33:37,281] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:33:37,345] adding document #1230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:34:04,369] discarding 33064 tokens: [('ckhs', 1), ('amaugou', 1), ('bellators', 1), ('browarski', 1), ('cedenblad', 1), ('constandache', 1), ('haciani', 1), ('maiquel', 1), ('mastioli', 1), ('mortelette', 1)]...\n",
      "[2022-09-26 14:34:04,371] keeping 2000000 tokens which were in no less than 0 and no more than 1240000 (=100.0%) documents\n",
      "[2022-09-26 14:34:09,660] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:34:09,754] adding document #1240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:34:36,876] discarding 34942 tokens: [('fmca', 1), ('giełczyńska', 1), ('hakehilot', 1), ('woziwodzka', 1), ('zatylna', 1), ('nauseb', 1), ('elongatedand', 1), ('phansianelle', 1), ('phasianellidae', 1), ('bcbusiness', 1)]...\n",
      "[2022-09-26 14:34:36,878] keeping 2000000 tokens which were in no less than 0 and no more than 1250000 (=100.0%) documents\n",
      "[2022-09-26 14:34:40,755] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:34:40,819] adding document #1250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:35:10,309] discarding 36047 tokens: [('brawerman', 1), ('fivepoint', 1), ('alfredalumni', 1), ('brodieco', 1), ('calendar_show', 1), ('ceramicstoday', 1), ('chitton', 1), ('fultonstreetgallery', 1), ('home_main', 1), ('lattitudegallery', 1)]...\n",
      "[2022-09-26 14:35:10,311] keeping 2000000 tokens which were in no less than 0 and no more than 1260000 (=100.0%) documents\n",
      "[2022-09-26 14:35:15,965] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:35:16,060] adding document #1260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:35:45,686] discarding 39388 tokens: [('berrondo', 1), ('biliardoweb', 1), ('birilli', 1), ('birillo', 1), ('boccette', 1), ('cavazzana', 1), ('cifalà', 1), ('contentitem', 1), ('fibis', 1), ('gualemi', 1)]...\n",
      "[2022-09-26 14:35:45,688] keeping 2000000 tokens which were in no less than 0 and no more than 1270000 (=100.0%) documents\n",
      "[2022-09-26 14:35:49,577] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:35:49,642] adding document #1270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:36:18,531] discarding 35399 tokens: [('diferencijalne', 1), ('fizici', 1), ('geometrije', 1), ('kvantnoj', 1), ('nerelativističkoj', 1), ('opšte', 1), ('simetriji', 1), ('дамњановић', 1), ('activolcans', 1), ('modvolc', 1)]...\n",
      "[2022-09-26 14:36:18,533] keeping 2000000 tokens which were in no less than 0 and no more than 1280000 (=100.0%) documents\n",
      "[2022-09-26 14:36:22,415] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:36:22,480] adding document #1280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:36:50,114] discarding 42281 tokens: [('cegos', 1), ('namorada', 1), ('popô', 1), ('prefiro', 1), ('ségio', 1), ('vestidos', 1), ('violência', 1), ('eye_problem', 1), ('hinu', 1), ('milogardner', 1)]...\n",
      "[2022-09-26 14:36:50,116] keeping 2000000 tokens which were in no less than 0 and no more than 1290000 (=100.0%) documents\n",
      "[2022-09-26 14:36:54,150] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:36:54,216] adding document #1290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:37:21,478] discarding 31121 tokens: [('addinston', 1), ('ugston', 1), ('struttergear', 1), ('zpittvar', 1), ('brylski', 1), ('burroweed', 1), ('grossenheider', 1), ('pencillatus', 1), ('scrub#3§', 1), ('prothermolysin', 1)]...\n",
      "[2022-09-26 14:37:21,480] keeping 2000000 tokens which were in no less than 0 and no more than 1300000 (=100.0%) documents\n",
      "[2022-09-26 14:37:26,577] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:37:26,642] adding document #1300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:37:53,754] discarding 34681 tokens: [('abdulagadir', 1), ('alembekova', 1), ('bahmad', 1), ('bakheet', 1), ('bisluke', 1), ('bledman', 1), ('bryshon', 1), ('chemning', 1), ('cleiton', 1), ('deiac', 1)]...\n",
      "[2022-09-26 14:37:53,756] keeping 2000000 tokens which were in no less than 0 and no more than 1310000 (=100.0%) documents\n",
      "[2022-09-26 14:37:59,088] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:37:59,182] adding document #1310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:38:26,772] discarding 36142 tokens: [('vpln', 1), ('guluband', 1), ('mithoo', 1), ('phulan', 1), ('xhush', 1), ('achiola', 1), ('beikos', 1), ('museumin', 1), ('myede', 1), ('notesa', 1)]...\n",
      "[2022-09-26 14:38:26,774] keeping 2000000 tokens which were in no less than 0 and no more than 1320000 (=100.0%) documents\n",
      "[2022-09-26 14:38:32,049] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:38:32,142] adding document #1320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:38:58,347] discarding 33428 tokens: [('sulatycky', 1), ('eilzabeth', 1), ('hinchingbroke', 1), ('jeslyn', 1), ('abscheid', 1), ('taisui', 1), ('楊任', 1), ('甲子太歲之神', 1), ('arliano', 1), ('burlamacca', 1)]...\n",
      "[2022-09-26 14:38:58,349] keeping 2000000 tokens which were in no less than 0 and no more than 1330000 (=100.0%) documents\n",
      "[2022-09-26 14:39:02,356] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:39:02,422] adding document #1330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:39:30,759] discarding 37666 tokens: [('elphnstone', 1), ('gauto', 1), ('ibérique', 1), ('iscovich', 1), ('lebonian', 1), ('lestingi', 1), ('lomianto', 1), ('pontrémoli', 1), ('rolandelli', 1), ('smudt', 1)]...\n",
      "[2022-09-26 14:39:30,761] keeping 2000000 tokens which were in no less than 0 and no more than 1340000 (=100.0%) documents\n",
      "[2022-09-26 14:39:36,052] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:39:36,147] adding document #1340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:40:05,083] discarding 36539 tokens: [('cortesina', 1), ('mccolman', 1), ('amaruka', 1), ('amarukaśataka', 1), ('amarusataka', 1), ('amarusatakam', 1), ('amarushataka', 1), ('amaruśataka', 1), ('devadhar', 1), ('ravichandra', 1)]...\n",
      "[2022-09-26 14:40:05,084] keeping 2000000 tokens which were in no less than 0 and no more than 1350000 (=100.0%) documents\n",
      "[2022-09-26 14:40:09,149] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:40:09,215] adding document #1350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:40:37,609] discarding 36867 tokens: [('czeydner', 1), ('ermil', 1), ('feketehalmy', 1), ('fioravanzo', 1), ('gorondy', 1), ('jány', 1), ('kisbarnak', 1), ('nemzetvezető', 1), ('rattanakun', 1), ('reichmarshall', 1)]...\n",
      "[2022-09-26 14:40:37,610] keeping 2000000 tokens which were in no less than 0 and no more than 1360000 (=100.0%) documents\n",
      "[2022-09-26 14:40:42,935] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:40:43,032] adding document #1360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:41:12,976] discarding 40843 tokens: [('apmamman', 1), ('bielich', 1), ('crossmahon', 1), ('mluver', 1), ('resilire', 1), ('unddr', 1), ('wcdr', 1), ('decimazione', 1), ('kallstroem', 1), ('nuidheacht', 1)]...\n",
      "[2022-09-26 14:41:12,979] keeping 2000000 tokens which were in no less than 0 and no more than 1370000 (=100.0%) documents\n",
      "[2022-09-26 14:41:18,393] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:41:18,488] adding document #1370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:41:46,419] discarding 34754 tokens: [('cantalonia', 1), ('milldred', 1), ('ourdigitalworld', 1), ('geitsætri', 1), ('leirdalen', 1), ('storgjuvtinden', 1), ('storjuvbrean', 1), ('storjuvet', 1), ('storjuvtinden', 1), ('transcddj', 1)]...\n",
      "[2022-09-26 14:41:46,421] keeping 2000000 tokens which were in no less than 0 and no more than 1380000 (=100.0%) documents\n",
      "[2022-09-26 14:41:50,324] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:41:50,393] adding document #1380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:42:17,285] discarding 36597 tokens: [('senthamil', 1), ('thillainathan', 1), ('zabrina', 1), ('amerex', 1), ('frostline', 1), ('hotohara', 1), ('kesshitai', 1), ('kyaeen', 1), ('miyasako', 1), ('rodoplphe', 1)]...\n",
      "[2022-09-26 14:42:17,286] keeping 2000000 tokens which were in no less than 0 and no more than 1390000 (=100.0%) documents\n",
      "[2022-09-26 14:42:21,282] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:42:21,348] adding document #1390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:42:49,727] discarding 34986 tokens: [('downarowicz', 1), ('kiernik', 1), ('kierownik', 1), ('smólski', 1), ('bródáin', 1), ('dancity', 1), ('deachunter', 1), ('electrosleep', 1), ('eyott', 1), ('rhasaan', 1)]...\n",
      "[2022-09-26 14:42:49,729] keeping 2000000 tokens which were in no less than 0 and no more than 1400000 (=100.0%) documents\n",
      "[2022-09-26 14:42:54,679] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:42:54,775] adding document #1400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:43:23,943] discarding 36607 tokens: [('egīls', 1), ('kinyor', 1), ('kocuvan', 1), ('kucej', 1), ('tēbelis', 1), ('zadoynov', 1), ('lignière', 1), ('asaeqw', 1), ('axskb', 1), ('hennerton', 1)]...\n",
      "[2022-09-26 14:43:23,944] keeping 2000000 tokens which were in no less than 0 and no more than 1410000 (=100.0%) documents\n",
      "[2022-09-26 14:43:27,890] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:43:27,956] adding document #1410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:43:57,375] discarding 32022 tokens: [('hirshel', 1), ('elmsn', 1), ('blasenschnecken', 1), ('elodea§', 1), ('gemisto', 1), ('lithoglyphus', 1), ('physcella', 1), ('physidae', 1), ('phytobentos', 1), ('uktag', 1)]...\n",
      "[2022-09-26 14:43:57,377] keeping 2000000 tokens which were in no less than 0 and no more than 1420000 (=100.0%) documents\n",
      "[2022-09-26 14:44:02,655] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:44:02,749] adding document #1420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:44:31,671] discarding 35670 tokens: [('dooryard#0', 1), ('hocktideh', 1), ('boutviseth', 1), ('kirisute', 1), ('mackfall', 1), ('usacti', 1), ('usharddigi', 1), ('bandmastership', 1), ('lachryis', 1), ('willowbye', 1)]...\n",
      "[2022-09-26 14:44:31,673] keeping 2000000 tokens which were in no less than 0 and no more than 1430000 (=100.0%) documents\n",
      "[2022-09-26 14:44:35,846] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:44:35,911] adding document #1430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:45:03,740] discarding 35098 tokens: [('argelliers', 1), ('arnautz', 1), ('ataüt', 1), ('corbatàs', 1), ('cucunhan', 1), ('maucòr', 1), ('mètge', 1), ('nuòch', 1), ('pietat', 1), ('secrèt', 1)]...\n",
      "[2022-09-26 14:45:03,743] keeping 2000000 tokens which were in no less than 0 and no more than 1440000 (=100.0%) documents\n",
      "[2022-09-26 14:45:09,139] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:45:09,233] adding document #1440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:45:35,584] discarding 32681 tokens: [('uhtrex', 1), ('gcvt', 1), ('subtilise', 1), ('azoarcus', 1), ('exog', 1), ('hegs', 1), ('maturases', 1), ('josyln', 1), ('borsodban', 1), ('cserehát', 1)]...\n",
      "[2022-09-26 14:45:35,586] keeping 2000000 tokens which were in no less than 0 and no more than 1450000 (=100.0%) documents\n",
      "[2022-09-26 14:45:40,896] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:45:40,991] adding document #1450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:46:04,407] discarding 30098 tokens: [('bfnc', 1), ('cebron', 1), ('gourgéennes', 1), ('gourgéens', 1), ('heptadeca', 1), ('mycocentrosporina', 1), ('stafon', 1), ('acrocylindrium', 1), ('caerulens', 1), ('cerulein', 1)]...\n",
      "[2022-09-26 14:46:04,409] keeping 2000000 tokens which were in no less than 0 and no more than 1460000 (=100.0%) documents\n",
      "[2022-09-26 14:46:09,800] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:46:09,895] adding document #1460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:46:39,356] discarding 37280 tokens: [('federweisser', 1), ('felsenmühle', 1), ('gnadenmutter', 1), ('goldbordierter', 1), ('kranker', 1), ('mehrgenerationenhof', 1), ('nachsorge', 1), ('zunftbaum', 1), ('fennister', 1), ('burnells', 1)]...\n",
      "[2022-09-26 14:46:39,359] keeping 2000000 tokens which were in no less than 0 and no more than 1470000 (=100.0%) documents\n",
      "[2022-09-26 14:46:44,772] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:46:44,868] adding document #1470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:47:13,620] discarding 32477 tokens: [('etablisements', 1), ('polynesie', 1), ('établisements', 1), ('liberpluscarden', 1), ('macgreagor', 1), ('pluscardensis', 1), ('ophirville', 1), ('bronchorst', 1), ('catalijntje', 1), ('muzikaal', 1)]...\n",
      "[2022-09-26 14:47:13,622] keeping 2000000 tokens which were in no less than 0 and no more than 1480000 (=100.0%) documents\n",
      "[2022-09-26 14:47:19,495] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:47:19,590] adding document #1480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:47:46,750] discarding 32244 tokens: [('boysag', 1), ('cheyava', 1), ('nankwoeap', 1), ('shinumo', 1), ('tuckup', 1), ('carcone', 1), ('giannoccaro', 1), ('machitski', 1), ('sarafree', 1), ('stepec', 1)]...\n",
      "[2022-09-26 14:47:46,752] keeping 2000000 tokens which were in no less than 0 and no more than 1490000 (=100.0%) documents\n",
      "[2022-09-26 14:47:52,079] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:47:52,173] adding document #1490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:48:19,981] discarding 37192 tokens: [('trabalón', 1), ('lekstraat', 1), ('obrechtplein', 1), ('tesjoengat', 1), ('enguia', 1), ('alcances', 1), ('mirrab', 1), ('tulsibai', 1), ('houliao', 1), ('繽紛澎湖', 1)]...\n",
      "[2022-09-26 14:48:19,983] keeping 2000000 tokens which were in no less than 0 and no more than 1500000 (=100.0%) documents\n",
      "[2022-09-26 14:48:24,468] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:48:24,534] adding document #1500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:48:51,484] discarding 33750 tokens: [('باسم', 1), ('خريشي', 1), ('سابيلا', 1), ('عاطف', 1), ('قفيشة', 1), ('هزاع', 1), ('chandata', 1), ('ebertidia', 1), ('lepidodelta', 1), ('arbănași', 1)]...\n",
      "[2022-09-26 14:48:51,487] keeping 2000000 tokens which were in no less than 0 and no more than 1510000 (=100.0%) documents\n",
      "[2022-09-26 14:48:56,840] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:48:56,934] adding document #1510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:49:23,558] discarding 30793 tokens: [('özyılmazel', 1), ('wanahoo', 1), ('shrublet#0§', 1), ('jamry', 1), ('kępińska', 1), ('lipizanner', 1), ('barrowmount', 1), ('grangesilvia', 1), ('jeanville', 1), ('ferreirai', 1)]...\n",
      "[2022-09-26 14:49:23,561] keeping 2000000 tokens which were in no less than 0 and no more than 1520000 (=100.0%) documents\n",
      "[2022-09-26 14:49:28,894] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:49:28,990] adding document #1520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:49:54,555] discarding 24415 tokens: [('gemmotherapy', 1), ('therapīa', 1), ('dekeuninck', 1), ('märchenmonds', 1), ('steppenreiter', 1), ('zauberin', 1), ('lepidos', 1), ('λεπιδος', 1), ('caelestipileata', 1), ('lindenicana', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:49:54,557] keeping 2000000 tokens which were in no less than 0 and no more than 1530000 (=100.0%) documents\n",
      "[2022-09-26 14:50:00,224] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:50:00,319] adding document #1530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:50:28,532] discarding 32467 tokens: [('论再生缘', 1), ('读陈寅恪', 1), ('近代史研究', 1), ('追忆陈寅恪', 1), ('里仁書局', 1), ('金明館叢稿二編', 1), ('金明館叢稿初編', 1), ('附唐篔詩存', 1), ('陈寅恪', 1), ('陈寅恪与傅斯年', 1)]...\n",
      "[2022-09-26 14:50:28,534] keeping 2000000 tokens which were in no less than 0 and no more than 1540000 (=100.0%) documents\n",
      "[2022-09-26 14:50:33,890] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:50:33,957] adding document #1540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:51:02,261] discarding 37271 tokens: [('striatulate', 1), ('lairson', 1), ('macjams', 1), ('paranoidsaints', 1), ('theexkings', 1), ('tllsfjx', 1), ('vanausdal', 1), ('pilgreen', 1), ('pulpeteering', 1), ('shaunnessy', 1)]...\n",
      "[2022-09-26 14:51:02,263] keeping 2000000 tokens which were in no less than 0 and no more than 1550000 (=100.0%) documents\n",
      "[2022-09-26 14:51:06,281] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:51:06,346] adding document #1550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:51:34,429] discarding 38179 tokens: [('sokirinskoye', 1), ('turbinnoye', 1), ('bondarenka', 1), ('chablj', 1), ('jöhlingen', 1), ('kolodez', 1), ('novorossiysksoutheast', 1), ('copycontrol', 1), ('šota', 1), ('galushkina', 1)]...\n",
      "[2022-09-26 14:51:34,430] keeping 2000000 tokens which were in no less than 0 and no more than 1560000 (=100.0%) documents\n",
      "[2022-09-26 14:51:39,900] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:51:39,997] adding document #1560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:52:08,562] discarding 39458 tokens: [('seebeckswerft', 1), ('barguil', 1), ('geogical', 1), ('oomodels', 1), ('zoopt', 1), ('forxiga', 1), ('qtrilmet', 1), ('xigduo', 1), ('aumbries', 1), ('exnaboe', 1)]...\n",
      "[2022-09-26 14:52:08,564] keeping 2000000 tokens which were in no less than 0 and no more than 1570000 (=100.0%) documents\n",
      "[2022-09-26 14:52:14,251] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:52:14,319] adding document #1570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:52:44,271] discarding 31947 tokens: [('wirelines', 1), ('wlspa', 1), ('ainei', 1), ('shimaathim', 1), ('suchathim', 1), ('tirathim', 1), ('yabiz', 1), ('kolianos', 1), ('lawahdiwa', 1), ('tenatsali', 1)]...\n",
      "[2022-09-26 14:52:44,273] keeping 2000000 tokens which were in no less than 0 and no more than 1580000 (=100.0%) documents\n",
      "[2022-09-26 14:52:49,663] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:52:49,759] adding document #1580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:53:20,370] discarding 36090 tokens: [('polysaccharidic', 1), ('porogens', 1), ('prototissue', 1), ('quinxell', 1), ('rateresearchers', 1), ('recellularized', 1), ('reharvested', 1), ('reprocell', 1), ('therics', 1), ('tisxell', 1)]...\n",
      "[2022-09-26 14:53:20,372] keeping 2000000 tokens which were in no less than 0 and no more than 1590000 (=100.0%) documents\n",
      "[2022-09-26 14:53:25,876] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:53:25,974] adding document #1590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:53:53,297] discarding 29764 tokens: [('siebenröhrenbrunnen', 1), ('spatzenschaukel', 1), ('staufenbergschule', 1), ('theaterschiff', 1), ('trappensee', 1), ('wartbergschule', 1), ('weingärtnergesellschaft', 1), ('weinvilla', 1), ('barakkebro', 1), ('barakkebroen', 1)]...\n",
      "[2022-09-26 14:53:53,299] keeping 2000000 tokens which were in no less than 0 and no more than 1600000 (=100.0%) documents\n",
      "[2022-09-26 14:53:58,304] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:53:58,370] adding document #1600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:54:27,991] discarding 33978 tokens: [('mantaroensis', 1), ('marcusianus', 1), ('markleanus', 1), ('martinetianus', 1), ('mathewsianus', 1), ('matucanicus', 1), ('meionanthus', 1), ('melaphyllus', 1), ('meyersii', 1), ('micensis', 1)]...\n",
      "[2022-09-26 14:54:27,993] keeping 2000000 tokens which were in no less than 0 and no more than 1610000 (=100.0%) documents\n",
      "[2022-09-26 14:54:31,852] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:54:31,918] adding document #1610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:55:00,169] discarding 32262 tokens: [('entgrenzte', 1), ('externalization#0§', 1), ('kulturdata', 1), ('mantarobot', 1), ('romotive', 1), ('telesuite', 1), ('ushoh', 1), ('benselfish', 1), ('evitability', 1), ('painthall', 1)]...\n",
      "[2022-09-26 14:55:00,171] keeping 2000000 tokens which were in no less than 0 and no more than 1620000 (=100.0%) documents\n",
      "[2022-09-26 14:55:04,146] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:55:04,213] adding document #1620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:55:31,530] discarding 40505 tokens: [('villadepera', 1), ('villaferrueña', 1), ('villageriz', 1), ('villaguer', 1), ('villalazán', 1), ('villalube', 1), ('villalverde', 1), ('villanázar', 1), ('villaralbo', 1), ('villardeciervos', 1)]...\n",
      "[2022-09-26 14:55:31,532] keeping 2000000 tokens which were in no less than 0 and no more than 1630000 (=100.0%) documents\n",
      "[2022-09-26 14:55:35,511] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:55:35,577] adding document #1630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:56:02,829] discarding 43097 tokens: [('buberos', 1), ('candilichera', 1), ('castilruiz', 1), ('cañamaque', 1), ('cidones', 1), ('cigudosa', 1), ('cihuela', 1), ('cirujales', 1), ('coscurita', 1), ('dévanos', 1)]...\n",
      "[2022-09-26 14:56:02,831] keeping 2000000 tokens which were in no less than 0 and no more than 1640000 (=100.0%) documents\n",
      "[2022-09-26 14:56:06,776] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:56:06,844] adding document #1640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:56:33,776] discarding 30437 tokens: [('kasnitzes', 1), ('kasstoria', 1), ('kastóri', 1), ('koumpelidiki', 1), ('koutsoumpli', 1), ('kremmydopita', 1), ('kyknon', 1), ('kástoras', 1), ('kástōr', 1), ('makálo', 1)]...\n",
      "[2022-09-26 14:56:33,778] keeping 2000000 tokens which were in no less than 0 and no more than 1650000 (=100.0%) documents\n",
      "[2022-09-26 14:56:37,878] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 14:56:37,945] adding document #1650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:57:07,215] discarding 34948 tokens: [('kojja', 1), ('kunwaara', 1), ('mangalamukhi', 1), ('napunsaka', 1), ('napunsakudu', 1), ('pavaiyaa', 1), ('raarha', 1), ('shatabai', 1), ('treetiya', 1), ('জড', 1)]...\n",
      "[2022-09-26 14:57:07,217] keeping 2000000 tokens which were in no less than 0 and no more than 1660000 (=100.0%) documents\n",
      "[2022-09-26 14:57:11,185] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:57:11,252] adding document #1660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:57:42,902] discarding 33810 tokens: [('inventour', 1), ('vayle', 1), ('welnear', 1), ('firerate', 1), ('hebrideas', 1), ('iqyak', 1), ('folet', 1), ('kankkonnen', 1), ('gēlān', 1), ('khorasanite', 1)]...\n",
      "[2022-09-26 14:57:42,904] keeping 2000000 tokens which were in no less than 0 and no more than 1670000 (=100.0%) documents\n",
      "[2022-09-26 14:57:46,861] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:57:46,928] adding document #1670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:58:14,080] discarding 36245 tokens: [('insovency', 1), ('ipilan', 1), ('islatel', 1), ('jafgo', 1), ('kakiduguen', 1), ('kapalangan', 1), ('kimanait', 1), ('kitubo', 1), ('lambontong', 1), ('langcataon', 1)]...\n",
      "[2022-09-26 14:58:14,082] keeping 2000000 tokens which were in no less than 0 and no more than 1680000 (=100.0%) documents\n",
      "[2022-09-26 14:58:19,802] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:58:19,900] adding document #1680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:58:46,517] discarding 30717 tokens: [('pakahinahon', 1), ('pumutí', 1), ('qwerf', 1), ('sanghabi', 1), ('syllabize', 1), ('titík', 1), ('mahams', 1), ('mulkhis', 1), ('tilismat', 1), ('airubia', 1)]...\n",
      "[2022-09-26 14:58:46,520] keeping 2000000 tokens which were in no less than 0 and no more than 1690000 (=100.0%) documents\n",
      "[2022-09-26 14:58:50,450] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:58:50,517] adding document #1690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:59:16,749] discarding 39882 tokens: [('dashifen', 1), ('neoprog', 1), ('raglianti', 1), ('childhoodradios', 1), ('eggship', 1), ('jamesbutters', 1), ('orljonok', 1), ('frégoli', 1), ('papageorgio', 1), ('citnereoargenteus', 1)]...\n",
      "[2022-09-26 14:59:16,750] keeping 2000000 tokens which were in no less than 0 and no more than 1700000 (=100.0%) documents\n",
      "[2022-09-26 14:59:20,671] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:59:20,740] adding document #1700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:59:53,005] discarding 46851 tokens: [('payloadlist', 1), ('setheight', 1), ('setwidth', 1), ('some_a', 1), ('use_an_a', 1), ('tiaplay', 1), ('branchoire', 1), ('shelucheiha', 1), ('tsìpporah', 1), ('hoorism', 1)]...\n",
      "[2022-09-26 14:59:53,008] keeping 2000000 tokens which were in no less than 0 and no more than 1710000 (=100.0%) documents\n",
      "[2022-09-26 14:59:58,374] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 14:59:58,472] adding document #1710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:00:28,791] discarding 40955 tokens: [('pramuditavihāra', 1), ('pratisaṃvidvihāra', 1), ('pratyupanna', 1), ('ratnakaraksanti', 1), ('rdzun', 1), ('satyapratisaṃyukto', 1), ('satyākāravāda', 1), ('saṅdhinirmocana', 1), ('soterological', 1), ('sābhisaṃskāraḥ', 1)]...\n",
      "[2022-09-26 15:00:28,793] keeping 2000000 tokens which were in no less than 0 and no more than 1720000 (=100.0%) documents\n",
      "[2022-09-26 15:00:34,203] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:00:34,303] adding document #1720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:01:00,870] discarding 33663 tokens: [('өткенге', 1), ('өткердік', 1), ('bakmaz', 1), ('biloglav', 1), ('cromaris', 1), ('foša', 1), ('iadertines', 1), ('iadestines', 1), ('iadora', 1), ('ivata', 1)]...\n",
      "[2022-09-26 15:01:00,872] keeping 2000000 tokens which were in no less than 0 and no more than 1730000 (=100.0%) documents\n",
      "[2022-09-26 15:01:04,840] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:01:04,908] adding document #1730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:01:35,320] discarding 33583 tokens: [('namcheondong', 1), ('remian', 1), ('seacloud', 1), ('yangwoon', 1), ('yeongjudong', 1), ('badiozamani', 1), ('gastrich', 1), ('jurlene', 1), ('newsvans', 1), ('zellhoefer', 1)]...\n",
      "[2022-09-26 15:01:35,322] keeping 2000000 tokens which were in no less than 0 and no more than 1740000 (=100.0%) documents\n",
      "[2022-09-26 15:01:40,893] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:01:40,959] adding document #1740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:02:10,981] discarding 33181 tokens: [('ankhetkheperure', 1), ('neferneferutaten', 1), ('niphururiya', 1), ('semenchkare', 1), ('smenkare', 1), ('ânkhamon', 1), ('agrinational', 1), ('ekmensel', 1), ('degaichō', 1), ('gomadō', 1)]...\n",
      "[2022-09-26 15:02:10,983] keeping 2000000 tokens which were in no less than 0 and no more than 1750000 (=100.0%) documents\n",
      "[2022-09-26 15:02:16,330] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:02:16,427] adding document #1750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:02:48,357] discarding 30450 tokens: [('localhikes', 1), ('shiveringly', 1), ('ghazrael', 1), ('vowchurch', 1), ('adventrurer', 1), ('hawkston', 1), ('ruweila', 1), ('garnati', 1), ('hajduporta', 1), ('hysmaelita', 1)]...\n",
      "[2022-09-26 15:02:48,360] keeping 2000000 tokens which were in no less than 0 and no more than 1760000 (=100.0%) documents\n",
      "[2022-09-26 15:02:52,519] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:02:52,586] adding document #1760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:03:22,536] discarding 32543 tokens: [('sulphadoxine', 1), ('chanyoung', 1), ('chulwoo', 1), ('dongnyok', 1), ('gihyun', 1), ('hayoung', 1), ('heewon', 1), ('hoyoon', 1), ('jongwuk', 1), ('jungsub', 1)]...\n",
      "[2022-09-26 15:03:22,538] keeping 2000000 tokens which were in no less than 0 and no more than 1770000 (=100.0%) documents\n",
      "[2022-09-26 15:03:27,928] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:03:28,024] adding document #1770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:04:01,351] discarding 36359 tokens: [('ergebnispools', 1), ('gelegenheitsgesellschaft', 1), ('investitionskonsortien', 1), ('kreditkonsortien', 1), ('momentanés', 1), ('planungsgesellschaften', 1), ('instituere', 1), ('encerraem', 1), ('enclosedin', 1), ('flag_of_brazil_', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:04:01,354] keeping 2000000 tokens which were in no less than 0 and no more than 1780000 (=100.0%) documents\n",
      "[2022-09-26 15:04:06,861] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:04:06,960] adding document #1780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:04:45,509] discarding 30093 tokens: [('anōgawa', 1), ('iwatagawa', 1), ('kumozugawa', 1), ('kushidagawa', 1), ('akanbe', 1), ('denjinmakai', 1), ('densei', 1), ('gdri', 1), ('lazelber', 1), ('seireiki', 1)]...\n",
      "[2022-09-26 15:04:45,511] keeping 2000000 tokens which were in no less than 0 and no more than 1790000 (=100.0%) documents\n",
      "[2022-09-26 15:04:49,421] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:04:49,488] adding document #1790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:05:28,169] discarding 27780 tokens: [('hoveman', 1), ('fadnes', 1), ('fenriskjeften', 1), ('kinntanna', 1), ('ulvetanna', 1), ('medjeded', 1), ('كريمة', 1), ('debarawewa', 1), ('pannegamuwa', 1), ('donnéa', 1)]...\n",
      "[2022-09-26 15:05:28,170] keeping 2000000 tokens which were in no less than 0 and no more than 1800000 (=100.0%) documents\n",
      "[2022-09-26 15:05:32,087] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:05:32,153] adding document #1800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:06:00,938] discarding 30774 tokens: [('attoub', 1), ('bernardet', 1), ('brugnaut', 1), ('duthier', 1), ('décosse', 1), ('gobillot', 1), ('grésèque', 1), ('hautcoeur', 1), ('martot', 1), ('ouon', 1)]...\n",
      "[2022-09-26 15:06:00,940] keeping 2000000 tokens which were in no less than 0 and no more than 1810000 (=100.0%) documents\n",
      "[2022-09-26 15:06:06,715] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:06:06,812] adding document #1810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:06:34,497] discarding 31621 tokens: [('cantafavola', 1), ('filastrocche', 1), ('gieus', 1), ('soliloquios', 1), ('vocalizzo', 1), ('antbirlik', 1), ('dimče', 1), ('gaštarski', 1), ('nemetali', 1), ('oqt', 1)]...\n",
      "[2022-09-26 15:06:34,499] keeping 2000000 tokens which were in no less than 0 and no more than 1820000 (=100.0%) documents\n",
      "[2022-09-26 15:06:39,826] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:06:39,923] adding document #1820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:07:07,730] discarding 34749 tokens: [('کوزه', 1), ('adenicarcinoma', 1), ('clairechennault', 1), ('kfgc', 1), ('lalinc', 1), ('gekesai', 1), ('kaishukata', 1), ('matanbashi', 1), ('okinwan', 1), ('shochoku', 1)]...\n",
      "[2022-09-26 15:07:07,732] keeping 2000000 tokens which were in no less than 0 and no more than 1830000 (=100.0%) documents\n",
      "[2022-09-26 15:07:13,209] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:07:13,307] adding document #1830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:07:40,725] discarding 30224 tokens: [('rikugō', 1), ('micromathematics', 1), ('arratge', 1), ('cortezia', 1), ('guilielma', 1), ('guillelma', 1), ('guillielma', 1), ('guilllelma', 1), ('lanfrance', 1), ('pount', 1)]...\n",
      "[2022-09-26 15:07:40,727] keeping 2000000 tokens which were in no less than 0 and no more than 1840000 (=100.0%) documents\n",
      "[2022-09-26 15:07:44,762] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:07:44,829] adding document #1840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:08:11,935] discarding 29092 tokens: [('ayothidoss', 1), ('criy', 1), ('dalzbone', 1), ('mawdiangdiang', 1), ('neifm', 1), ('vishwayatan', 1), ('yogashram', 1), ('declinist', 1), ('fareedzakaria', 1), ('ijambo', 1)]...\n",
      "[2022-09-26 15:08:11,937] keeping 2000000 tokens which were in no less than 0 and no more than 1850000 (=100.0%) documents\n",
      "[2022-09-26 15:08:17,311] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:08:17,412] adding document #1850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:08:43,633] discarding 30692 tokens: [('undectet', 1), ('backtapped', 1), ('pietrus', 1), ('stepback', 1), ('findlan', 1), ('yevreinova', 1), ('the_very_best_of_us', 1), ('britneycat', 1), ('scoop#7', 1), ('conspirating', 1)]...\n",
      "[2022-09-26 15:08:43,635] keeping 2000000 tokens which were in no less than 0 and no more than 1860000 (=100.0%) documents\n",
      "[2022-09-26 15:08:49,146] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:08:49,244] adding document #1860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:09:17,843] discarding 32024 tokens: [('coladangelos', 1), ('fanagans', 1), ('heusaff', 1), ('atwalak', 1), ('fameck', 1), ('mastour', 1), ('scénariste', 1), ('sopadin', 1), ('فجر', 1), ('italianjapanese', 1)]...\n",
      "[2022-09-26 15:09:17,845] keeping 2000000 tokens which were in no less than 0 and no more than 1870000 (=100.0%) documents\n",
      "[2022-09-26 15:09:23,254] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:09:23,353] adding document #1870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:09:51,971] discarding 31275 tokens: [('howinson', 1), ('biosource', 1), ('fkps', 1), ('fsph', 1), ('semenggok', 1), ('upmkb', 1), ('mcscoup', 1), ('taravatanians', 1), ('ohanaja', 1), ('cragwich', 1)]...\n",
      "[2022-09-26 15:09:51,973] keeping 2000000 tokens which were in no less than 0 and no more than 1880000 (=100.0%) documents\n",
      "[2022-09-26 15:09:57,322] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:09:57,419] adding document #1880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:10:24,777] discarding 32738 tokens: [('carrswold', 1), ('chalaak', 1), ('chihuhua', 1), ('kupleri', 1), ('purushutam', 1), ('aquapets', 1), ('fizzie', 1), ('floptopus', 1), ('kitzi', 1), ('likabee', 1)]...\n",
      "[2022-09-26 15:10:24,779] keeping 2000000 tokens which were in no less than 0 and no more than 1890000 (=100.0%) documents\n",
      "[2022-09-26 15:10:28,750] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:10:28,819] adding document #1890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:10:57,998] discarding 32929 tokens: [('coabos', 1), ('karageorges', 1), ('jameshetfield', 1), ('marcelafae', 1), ('okner', 1), ('おさえ', 1), ('工程専用のミシンgyoda', 1), ('baboule', 1), ('bugarka', 1), ('demaged', 1)]...\n",
      "[2022-09-26 15:10:58,000] keeping 2000000 tokens which were in no less than 0 and no more than 1900000 (=100.0%) documents\n",
      "[2022-09-26 15:11:01,884] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:11:01,951] adding document #1900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:11:30,471] discarding 30457 tokens: [('erosi', 1), ('kitsmarishvili', 1), ('teneycke', 1), ('adbot', 1), ('mapblast', 1), ('smartad', 1), ('mabben', 1), ('bilgisi', 1), ('demegog', 1), ('doğmalıydı', 1)]...\n",
      "[2022-09-26 15:11:30,473] keeping 2000000 tokens which were in no less than 0 and no more than 1910000 (=100.0%) documents\n",
      "[2022-09-26 15:11:34,464] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:11:34,531] adding document #1910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:12:02,008] discarding 32400 tokens: [('alkheng', 1), ('arakanis', 1), ('bawmlai', 1), ('bawmzo', 1), ('chhawnmanga', 1), ('chinzah', 1), ('chuncung', 1), ('hauhulh', 1), ('hlawnchhing', 1), ('kanpalet', 1)]...\n",
      "[2022-09-26 15:12:02,009] keeping 2000000 tokens which were in no less than 0 and no more than 1920000 (=100.0%) documents\n",
      "[2022-09-26 15:12:07,327] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:12:07,424] adding document #1920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:12:36,063] discarding 50591 tokens: [('柞', 1), ('柠', 1), ('柣', 1), ('查', 1), ('柧', 1), ('柨', 1), ('柪', 1), ('柬', 1), ('柭', 1), ('柮', 1)]...\n",
      "[2022-09-26 15:12:36,065] keeping 2000000 tokens which were in no less than 0 and no more than 1930000 (=100.0%) documents\n",
      "[2022-09-26 15:12:41,460] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:12:41,561] adding document #1930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:13:09,701] discarding 27337 tokens: [('hamiklat', 1), ('paphnuce', 1), ('ogeed', 1), ('readingdesk', 1), ('wallpainting', 1), ('hurlbuts', 1), ('wollison', 1), ('assuntina', 1), ('buonanima', 1), ('farfallon', 1)]...\n",
      "[2022-09-26 15:13:09,703] keeping 2000000 tokens which were in no less than 0 and no more than 1940000 (=100.0%) documents\n",
      "[2022-09-26 15:13:15,064] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:13:15,163] adding document #1940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:13:48,702] discarding 34735 tokens: [('najeran', 1), ('najerillense', 1), ('naxara', 1), ('besumer', 1), ('cortimiglia', 1), ('deathjazz', 1), ('iorlando', 1), ('jordanos', 1), ('momfre', 1), ('mumfre', 1)]...\n",
      "[2022-09-26 15:13:48,704] keeping 2000000 tokens which were in no less than 0 and no more than 1950000 (=100.0%) documents\n",
      "[2022-09-26 15:13:54,125] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:13:54,194] adding document #1950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:14:27,563] discarding 33365 tokens: [('tangomão', 1), ('tangozen', 1), ('buwaih', 1), ('fanakhasru', 1), ('ikhbar', 1), ('kawatib', 1), ('treatese', 1), ('şūfī', 1), ('άbdul', 1), ('exupoli', 1)]...\n",
      "[2022-09-26 15:14:27,565] keeping 2000000 tokens which were in no less than 0 and no more than 1960000 (=100.0%) documents\n",
      "[2022-09-26 15:14:33,406] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:14:33,506] adding document #1960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:15:05,188] discarding 33323 tokens: [('oxazinone', 1), ('reserphine', 1), ('зета', 1), ('baella', 1), ('celibertis', 1), ('declassifed', 1), ('itturiaga', 1), ('lholé', 1), ('montensero', 1), ('ocoas', 1)]...\n",
      "[2022-09-26 15:15:05,190] keeping 2000000 tokens which were in no less than 0 and no more than 1970000 (=100.0%) documents\n",
      "[2022-09-26 15:15:09,192] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:15:09,262] adding document #1970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:15:37,918] discarding 39836 tokens: [('shubattu', 1), ('tönkyrä', 1), ('utopism', 1), ('łanowski', 1), ('aahope', 1), ('guretto', 1), ('kakinami', 1), ('majinga', 1), ('tinquist', 1), ('marlyatou', 1)]...\n",
      "[2022-09-26 15:15:37,920] keeping 2000000 tokens which were in no less than 0 and no more than 1980000 (=100.0%) documents\n",
      "[2022-09-26 15:15:41,877] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:15:41,947] adding document #1980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:16:17,713] discarding 36741 tokens: [('stanjevic', 1), ('tricherusa', 1), ('vlastelinstvo', 1), ('тројеручицa', 1), ('rcmss', 1), ('moonglade', 1), ('månstråle', 1), ('compsiition', 1), ('catherinethegreatroslin', 1), ('armagio', 1)]...\n",
      "[2022-09-26 15:16:17,715] keeping 2000000 tokens which were in no less than 0 and no more than 1990000 (=100.0%) documents\n",
      "[2022-09-26 15:16:23,141] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:16:23,243] adding document #1990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:16:57,039] discarding 35461 tokens: [('ilefster', 1), ('tetartanopes', 1), ('amoniri', 1), ('fenrisal', 1), ('paclu', 1), ('silwaan', 1), ('vironat', 1), ('meschever', 1), ('meschief', 1), ('alacanthus', 1)]...\n",
      "[2022-09-26 15:16:57,040] keeping 2000000 tokens which were in no less than 0 and no more than 2000000 (=100.0%) documents\n",
      "[2022-09-26 15:17:00,959] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:17:01,029] adding document #2000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:17:30,556] discarding 34036 tokens: [('vellehe', 1), ('vellesed', 1), ('velleset', 1), ('velʹlʹ', 1), ('venehen', 1), ('veneheze', 1), ('vepsjan', 1), ('voihe', 1), ('voylahta', 1), ('väti', 1)]...\n",
      "[2022-09-26 15:17:30,559] keeping 2000000 tokens which were in no less than 0 and no more than 2010000 (=100.0%) documents\n",
      "[2022-09-26 15:17:34,665] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:17:34,735] adding document #2010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:18:01,336] discarding 29498 tokens: [('ballettmeister', 1), ('botschaftsrat', 1), ('branske', 1), ('brothusen', 1), ('ehrenmann', 1), ('esquillon', 1), ('friseurgehilfe', 1), ('frühlingsluft', 1), ('gehilfe', 1), ('generalsekretärs', 1)]...\n",
      "[2022-09-26 15:18:01,338] keeping 2000000 tokens which were in no less than 0 and no more than 2020000 (=100.0%) documents\n",
      "[2022-09-26 15:18:05,302] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:18:05,371] adding document #2020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:18:32,156] discarding 34526 tokens: [('sèkretno', 1), ('terjesztésű', 1), ('tjenestebrug', 1), ('trúnaðarmál', 1), ('täiesti', 1), ('uiters', 1), ('ultrasecreto', 1), ('ultrassecreto', 1), ('vajadzībām', 1), ('verschlusssache', 1)]...\n",
      "[2022-09-26 15:18:32,158] keeping 2000000 tokens which were in no less than 0 and no more than 2030000 (=100.0%) documents\n",
      "[2022-09-26 15:18:36,130] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:18:36,200] adding document #2030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:19:02,815] discarding 35902 tokens: [('mcxess', 1), ('nulnegenhonderd', 1), ('telkomnet', 1), ('hackombe', 1), ('ˈmakjʊlə', 1), ('degaspari', 1), ('fordepending', 1), ('germanite#0', 1), ('omaruruensis', 1), ('dartfordharriersac', 1)]...\n",
      "[2022-09-26 15:19:02,817] keeping 2000000 tokens which were in no less than 0 and no more than 2040000 (=100.0%) documents\n",
      "[2022-09-26 15:19:08,146] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:19:08,247] adding document #2040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:19:36,461] discarding 34249 tokens: [('férule', 1), ('humiliante', 1), ('légitimement', 1), ('poluaient', 1), ('rassemblée', 1), ('recouvrée', 1), ('résistèrent', 1), ('séant', 1), ('émancipateur', 1), ('abawe', 1)]...\n",
      "[2022-09-26 15:19:36,464] keeping 2000000 tokens which were in no less than 0 and no more than 2050000 (=100.0%) documents\n",
      "[2022-09-26 15:19:42,328] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:19:42,431] adding document #2050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:20:12,104] discarding 37871 tokens: [('wǎfángdiàn', 1), ('xiajiahezi', 1), ('xiaochangshan', 1), ('xīgǎng', 1), ('zhuānghé', 1), ('ōhiroba', 1), ('大広場', 1), ('èble', 1), ('belgerd', 1), ('kristinesson', 1)]...\n",
      "[2022-09-26 15:20:12,106] keeping 2000000 tokens which were in no less than 0 and no more than 2060000 (=100.0%) documents\n",
      "[2022-09-26 15:20:16,043] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:20:16,112] adding document #2060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:20:46,559] discarding 33311 tokens: [('bertejo', 1), ('christolytes', 1), ('eerdmanns', 1), ('egenomen', 1), ('harrowings', 1), ('νεκρὸς', 1), ('ἐγενόμην', 1), ('aabmds', 1), ('aamdtc', 1), ('asbms', 1)]...\n",
      "[2022-09-26 15:20:46,561] keeping 2000000 tokens which were in no less than 0 and no more than 2070000 (=100.0%) documents\n",
      "[2022-09-26 15:20:50,986] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:20:51,056] adding document #2070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:21:23,081] discarding 33049 tokens: [('tapirbaby', 1), ('frodien', 1), ('gorbing', 1), ('wilfner', 1), ('knüppelsteig', 1), ('schlern_pano', 1), ('schlernboden', 1), ('schlernhaus', 1), ('baldority', 1), ('mramotice', 1)]...\n",
      "[2022-09-26 15:21:23,083] keeping 2000000 tokens which were in no less than 0 and no more than 2080000 (=100.0%) documents\n",
      "[2022-09-26 15:21:28,560] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:21:28,660] adding document #2080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:21:57,323] discarding 33765 tokens: [('hastypeabblood', 1), ('hastypeoblood', 1), ('objectproperty', 1), ('lyricsother', 1), ('ibmdb', 1), ('opnqryf', 1), ('oslater', 1), ('programsupport', 1), ('qqqqry', 1), ('teraspace', 1)]...\n",
      "[2022-09-26 15:21:57,325] keeping 2000000 tokens which were in no less than 0 and no more than 2090000 (=100.0%) documents\n",
      "[2022-09-26 15:22:03,104] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:22:03,206] adding document #2090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:22:33,576] discarding 40120 tokens: [('snrrs', 1), ('socagers', 1), ('hypecoaceae', 1), ('pascle', 1), ('pijf', 1), ('psychoppharmacology', 1), ('qaulity', 1), ('youcomm', 1), ('adonideae', 1), ('adonis_annua_flor', 1)]...\n",
      "[2022-09-26 15:22:33,578] keeping 2000000 tokens which were in no less than 0 and no more than 2100000 (=100.0%) documents\n",
      "[2022-09-26 15:22:39,000] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:22:39,103] adding document #2100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:23:08,474] discarding 34354 tokens: [('परवहन', 1), ('সৰস', 1), ('ଵତ', 1), ('சரஸ', 1), ('妙音天', 1), ('hušbišag', 1), ('namtara', 1), ('namtaru', 1), ('andacceleration', 1), ('autonodrive', 1)]...\n",
      "[2022-09-26 15:23:08,476] keeping 2000000 tokens which were in no less than 0 and no more than 2110000 (=100.0%) documents\n",
      "[2022-09-26 15:23:13,993] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:23:14,094] adding document #2110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:23:42,874] discarding 36778 tokens: [('leptolegniellaceae', 1), ('leptomitaceae', 1), ('mykitas', 1), ('myzocytiopsidaceae', 1), ('olpidiopsidaceae', 1), ('olpidiopsidales', 1), ('peronophythoraceae', 1), ('peronosporales', 1), ('pontismataceae', 1), ('pseudosphaeritaceae', 1)]...\n",
      "[2022-09-26 15:23:42,876] keeping 2000000 tokens which were in no less than 0 and no more than 2120000 (=100.0%) documents\n",
      "[2022-09-26 15:23:48,299] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:23:48,401] adding document #2120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:24:16,620] discarding 27120 tokens: [('longmode', 1), ('processor_architecture', 1), ('top_mem', 1), ('vdbpsadbw', 1), ('vplzcntd', 1), ('vpmullq', 1), ('brubu', 1), ('skvala', 1), ('subasko', 1), ('akiatan', 1)]...\n",
      "[2022-09-26 15:24:16,622] keeping 2000000 tokens which were in no less than 0 and no more than 2130000 (=100.0%) documents\n",
      "[2022-09-26 15:24:22,119] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:24:22,220] adding document #2130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:24:49,222] discarding 34502 tokens: [('mebl', 1), ('midʒ', 1), ('midʒi', 1), ('miensá', 1), ('mienu', 1), ('mlaminá', 1), ('mmeɛnsã', 1), ('mmienú', 1), ('moɟa', 1), ('mwʌ', 1)]...\n",
      "[2022-09-26 15:24:49,223] keeping 2000000 tokens which were in no less than 0 and no more than 2140000 (=100.0%) documents\n",
      "[2022-09-26 15:24:54,340] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:24:54,411] adding document #2140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:25:23,140] discarding 53293 tokens: [('namhad', 1), ('neamin', 1), ('nemanh', 1), ('nemoni', 1), ('nemānjā', 1), ('neptur', 1), ('njā', 1), ('námait', 1), ('námat', 1), ('námhaid', 1)]...\n",
      "[2022-09-26 15:25:23,142] keeping 2000000 tokens which were in no less than 0 and no more than 2150000 (=100.0%) documents\n",
      "[2022-09-26 15:25:27,275] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:25:27,349] adding document #2150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:25:58,275] discarding 35697 tokens: [('megadollar', 1), ('rikuganger', 1), ('spaceganger', 1), ('umiganger', 1), ('cauda#0§', 1), ('caudaequinauk', 1), ('admy', 1), ('aircraftnorthern', 1), ('järvere', 1), ('서울버스정류장', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:25:58,276] keeping 2000000 tokens which were in no less than 0 and no more than 2160000 (=100.0%) documents\n",
      "[2022-09-26 15:26:03,967] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:26:04,062] adding document #2160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:26:31,916] discarding 35075 tokens: [('muthivanam', 1), ('rangasami', 1), ('taliaʻuli', 1), ('vaveni', 1), ('mariath', 1), ('nichteroy', 1), ('quilmeslater', 1), ('brendhan', 1), ('aristokratiske', 1), ('begyndelsen', 1)]...\n",
      "[2022-09-26 15:26:31,918] keeping 2000000 tokens which were in no less than 0 and no more than 2170000 (=100.0%) documents\n",
      "[2022-09-26 15:26:36,238] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:26:36,312] adding document #2170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:27:07,043] discarding 37579 tokens: [('accumersiel', 1), ('altensiel', 1), ('breken', 1), ('ersiel', 1), ('fulkum', 1), ('grootdiep', 1), ('gutsbrack', 1), ('houtewael', 1), ('jorisbraak', 1), ('kruizinga', 1)]...\n",
      "[2022-09-26 15:27:07,045] keeping 2000000 tokens which were in no less than 0 and no more than 2180000 (=100.0%) documents\n",
      "[2022-09-26 15:27:12,474] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:27:12,540] adding document #2180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:27:38,501] discarding 29676 tokens: [('ammed', 1), ('catletsburg', 1), ('destillières', 1), ('fournière', 1), ('lormois', 1), ('rainulphe', 1), ('roncé', 1), ('casblancakids', 1), ('openened', 1), ('hydroworld', 1)]...\n",
      "[2022-09-26 15:27:38,503] keeping 2000000 tokens which were in no less than 0 and no more than 2190000 (=100.0%) documents\n",
      "[2022-09-26 15:27:43,466] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:27:43,561] adding document #2190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:28:10,404] discarding 30950 tokens: [('altsteinzeitlichen', 1), ('besiedlungsgeschichte', 1), ('fundmaterial', 1), ('galgenberges', 1), ('galgenleithen', 1), ('neolithische', 1), ('prasads', 1), ('campiere', 1), ('inotsume', 1), ('rispara', 1)]...\n",
      "[2022-09-26 15:28:10,405] keeping 2000000 tokens which were in no less than 0 and no more than 2200000 (=100.0%) documents\n",
      "[2022-09-26 15:28:15,860] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:28:15,955] adding document #2200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:28:46,517] discarding 30364 tokens: [('allog', 1), ('shawgo', 1), ('bakon', 1), ('loginovo', 1), ('uptegrove', 1), ('anisothecium', 1), ('aongstroemia', 1), ('aongstroemiopsis', 1), ('braunfelsia', 1), ('brotherobryum', 1)]...\n",
      "[2022-09-26 15:28:46,518] keeping 2000000 tokens which were in no less than 0 and no more than 2210000 (=100.0%) documents\n",
      "[2022-09-26 15:28:50,562] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:28:50,635] adding document #2210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:29:24,689] discarding 35085 tokens: [('hrangbana', 1), ('kamalanagar', 1), ('lengpui', 1), ('selesih', 1), ('thankima', 1), ('zawlnuam', 1), ('zirtiri', 1), ('darotha', 1), ('godutai', 1), ('karkhanis', 1)]...\n",
      "[2022-09-26 15:29:24,692] keeping 2000000 tokens which were in no less than 0 and no more than 2220000 (=100.0%) documents\n",
      "[2022-09-26 15:29:30,106] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:29:30,202] adding document #2220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:30:09,372] discarding 35211 tokens: [('faaluja', 1), ('martouba', 1), ('wcpcc', 1), ('maplegate', 1), ('capharsaba', 1), ('qalqiliyya', 1), ('serakha', 1), ('afanassyi', 1), ('ukazes', 1), ('fcdinamo', 1)]...\n",
      "[2022-09-26 15:30:09,374] keeping 2000000 tokens which were in no less than 0 and no more than 2230000 (=100.0%) documents\n",
      "[2022-09-26 15:30:14,685] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:30:14,780] adding document #2230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:30:48,382] discarding 34356 tokens: [('boldra', 1), ('wassinger', 1), ('ismāʿīli', 1), ('mcnae', 1), ('mctrusty', 1), ('cazian', 1), ('bequita', 1), ('delpérier', 1), ('kerkove', 1), ('mușoiu', 1)]...\n",
      "[2022-09-26 15:30:48,384] keeping 2000000 tokens which were in no less than 0 and no more than 2240000 (=100.0%) documents\n",
      "[2022-09-26 15:30:54,115] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:30:54,212] adding document #2240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:31:22,890] discarding 49978 tokens: [('carrowbawn', 1), ('claretuam', 1), ('bendugu', 1), ('binduku', 1), ('fadku', 1), ('kinba', 1), ('sanqara', 1), ('sanuna', 1), ('sibiridugu', 1), ('tilimsani', 1)]...\n",
      "[2022-09-26 15:31:22,892] keeping 2000000 tokens which were in no less than 0 and no more than 2250000 (=100.0%) documents\n",
      "[2022-09-26 15:31:26,980] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:31:27,079] adding document #2250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:32:01,467] discarding 38067 tokens: [('carnicelli', 1), ('darwiniaflower', 1), ('kralovske', 1), ('prirodevedecke', 1), ('spolecnosti', 1), ('并', 1), ('funubis', 1), ('littlebugplanet', 1), ('sackperson', 1), ('commanderbecoming', 1)]...\n",
      "[2022-09-26 15:32:01,469] keeping 2000000 tokens which were in no less than 0 and no more than 2260000 (=100.0%) documents\n",
      "[2022-09-26 15:32:05,406] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:32:05,473] adding document #2260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:32:35,349] discarding 38907 tokens: [('keysan', 1), ('printwheel', 1), ('roinsard', 1), ('selectrici', 1), ('selectricii', 1), ('selectriciii', 1), ('seriesii', 1), ('typamatic', 1), ('typing#1§', 1), ('vydek', 1)]...\n",
      "[2022-09-26 15:32:35,351] keeping 2000000 tokens which were in no less than 0 and no more than 2270000 (=100.0%) documents\n",
      "[2022-09-26 15:32:40,957] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:32:41,055] adding document #2270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:33:09,841] discarding 36691 tokens: [('pitchaiya', 1), ('chakrashila', 1), ('geei', 1), ('languar', 1), ('multistructural', 1), ('soumyadeep', 1), ('banyangabu', 1), ('kibiito', 1), ('rwimi', 1), ('feastin', 1)]...\n",
      "[2022-09-26 15:33:09,842] keeping 2000000 tokens which were in no less than 0 and no more than 2280000 (=100.0%) documents\n",
      "[2022-09-26 15:33:13,780] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:33:13,847] adding document #2280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:33:43,542] discarding 34110 tokens: [('feroviarii', 1), ('gomelt', 1), ('pérronas', 1), ('landgirl', 1), ('lungfailure', 1), ('dibyarka', 1), ('druiblate', 1), ('nicenovia', 1), ('asfia', 1), ('assek', 1)]...\n",
      "[2022-09-26 15:33:43,544] keeping 2000000 tokens which were in no less than 0 and no more than 2290000 (=100.0%) documents\n",
      "[2022-09-26 15:33:48,517] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:33:48,614] adding document #2290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:34:20,113] discarding 36467 tokens: [('swimtastic', 1), ('ukactive', 1), ('aketch', 1), ('fjalljakt', 1), ('moorbird', 1), ('moorthe', 1), ('adoptional', 1), ('domicili', 1), ('khayar', 1), ('proported', 1)]...\n",
      "[2022-09-26 15:34:20,115] keeping 2000000 tokens which were in no less than 0 and no more than 2300000 (=100.0%) documents\n",
      "[2022-09-26 15:34:25,043] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:34:25,121] adding document #2300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:34:52,223] discarding 35309 tokens: [('ardgerry', 1), ('garroch', 1), ('junemann', 1), ('tradeswomen', 1), ('durcau', 1), ('elissia', 1), ('bochkaryova', 1), ('clavicytheriumdelin', 1), ('lyraklavier', 1), ('canoekayak', 1)]...\n",
      "[2022-09-26 15:34:52,224] keeping 2000000 tokens which were in no less than 0 and no more than 2310000 (=100.0%) documents\n",
      "[2022-09-26 15:34:56,212] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:34:56,278] adding document #2310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:35:25,453] discarding 42089 tokens: [('venicevenice', 1), ('eatingnrkempley_a', 1), ('genender', 1), ('vizeflugmeister', 1), ('agarru', 1), ('naruan', 1), ('jomornji', 1), ('menngen', 1), ('ridji', 1), ('wositzky', 1)]...\n",
      "[2022-09-26 15:35:25,455] keeping 2000000 tokens which were in no less than 0 and no more than 2320000 (=100.0%) documents\n",
      "[2022-09-26 15:35:31,303] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:35:31,401] adding document #2320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:35:58,986] discarding 35191 tokens: [('herker', 1), ('wellmed', 1), ('zvvz', 1), ('mazagine', 1), ('saratogans', 1), ('huggaland', 1), ('hugsy', 1), ('aivados', 1), ('alentjeo', 1), ('geraldos', 1)]...\n",
      "[2022-09-26 15:35:58,988] keeping 2000000 tokens which were in no less than 0 and no more than 2330000 (=100.0%) documents\n",
      "[2022-09-26 15:36:04,315] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:36:04,410] adding document #2330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:36:33,696] discarding 35875 tokens: [('engelmacher', 1), ('hanfstreu', 1), ('notfall', 1), ('oscargewinner', 1), ('spielzeugland', 1), ('trugspur', 1), ('ousebridge', 1), ('standfordians', 1), ('atyachri', 1), ('sarpsatra', 1)]...\n",
      "[2022-09-26 15:36:33,698] keeping 2000000 tokens which were in no less than 0 and no more than 2340000 (=100.0%) documents\n",
      "[2022-09-26 15:36:37,716] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:36:37,782] adding document #2340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:37:08,001] discarding 36278 tokens: [('weerawardhana', 1), ('weerawardhena', 1), ('weerawrdena', 1), ('wijesiriwardane', 1), ('chalkbeat', 1), ('ertog', 1), ('forgjengere', 1), ('oldsaksamlingen', 1), ('stenalder', 1), ('øxer', 1)]...\n",
      "[2022-09-26 15:37:08,002] keeping 2000000 tokens which were in no less than 0 and no more than 2350000 (=100.0%) documents\n",
      "[2022-09-26 15:37:13,388] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:37:13,485] adding document #2350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:37:44,112] discarding 30129 tokens: [('barworth', 1), ('overshot#2', 1), ('shadowstats', 1), ('hiltachk', 1), ('lifer#0§', 1), ('marsalee', 1), ('ncvli', 1), ('btum', 1), ('cooccurrences', 1), ('niversité', 1)]...\n",
      "[2022-09-26 15:37:44,114] keeping 2000000 tokens which were in no less than 0 and no more than 2360000 (=100.0%) documents\n",
      "[2022-09-26 15:37:49,555] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:37:49,652] adding document #2360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:38:17,645] discarding 30758 tokens: [('bluntseed', 1), ('shockblast', 1), ('bangyi', 1), ('gonandarit', 1), ('hlayga', 1), ('htauk', 1), ('kanyin', 1), ('lanbu', 1), ('laungshay', 1), ('legaing', 1)]...\n",
      "[2022-09-26 15:38:17,647] keeping 2000000 tokens which were in no less than 0 and no more than 2370000 (=100.0%) documents\n",
      "[2022-09-26 15:38:23,115] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:38:23,211] adding document #2370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:38:57,407] discarding 32495 tokens: [('caputitla', 1), ('embriz', 1), ('madaí', 1), ('rochín', 1), ('zayrik', 1), ('anorthopygidae', 1), ('coenholectypidae', 1), ('discoididae', 1), ('holectypidae', 1), ('holectypoida', 1)]...\n",
      "[2022-09-26 15:38:57,409] keeping 2000000 tokens which were in no less than 0 and no more than 2380000 (=100.0%) documents\n",
      "[2022-09-26 15:39:02,707] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:39:02,793] adding document #2380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:39:32,677] discarding 35370 tokens: [('güldür', 1), ('bekarlığa', 1), ('menekşeler', 1), ('nagsimula', 1), ('capulana', 1), ('prännin', 1), ('safaviya', 1), ('satellita', 1), ('giradol', 1), ('fleamen', 1)]...\n",
      "[2022-09-26 15:39:32,678] keeping 2000000 tokens which were in no less than 0 and no more than 2390000 (=100.0%) documents\n",
      "[2022-09-26 15:39:36,585] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:39:36,651] adding document #2390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:40:02,695] discarding 35684 tokens: [('ciarriochi', 1), ('stuvik', 1), ('gravene', 1), ('warscheneck', 1), ('ornamenters', 1), ('armandhakiselditatiana', 1), ('abaucán', 1), ('ramsundsberget', 1), ('calfactant', 1), ('stravogiannidis', 1)]...\n",
      "[2022-09-26 15:40:02,697] keeping 2000000 tokens which were in no less than 0 and no more than 2400000 (=100.0%) documents\n",
      "[2022-09-26 15:40:06,633] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:40:06,700] adding document #2400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:40:35,099] discarding 35315 tokens: [('rianaproperty', 1), ('wiladatika', 1), ('kathiawadi', 1), ('prepotentiels', 1), ('bahoon', 1), ('larkee', 1), ('denolly', 1), ('suljetuin', 1), ('uskoneet', 1), ('yhä', 1)]...\n",
      "[2022-09-26 15:40:35,101] keeping 2000000 tokens which were in no less than 0 and no more than 2410000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:40:40,572] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:40:40,670] adding document #2410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:41:09,837] discarding 33938 tokens: [('segirise', 1), ('sishel', 1), ('bagofunlimited', 1), ('cucharacha', 1), ('lastminutetravel', 1), ('menghia', 1), ('netpliance', 1), ('nsfwireless', 1), ('onmoney', 1), ('ourbeginning', 1)]...\n",
      "[2022-09-26 15:41:09,839] keeping 2000000 tokens which were in no less than 0 and no more than 2420000 (=100.0%) documents\n",
      "[2022-09-26 15:41:15,363] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:41:15,460] adding document #2420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:41:44,832] discarding 38276 tokens: [('spectrumfantasticartlive', 1), ('paszke', 1), ('pirjanowicz', 1), ('bundeshausplatz', 1), ('holzwerkhof', 1), ('member_news', 1), ('nationalawardwinners', 1), ('deireanach', 1), ('gallimh', 1), ('kitoo', 1)]...\n",
      "[2022-09-26 15:41:44,835] keeping 2000000 tokens which were in no less than 0 and no more than 2430000 (=100.0%) documents\n",
      "[2022-09-26 15:41:48,907] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:41:48,975] adding document #2430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:42:18,184] discarding 37452 tokens: [('usukeshi', 1), ('中島洋平', 1), ('倉持', 1), ('小鹿', 1), ('kodain', 1), ('大谷吉隆墓', 1), ('岡山烽火場', 1), ('徳川家康最初陣地', 1), ('徳川家康最後陣地', 1), ('東首塚', 1)]...\n",
      "[2022-09-26 15:42:18,187] keeping 2000000 tokens which were in no less than 0 and no more than 2440000 (=100.0%) documents\n",
      "[2022-09-26 15:42:24,291] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:42:24,405] adding document #2440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:42:49,972] discarding 29142 tokens: [('ratsaphakhinay', 1), ('somdeth', 1), ('almendron', 1), ('chapelín', 1), ('taquillas', 1), ('khumphoui', 1), ('mangkhala', 1), ('mavattaha', 1), ('phengdara', 1), ('sammathi', 1)]...\n",
      "[2022-09-26 15:42:49,974] keeping 2000000 tokens which were in no less than 0 and no more than 2450000 (=100.0%) documents\n",
      "[2022-09-26 15:42:53,980] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:42:54,045] adding document #2450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:43:21,004] discarding 29574 tokens: [('агара', 1), ('аиҳá', 1), ('аиҳа', 1), ('ауаҩы', 1), ('аҩны', 1), ('аҭы', 1), ('аҷкәы', 1), ('аҽқәа', 1), ('аԥибаҽра', 1), ('аԥхьара', 1)]...\n",
      "[2022-09-26 15:43:21,006] keeping 2000000 tokens which were in no less than 0 and no more than 2460000 (=100.0%) documents\n",
      "[2022-09-26 15:43:26,518] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:43:26,614] adding document #2460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:43:56,626] discarding 32887 tokens: [('piclear', 1), ('popleka', 1), ('previnaire', 1), ('ras_track', 1), ('seapar', 1), ('sicrist', 1), ('sonosax', 1), ('sunbrute', 1), ('tresfon', 1), ('turbulation', 1)]...\n",
      "[2022-09-26 15:43:56,629] keeping 2000000 tokens which were in no less than 0 and no more than 2470000 (=100.0%) documents\n",
      "[2022-09-26 15:44:02,015] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:44:02,111] adding document #2470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:44:28,993] discarding 30974 tokens: [('muschenich', 1), ('ordnungslehre', 1), ('fpkb', 1), ('jeserun', 1), ('bardowicks', 1), ('rubiconis', 1), ('rubicu', 1), ('africaniella', 1), ('anocentor', 1), ('anomalohimalaya', 1)]...\n",
      "[2022-09-26 15:44:28,995] keeping 2000000 tokens which were in no less than 0 and no more than 2480000 (=100.0%) documents\n",
      "[2022-09-26 15:44:34,879] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:44:34,976] adding document #2480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:45:05,581] discarding 77361 tokens: [('nasrallahs', 1), ('kolanijan', 1), ('aishwary', 1), ('ambadawekar', 1), ('amrohawi', 1), ('bahishkrit', 1), ('bhaiyasaheb', 1), ('murbadkar', 1), ('rajgraha', 1), ('urgano', 1)]...\n",
      "[2022-09-26 15:45:05,584] keeping 2000000 tokens which were in no less than 0 and no more than 2490000 (=100.0%) documents\n",
      "[2022-09-26 15:45:11,012] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:45:11,116] adding document #2490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:45:44,564] discarding 43306 tokens: [('katthanaen', 1), ('kyprogenea', 1), ('leukotera', 1), ('lipois', 1), ('lūcet', 1), ('nuzialedavanti', 1), ('petendus', 1), ('phereis', 1), ('phrena', 1), ('prodeas', 1)]...\n",
      "[2022-09-26 15:45:44,566] keeping 2000000 tokens which were in no less than 0 and no more than 2500000 (=100.0%) documents\n",
      "[2022-09-26 15:45:49,968] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:45:50,067] adding document #2500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:46:22,071] discarding 37665 tokens: [('tämmöinen', 1), ('valoisan', 1), ('ochrurus', 1), ('samamisicus', 1), ('foreordain', 1), ('raifia', 1), ('bannonbridge', 1), ('longbets', 1), ('longviewer', 1), ('pandictionary', 1)]...\n",
      "[2022-09-26 15:46:22,073] keeping 2000000 tokens which were in no less than 0 and no more than 2510000 (=100.0%) documents\n",
      "[2022-09-26 15:46:27,801] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:46:27,899] adding document #2510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:46:56,888] discarding 35116 tokens: [('варуша', 1), ('картала', 1), ('триъгълника', 1), ('тръневъ', 1), ('тръновъ', 1), ('търновград', 1), ('фичето', 1), ('чолаковци', 1), ('طرنوه', 1), ('climatebooks', 1)]...\n",
      "[2022-09-26 15:46:56,890] keeping 2000000 tokens which were in no less than 0 and no more than 2520000 (=100.0%) documents\n",
      "[2022-09-26 15:47:00,912] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:47:00,979] adding document #2520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:47:29,503] discarding 35721 tokens: [('jismor', 1), ('seperae', 1), ('xingfengsu', 1), ('回光', 1), ('回教民族', 1), ('教派', 1), ('文波', 1), ('金吉堂', 1), ('centimetros', 1), ('cluboficialconuff', 1)]...\n",
      "[2022-09-26 15:47:29,505] keeping 2000000 tokens which were in no less than 0 and no more than 2530000 (=100.0%) documents\n",
      "[2022-09-26 15:47:33,493] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:47:33,560] adding document #2530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:48:01,902] discarding 31456 tokens: [('skewville', 1), ('uncomissioned', 1), ('bonatstraße', 1), ('helmboldstraße', 1), ('nordelbien', 1), ('bimoto', 1), ('caravanner', 1), ('einride', 1), ('essesse', 1), ('finishline', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:48:01,903] keeping 2000000 tokens which were in no less than 0 and no more than 2540000 (=100.0%) documents\n",
      "[2022-09-26 15:48:07,648] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:48:07,744] adding document #2540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:48:34,273] discarding 31815 tokens: [('tamlaghoskutt', 1), ('tammaskey', 1), ('tamneosker', 1), ('tamneyaskey', 1), ('tateneosker', 1), ('tauniagher', 1), ('tawnaosker', 1), ('tawneosker', 1), ('tomnasker', 1), ('townasker', 1)]...\n",
      "[2022-09-26 15:48:34,275] keeping 2000000 tokens which were in no less than 0 and no more than 2550000 (=100.0%) documents\n",
      "[2022-09-26 15:48:39,869] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:48:39,967] adding document #2550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:49:13,819] discarding 36604 tokens: [('anthrṓp', 1), ('bhár', 1), ('coniūx', 1), ('modificatory', 1), ('podí', 1), ('prātipadika', 1), ('rosā', 1), ('senātu', 1), ('thérm', 1), ('tīmaeis', 1)]...\n",
      "[2022-09-26 15:49:13,821] keeping 2000000 tokens which were in no less than 0 and no more than 2560000 (=100.0%) documents\n",
      "[2022-09-26 15:49:17,807] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:49:17,876] adding document #2560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:49:57,747] discarding 38541 tokens: [('üta', 1), ('ɡɔɹaki', 1), ('ᵛtumi', 1), ('ᵛtwmalwk', 1), ('ᶠapuni', 1), ('ᶠapwnalwk', 1), ('angelco', 1), ('kenlowe', 1), ('lilleys', 1), ('outhandles', 1)]...\n",
      "[2022-09-26 15:49:57,750] keeping 2000000 tokens which were in no less than 0 and no more than 2570000 (=100.0%) documents\n",
      "[2022-09-26 15:50:03,162] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:50:03,262] adding document #2570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:50:34,782] discarding 56458 tokens: [('bauernstand', 1), ('bettbrunn', 1), ('bischofes', 1), ('bogenberge', 1), ('erziehungshäuser', 1), ('festfeier', 1), ('fürbitter', 1), ('gedächtnisse', 1), ('geneigten', 1), ('gnadenbildes', 1)]...\n",
      "[2022-09-26 15:50:34,784] keeping 2000000 tokens which were in no less than 0 and no more than 2580000 (=100.0%) documents\n",
      "[2022-09-26 15:50:40,515] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:50:40,618] adding document #2580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:51:10,818] discarding 34905 tokens: [('mangxô', 1), ('megahalya', 1), ('murithai', 1), ('nababarta', 1), ('nigom', 1), ('nit_silchar_guest_house', 1), ('nritra', 1), ('onkeeya', 1), ('pahukata', 1), ('prantojyoti', 1)]...\n",
      "[2022-09-26 15:51:10,820] keeping 2000000 tokens which were in no less than 0 and no more than 2590000 (=100.0%) documents\n",
      "[2022-09-26 15:51:16,221] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:51:16,319] adding document #2590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:51:46,902] discarding 32906 tokens: [('qasadhere', 1), ('adayle', 1), ('banaaney', 1), ('bilcisha', 1), ('boorame', 1), ('buusaar', 1), ('calijiciir', 1), ('caracase', 1), ('cinjirta', 1), ('dhamaso', 1)]...\n",
      "[2022-09-26 15:51:46,904] keeping 2000000 tokens which were in no less than 0 and no more than 2600000 (=100.0%) documents\n",
      "[2022-09-26 15:51:51,607] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:51:51,675] adding document #2600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:52:18,441] discarding 28877 tokens: [('brobury', 1), ('patalino', 1), ('tellability', 1), ('icmla', 1), ('elisenbergløkkens', 1), ('elisenbergveien', 1), ('husflidskole', 1), ('kristinelundveien', 1), ('prodder', 1), ('karamella', 1)]...\n",
      "[2022-09-26 15:52:18,443] keeping 2000000 tokens which were in no less than 0 and no more than 2610000 (=100.0%) documents\n",
      "[2022-09-26 15:52:22,519] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:52:22,587] adding document #2610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:52:49,665] discarding 28470 tokens: [('ненадовић', 1), ('belingeham', 1), ('frithe', 1), ('hambroughe', 1), ('pauncey', 1), ('pauncy', 1), ('surity', 1), ('natahniel', 1), ('prudlowe', 1), ('roward', 1)]...\n",
      "[2022-09-26 15:52:49,667] keeping 2000000 tokens which were in no less than 0 and no more than 2620000 (=100.0%) documents\n",
      "[2022-09-26 15:52:55,120] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:52:55,217] adding document #2620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:53:28,985] discarding 30175 tokens: [('balagia', 1), ('ldad', 1), ('mazzant', 1), ('nonissue', 1), ('servergy', 1), ('severgy', 1), ('spare#9§', 1), ('virasin', 1), ('békaa', 1), ('ceuzb', 1)]...\n",
      "[2022-09-26 15:53:28,988] keeping 2000000 tokens which were in no less than 0 and no more than 2630000 (=100.0%) documents\n",
      "[2022-09-26 15:53:34,727] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:53:34,825] adding document #2630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:54:15,070] discarding 37248 tokens: [('𑰎', 1), ('𑱲', 1), ('𑴌', 1), ('𑵱', 1), ('christlam', 1), ('mokhtefi', 1), ('fleckum', 1), ('alshalabi', 1), ('chiefforward', 1), ('enemyaction', 1)]...\n",
      "[2022-09-26 15:54:15,072] keeping 2000000 tokens which were in no less than 0 and no more than 2640000 (=100.0%) documents\n",
      "[2022-09-26 15:54:19,366] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:54:19,435] adding document #2640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:54:49,556] discarding 37359 tokens: [('พพะก', 1), ('มรกฏ', 1), ('มสาย', 1), ('มหาอ', 1), ('มาลาก', 1), ('ยากร', 1), ('รประว', 1), ('รองทรง', 1), ('ลดาว', 1), ('ลมห', 1)]...\n",
      "[2022-09-26 15:54:49,558] keeping 2000000 tokens which were in no less than 0 and no more than 2650000 (=100.0%) documents\n",
      "[2022-09-26 15:54:55,124] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:54:55,232] adding document #2650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:55:25,989] discarding 34894 tokens: [('plattcrossing', 1), ('stolkjaerre', 1), ('teacart', 1), ('abelhammer', 1), ('bother#7', 1), ('brosnanhimself', 1), ('ccomr', 1), ('iheartlatino', 1), ('iheartsports', 1), ('rioridan', 1)]...\n",
      "[2022-09-26 15:55:25,991] keeping 2000000 tokens which were in no less than 0 and no more than 2660000 (=100.0%) documents\n",
      "[2022-09-26 15:55:29,980] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:55:30,047] adding document #2660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 15:55:58,415] discarding 35316 tokens: [('pétrement', 1), ('radzins', 1), ('rueballu', 1), ('simoneweil', 1), ('wroteː', 1), ('fluorothiazole', 1), ('gemonil§', 1), ('oxazolidinediones', 1), ('proconvulsively', 1), ('valproylamides', 1)]...\n",
      "[2022-09-26 15:55:58,417] keeping 2000000 tokens which were in no less than 0 and no more than 2670000 (=100.0%) documents\n",
      "[2022-09-26 15:56:03,941] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:56:04,039] adding document #2670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:56:33,572] discarding 35929 tokens: [('yukkariussa', 1), ('yukuraransā', 1), ('yukushi', 1), ('yumun', 1), ('yutasaru', 1), ('yuttaikwattai', 1), ('yātaru', 1), ('zuibun', 1), ('ēsachi', 1), ('ībappēshīnē', 1)]...\n",
      "[2022-09-26 15:56:33,574] keeping 2000000 tokens which were in no less than 0 and no more than 2680000 (=100.0%) documents\n",
      "[2022-09-26 15:56:39,156] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:56:39,255] adding document #2680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:57:09,499] discarding 43253 tokens: [('malakowturm', 1), ('malokos', 1), ('mindarts', 1), ('nevelstraße', 1), ('oelbach', 1), ('oktobermarkt', 1), ('ostasiatika', 1), ('rietkötter', 1), ('rubissimo', 1), ('ruhrhöhen', 1)]...\n",
      "[2022-09-26 15:57:09,501] keeping 2000000 tokens which were in no less than 0 and no more than 2690000 (=100.0%) documents\n",
      "[2022-09-26 15:57:14,959] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:57:15,060] adding document #2690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:57:43,767] discarding 35821 tokens: [('støbakk', 1), ('øygardshallen', 1), ('lorbergii', 1), ('prigold', 1), ('aubrius', 1), ('schleichius', 1), ('lepsoeya', 1), ('vatnefjorden', 1), ('landbruksmuseet', 1), ('vestnesavisa', 1)]...\n",
      "[2022-09-26 15:57:43,769] keeping 2000000 tokens which were in no less than 0 and no more than 2700000 (=100.0%) documents\n",
      "[2022-09-26 15:57:47,719] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:57:47,786] adding document #2700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:58:16,061] discarding 44149 tokens: [('varíety', 1), ('audioofclassics', 1), ('invadin', 1), ('bentchakal', 1), ('divengele', 1), ('harzoune', 1), ('aghnatios', 1), ('enkastron', 1), ('epachy', 1), ('pshomt', 1)]...\n",
      "[2022-09-26 15:58:16,063] keeping 2000000 tokens which were in no less than 0 and no more than 2710000 (=100.0%) documents\n",
      "[2022-09-26 15:58:21,653] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:58:21,754] adding document #2710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:58:53,390] discarding 49120 tokens: [('makibashi', 1), ('grigadan', 1), ('gulisi', 1), ('dichlorohydrins', 1), ('glukus', 1), ('teledo', 1), ('boesenoxide', 1), ('chemeoselective', 1), ('chlorosuccinimidosulfonium', 1), ('dibromoolefin', 1)]...\n",
      "[2022-09-26 15:58:53,392] keeping 2000000 tokens which were in no less than 0 and no more than 2720000 (=100.0%) documents\n",
      "[2022-09-26 15:58:58,809] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:58:58,910] adding document #2720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:59:28,004] discarding 39251 tokens: [('ubvm', 1), ('uiucvmd', 1), ('urbana_il', 1), ('utcvm', 1), ('va_tech', 1), ('villvm', 1), ('wash_dc', 1), ('rievele', 1), ('identitaet', 1), ('iostudies', 1)]...\n",
      "[2022-09-26 15:59:28,007] keeping 2000000 tokens which were in no less than 0 and no more than 2730000 (=100.0%) documents\n",
      "[2022-09-26 15:59:33,416] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 15:59:33,516] adding document #2730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:00:02,504] discarding 35735 tokens: [('cragnali', 1), ('cubilette', 1), ('gadzic', 1), ('ghettobird', 1), ('keiffier', 1), ('medvesek', 1), ('nusevic', 1), ('areumdaoon', 1), ('arminae', 1), ('drykuss', 1)]...\n",
      "[2022-09-26 16:00:02,506] keeping 2000000 tokens which were in no less than 0 and no more than 2740000 (=100.0%) documents\n",
      "[2022-09-26 16:00:06,499] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:00:06,568] adding document #2740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:00:36,566] discarding 38171 tokens: [('germidis', 1), ('institutionist', 1), ('kadaras', 1), ('kupedes', 1), ('microempresas', 1), ('mutalima', 1), ('padmanabahn', 1), ('arkosh', 1), ('uncomprehensible', 1), ('arrouca', 1)]...\n",
      "[2022-09-26 16:00:36,568] keeping 2000000 tokens which were in no less than 0 and no more than 2750000 (=100.0%) documents\n",
      "[2022-09-26 16:00:40,536] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:00:40,604] adding document #2750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:01:10,949] discarding 35238 tokens: [('drεjtat', 1), ('dukε', 1), ('dəʃ', 1), ('dɾɛj', 1), ('eσte', 1), ('frigohet', 1), ('frikohét', 1), ('frikohεt', 1), ('hɛt', 1), ('kombε', 1)]...\n",
      "[2022-09-26 16:01:10,951] keeping 2000000 tokens which were in no less than 0 and no more than 2760000 (=100.0%) documents\n",
      "[2022-09-26 16:01:15,009] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:01:15,078] adding document #2760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:01:44,124] discarding 36517 tokens: [('maxxloader', 1), ('mfog', 1), ('sarpl', 1), ('warfreakz', 1), ('maulett', 1), ('blkcula', 1), ('blkvampires', 1), ('blkvampiresx', 1), ('blockk', 1), ('harlequinx', 1)]...\n",
      "[2022-09-26 16:01:44,126] keeping 2000000 tokens which were in no less than 0 and no more than 2770000 (=100.0%) documents\n",
      "[2022-09-26 16:01:48,189] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:01:48,289] adding document #2770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:02:18,275] discarding 36542 tokens: [('waitoka', 1), ('earliwine', 1), ('tweetcaroline', 1), ('tourmament', 1), ('darys', 1), ('nonappointive', 1), ('polarlichts', 1), ('arsuserfiles', 1), ('elaeopyron', 1), ('expansíon', 1)]...\n",
      "[2022-09-26 16:02:18,277] keeping 2000000 tokens which were in no less than 0 and no more than 2780000 (=100.0%) documents\n",
      "[2022-09-26 16:02:22,925] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:02:22,993] adding document #2780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:02:53,959] discarding 37370 tokens: [('ecobahn', 1), ('pollana', 1), ('zqtd', 1), ('anero', 1), ('avecedo', 1), ('hoznayo', 1), ('arsenkina', 1), ('dongxu', 1), ('ferrazza', 1), ('janatová', 1)]...\n",
      "[2022-09-26 16:02:53,961] keeping 2000000 tokens which were in no less than 0 and no more than 2790000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:02:58,476] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:02:58,544] adding document #2790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:03:27,864] discarding 44587 tokens: [('flabbiest', 1), ('morganthall', 1), ('barlesi', 1), ('beaubélique', 1), ('bigourie', 1), ('bosqueau', 1), ('brandela', 1), ('broodcoren', 1), ('comole', 1), ('debard', 1)]...\n",
      "[2022-09-26 16:03:27,867] keeping 2000000 tokens which were in no less than 0 and no more than 2800000 (=100.0%) documents\n",
      "[2022-09-26 16:03:33,291] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:03:33,393] adding document #2800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:04:03,887] discarding 37042 tokens: [('moium', 1), ('papei', 1), ('safli', 1), ('uhlawʔ', 1), ('usawʔ', 1), ('uyomʔ', 1), ('pagnuco', 1), ('고종욱', 1), ('amlabad', 1), ('bhowra', 1)]...\n",
      "[2022-09-26 16:04:03,889] keeping 2000000 tokens which were in no less than 0 and no more than 2810000 (=100.0%) documents\n",
      "[2022-09-26 16:04:07,892] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:04:07,962] adding document #2810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:04:36,450] discarding 32870 tokens: [('jasztrebie', 1), ('zofiowka', 1), ('syrkov', 1), ('zverin', 1), ('zverinets', 1), ('groenwegen', 1), ('sieveri', 1), ('dasonomia', 1), ('departemento', 1), ('ecologicos', 1)]...\n",
      "[2022-09-26 16:04:36,452] keeping 2000000 tokens which were in no less than 0 and no more than 2820000 (=100.0%) documents\n",
      "[2022-09-26 16:04:40,558] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:04:40,626] adding document #2820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:05:12,677] discarding 36184 tokens: [('feikin', 1), ('mardonwali', 1), ('yadvinder', 1), ('fariyaad', 1), ('karamdas', 1), ('sherawaliye', 1), ('sewaram', 1), ('maharaniji', 1), ('marvadi', 1), ('raniji', 1)]...\n",
      "[2022-09-26 16:05:12,679] keeping 2000000 tokens which were in no less than 0 and no more than 2830000 (=100.0%) documents\n",
      "[2022-09-26 16:05:18,414] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:05:18,514] adding document #2830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:05:50,139] discarding 37198 tokens: [('kunjung', 1), ('oyon', 1), ('sindanglaya', 1), ('supomo', 1), ('wiriatmadja', 1), ('aydınlı', 1), ('cercisli', 1), ('sehlikoğlu', 1), ('sportkluben', 1), ('guacirope', 1)]...\n",
      "[2022-09-26 16:05:50,141] keeping 2000000 tokens which were in no less than 0 and no more than 2840000 (=100.0%) documents\n",
      "[2022-09-26 16:05:55,937] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:05:56,037] adding document #2840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:06:26,321] discarding 36576 tokens: [('adapk', 1), ('artchowk', 1), ('artdir', 1), ('artists_profile', 1), ('jamil_baloch', 1), ('khaasgallery', 1), ('thecollectionkhaasartist', 1), ('thedrawingroom', 1), ('todaysprintdetail', 1), ('vaslart', 1)]...\n",
      "[2022-09-26 16:06:26,323] keeping 2000000 tokens which were in no less than 0 and no more than 2850000 (=100.0%) documents\n",
      "[2022-09-26 16:06:32,058] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:06:32,159] adding document #2850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:07:05,451] discarding 39173 tokens: [('wendyl', 1), ('communismes', 1), ('méfiance', 1), ('ahmadbayov', 1), ('zivar', 1), ('guescon', 1), ('herapion', 1), ('piquerobi', 1), ('branduani', 1), ('tenuiella', 1)]...\n",
      "[2022-09-26 16:07:05,453] keeping 2000000 tokens which were in no less than 0 and no more than 2860000 (=100.0%) documents\n",
      "[2022-09-26 16:07:10,605] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:07:10,675] adding document #2860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:07:40,696] discarding 34324 tokens: [('celetus', 1), ('draculo', 1), ('pogognathus', 1), ('ganzulia', 1), ('alungata', 1), ('atrisaxi', 1), ('binhdinhensis', 1), ('caudana', 1), ('charneri', 1), ('desmiti', 1)]...\n",
      "[2022-09-26 16:07:40,698] keeping 2000000 tokens which were in no less than 0 and no more than 2870000 (=100.0%) documents\n",
      "[2022-09-26 16:07:44,810] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:07:44,879] adding document #2870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:08:16,362] discarding 37771 tokens: [('tonkowda', 1), ('miniseason', 1), ('rohert', 1), ('マイページ', 1), ('倉持明日香', 1), ('期生', 1), ('第一回研究生', 1), ('nbod', 1), ('reaeration', 1), ('eroine', 1)]...\n",
      "[2022-09-26 16:08:16,364] keeping 2000000 tokens which were in no less than 0 and no more than 2880000 (=100.0%) documents\n",
      "[2022-09-26 16:08:22,101] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:08:22,189] adding document #2880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:09:00,037] discarding 34178 tokens: [('keepthewebopen', 1), ('arielia', 1), ('citharopsis', 1), ('clinomitra', 1), ('cymakra', 1), ('diptychomitra', 1), ('helenella', 1), ('maorimorpha', 1), ('mitramorpha', 1), ('mitrellatoma', 1)]...\n",
      "[2022-09-26 16:09:00,039] keeping 2000000 tokens which were in no less than 0 and no more than 2890000 (=100.0%) documents\n",
      "[2022-09-26 16:09:05,735] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:09:05,834] adding document #2890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:09:35,192] discarding 34837 tokens: [('aidondla', 1), ('bhujangayyana', 1), ('goravas', 1), ('idolle', 1), ('janmaku', 1), ('kagada', 1), ('kendasampige', 1), ('parapancha', 1), ('prathiroopi', 1), ('singaaravva', 1)]...\n",
      "[2022-09-26 16:09:35,194] keeping 2000000 tokens which were in no less than 0 and no more than 2900000 (=100.0%) documents\n",
      "[2022-09-26 16:09:40,673] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:09:40,775] adding document #2900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:10:09,976] discarding 32477 tokens: [('faceplace', 1), ('gloworm', 1), ('garnsviken', 1), ('kyrkroin', 1), ('sydväst', 1), ('chōichirō', 1), ('romp#5§', 1), ('aberglasslyn', 1), ('allynbrook', 1), ('bagwyllydiart', 1)]...\n",
      "[2022-09-26 16:10:09,977] keeping 2000000 tokens which were in no less than 0 and no more than 2910000 (=100.0%) documents\n",
      "[2022-09-26 16:10:15,369] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:10:15,469] adding document #2910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:10:47,860] discarding 35626 tokens: [('cubby#0§', 1), ('frejdenberg', 1), ('goldsteinarrangement', 1), ('stutch', 1), ('jlmmy', 1), ('jangsanhuji', 1), ('munjip', 1), ('東萊勝覽書後誌', 1), ('piljev', 1), ('piljevac', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:10:47,866] keeping 2000000 tokens which were in no less than 0 and no more than 2920000 (=100.0%) documents\n",
      "[2022-09-26 16:10:52,686] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:10:52,789] adding document #2920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:11:26,086] discarding 35059 tokens: [('orgelbauwerkstatt', 1), ('foredmonton', 1), ('jeneroux', 1), ('fluosol', 1), ('perfluoroalkylamines', 1), ('perfluoroamines', 1), ('perfluoromethyldiethylamine', 1), ('theamericanboynovember', 1), ('alcancez', 1), ('dimagiba', 1)]...\n",
      "[2022-09-26 16:11:26,087] keeping 2000000 tokens which were in no less than 0 and no more than 2930000 (=100.0%) documents\n",
      "[2022-09-26 16:11:30,052] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:11:30,121] adding document #2930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:11:59,572] discarding 33812 tokens: [('tamahoko', 1), ('gonew', 1), ('havaianas', 1), ('kaszek', 1), ('kumruian', 1), ('testinha', 1), ('davinger', 1), ('swannegan', 1), ('cultuurtuinlaan', 1), ('hardjoprajitno', 1)]...\n",
      "[2022-09-26 16:11:59,574] keeping 2000000 tokens which were in no less than 0 and no more than 2940000 (=100.0%) documents\n",
      "[2022-09-26 16:12:05,204] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:12:05,305] adding document #2940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:12:36,559] discarding 37878 tokens: [('chyou', 1), ('tarawati', 1), ('sethh', 1), ('inadomi', 1), ('cántale', 1), ('piliafas', 1), ('buecker', 1), ('zahreela', 1), ('houkokuji', 1), ('dhomoni', 1)]...\n",
      "[2022-09-26 16:12:36,561] keeping 2000000 tokens which were in no less than 0 and no more than 2950000 (=100.0%) documents\n",
      "[2022-09-26 16:12:40,607] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:12:40,676] adding document #2950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:13:10,818] discarding 35092 tokens: [('maghjanuary', 1), ('saonjuly', 1), ('shukradin', 1), ('ओन', 1), ('तspring', 1), ('शरत', 1), ('sreerangam', 1), ('süč', 1), ('benthopectininae', 1), ('jangoux', 1)]...\n",
      "[2022-09-26 16:13:10,821] keeping 2000000 tokens which were in no less than 0 and no more than 2960000 (=100.0%) documents\n",
      "[2022-09-26 16:13:16,086] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:13:16,161] adding document #2960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:13:46,915] discarding 40162 tokens: [('shuhosha', 1), ('toyokeizai', 1), ('vinaconex', 1), ('nallaram', 1), ('lliev', 1), ('suleimenova', 1), ('transfomerus', 1), ('бубнов', 1), ('sunnily', 1), ('digmore', 1)]...\n",
      "[2022-09-26 16:13:46,920] keeping 2000000 tokens which were in no less than 0 and no more than 2970000 (=100.0%) documents\n",
      "[2022-09-26 16:13:52,392] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:13:52,493] adding document #2970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:14:23,907] discarding 36762 tokens: [('umbraetae', 1), ('unimaculosa', 1), ('wilverthi', 1), ('chibabox', 1), ('kashiwakuma', 1), ('ogawadai', 1), ('ryōsō', 1), ('yamakura', 1), ('栗山川', 1), ('acromerit', 1)]...\n",
      "[2022-09-26 16:14:23,909] keeping 2000000 tokens which were in no less than 0 and no more than 2980000 (=100.0%) documents\n",
      "[2022-09-26 16:14:28,028] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:14:28,098] adding document #2980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:14:58,287] discarding 30329 tokens: [('weang', 1), ('arendarhonons', 1), ('ataronchronons', 1), ('attignawantans', 1), ('attigneenongnahacs', 1), ('tahontaenrats', 1), ('lucewicz', 1), ('bretzman', 1), ('goreishi', 1), ('erðanumúsík', 1)]...\n",
      "[2022-09-26 16:14:58,289] keeping 2000000 tokens which were in no less than 0 and no more than 2990000 (=100.0%) documents\n",
      "[2022-09-26 16:15:02,294] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:15:02,363] adding document #2990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:15:33,533] discarding 35283 tokens: [('skanwan', 1), ('catowisconsin', 1), ('clevelandwisconsinpostoffice', 1), ('clevelandwisconsinrailroadsign', 1), ('clevelandwisconsinrailway', 1), ('clevelandwisconsinvillagehall', 1), ('franciscreek', 1), ('maribelwisconsindowntown', 1), ('maribelwisconsinsign', 1), ('omīnīw', 1)]...\n",
      "[2022-09-26 16:15:33,534] keeping 2000000 tokens which were in no less than 0 and no more than 3000000 (=100.0%) documents\n",
      "[2022-09-26 16:15:37,582] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:15:37,652] adding document #3000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:16:06,813] discarding 32193 tokens: [('altontowers', 1), ('pkdhypersonicdrop', 1), ('txgi', 1), ('discipulum', 1), ('dyophysitists', 1), ('dyothelite', 1), ('eschatologike', 1), ('eucharistiaka', 1), ('eucharistiake', 1), ('ginnesthai', 1)]...\n",
      "[2022-09-26 16:16:06,815] keeping 2000000 tokens which were in no less than 0 and no more than 3010000 (=100.0%) documents\n",
      "[2022-09-26 16:16:12,501] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:16:12,603] adding document #3010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:16:42,639] discarding 36996 tokens: [('ashawague', 1), ('ashawaug', 1), ('mettatuxet', 1), ('narragasnett', 1), ('rpsec', 1), ('bethesdaunitedmethodistpowdersvillesc', 1), ('carpentershousepowdersville', 1), ('lifespringchurchofgodpowdersville', 1), ('newspringchurchpowdersville', 1), ('palmettobaptistchurchpowdersville', 1)]...\n",
      "[2022-09-26 16:16:42,641] keeping 2000000 tokens which were in no less than 0 and no more than 3020000 (=100.0%) documents\n",
      "[2022-09-26 16:16:48,515] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:16:48,618] adding document #3020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:17:20,569] discarding 30444 tokens: [('sporthelme', 1), ('stechhelm', 1), ('fryggesboda', 1), ('bastkärn', 1), ('knöl', 1), ('mossgruvan', 1), ('silverhöjden', 1), ('stjärnfors', 1), ('ställberg', 1), ('yxsjöberg', 1)]...\n",
      "[2022-09-26 16:17:20,571] keeping 2000000 tokens which were in no less than 0 and no more than 3030000 (=100.0%) documents\n",
      "[2022-09-26 16:17:26,177] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:17:26,276] adding document #3030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:17:58,315] discarding 41656 tokens: [('motorsällskap', 1), ('skephult', 1), ('tostared', 1), ('fubbins', 1), ('leuhr', 1), ('wedgcor', 1), ('avonrider', 1), ('alesburg', 1), ('meshefski', 1), ('lipsh', 1)]...\n",
      "[2022-09-26 16:17:58,317] keeping 2000000 tokens which were in no less than 0 and no more than 3040000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:18:03,736] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:18:03,840] adding document #3040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:18:33,245] discarding 35249 tokens: [('tionondega', 1), ('tionnontogen', 1), ('tionondogue', 1), ('tionondoraga', 1), ('librarycobblestone', 1), ('southeasernt', 1), ('cechnicki', 1), ('earliene', 1), ('villageofatlanticbeach', 1), ('kedenberg', 1)]...\n",
      "[2022-09-26 16:18:33,247] keeping 2000000 tokens which were in no less than 0 and no more than 3050000 (=100.0%) documents\n",
      "[2022-09-26 16:18:37,266] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:18:37,335] adding document #3050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:19:08,756] discarding 34522 tokens: [('kenokee', 1), ('mesquitelocalnews', 1), ('nvcris', 1), ('manchester_township_ami_manchester', 1), ('manchester_township_farm', 1), ('manchester_township_private_residence', 1), ('northfield_township_farm', 1), ('northfield_township_jasman_building', 1), ('northfield_township_rhetech', 1), ('northfield_township_senior_center', 1)]...\n",
      "[2022-09-26 16:19:08,759] keeping 2000000 tokens which were in no less than 0 and no more than 3060000 (=100.0%) documents\n",
      "[2022-09-26 16:19:14,288] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:19:14,389] adding document #3060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:19:46,044] discarding 33232 tokens: [('bodegrave', 1), ('kavelsloten', 1), ('snijdelwijk', 1), ('brogilo', 1), ('wellerondom', 1), ('kantrud', 1), ('pioneercare', 1), ('cromstrien', 1), ('de_lier', 1), ('kopergravure', 1)]...\n",
      "[2022-09-26 16:19:46,046] keeping 2000000 tokens which were in no less than 0 and no more than 3070000 (=100.0%) documents\n",
      "[2022-09-26 16:19:51,478] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:19:51,579] adding document #3070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:20:27,650] discarding 35107 tokens: [('oudelaan', 1), ('saltshof', 1), ('sluiskamp', 1), ('uilenboom', 1), ('ververt', 1), ('weertjes', 1), ('zesakkers', 1), ('zevendreef', 1), ('nijstraote', 1), ('slingestream', 1)]...\n",
      "[2022-09-26 16:20:27,652] keeping 2000000 tokens which were in no less than 0 and no more than 3080000 (=100.0%) documents\n",
      "[2022-09-26 16:20:33,184] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:20:33,287] adding document #3080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:21:03,968] discarding 34962 tokens: [('stanstock', 1), ('ajdm', 1), ('reisterstownfest', 1), ('marshmont', 1), ('brookeside', 1), ('karylbrook', 1), ('pealiquor', 1), ('hrobar', 1), ('foxgrape', 1), ('haketts', 1)]...\n",
      "[2022-09-26 16:21:03,969] keeping 2000000 tokens which were in no less than 0 and no more than 3090000 (=100.0%) documents\n",
      "[2022-09-26 16:21:09,413] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:21:09,515] adding document #3090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:21:43,895] discarding 34884 tokens: [('巍勳洪業', 1), ('巧尔好度', 1), ('布德執義', 1), ('帝号', 1), ('应事有功', 1), ('弘道宣烈平世肅憲懿孝康惠大成大王', 1), ('弥年寿考', 1), ('强学好问', 1), ('强毅信正', 1), ('强毅果敢', 1)]...\n",
      "[2022-09-26 16:21:43,897] keeping 2000000 tokens which were in no less than 0 and no more than 3100000 (=100.0%) documents\n",
      "[2022-09-26 16:21:49,768] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:21:49,868] adding document #3100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:22:26,045] discarding 37983 tokens: [('hahira_elementary_school', 1), ('hairaairee', 1), ('edith_garlow_johnston_lakes_library', 1), ('featherstonhough', 1), ('sbogai', 1), ('tahlonekay', 1), ('talonega', 1), ('darienafricanbaptistchurch', 1), ('darienmethchurch', 1), ('darienpostbellumhouse', 1)]...\n",
      "[2022-09-26 16:22:26,047] keeping 2000000 tokens which were in no less than 0 and no more than 3110000 (=100.0%) documents\n",
      "[2022-09-26 16:22:30,080] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:22:30,151] adding document #3110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:23:03,099] discarding 37752 tokens: [('claiamncy', 1), ('mjarb', 1), ('yxstian', 1), ('rollertainment', 1), ('cherukanukna', 1), ('hakiwuna', 1), ('calvarynuevo', 1), ('olivegrovechurch', 1), ('firecliff', 1), ('surcom', 1)]...\n",
      "[2022-09-26 16:23:03,102] keeping 2000000 tokens which were in no less than 0 and no more than 3120000 (=100.0%) documents\n",
      "[2022-09-26 16:23:07,179] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:23:07,251] adding document #3120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:23:39,401] discarding 33073 tokens: [('icantellyou', 1), ('idontremember', 1), ('regionsontario', 1), ('demureness#0§', 1), ('marylike', 1), ('perchesk', 1), ('cummein', 1), ('wadesburgh', 1), ('balchild', 1), ('yatayti', 1)]...\n",
      "[2022-09-26 16:23:39,403] keeping 2000000 tokens which were in no less than 0 and no more than 3130000 (=100.0%) documents\n",
      "[2022-09-26 16:23:45,215] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:23:45,318] adding document #3130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:24:19,420] discarding 37066 tokens: [('taskeke', 1), ('tuskegeealabama', 1), ('gurleysville', 1), ('tvrcfl', 1), ('beysiegle', 1), ('unorna', 1), ('marengon', 1), ('helvingstons', 1), ('watvc', 1), ('bouzage', 1)]...\n",
      "[2022-09-26 16:24:19,422] keeping 2000000 tokens which were in no less than 0 and no more than 3140000 (=100.0%) documents\n",
      "[2022-09-26 16:24:23,575] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:24:23,646] adding document #3140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:24:58,584] discarding 32413 tokens: [('amangkuratiii', 1), ('madurais', 1), ('celebertti', 1), ('eslaquit', 1), ('amatto', 1), ('gontse', 1), ('kamillah', 1), ('ramarimela', 1), ('trumain', 1), ('kvenipneveli', 1)]...\n",
      "[2022-09-26 16:24:58,586] keeping 2000000 tokens which were in no less than 0 and no more than 3150000 (=100.0%) documents\n",
      "[2022-09-26 16:25:04,022] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:25:04,121] adding document #3150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:25:34,521] discarding 31718 tokens: [('alipato', 1), ('banatan', 1), ('bilinan', 1), ('boobita', 1), ('guding', 1), ('hinalay', 1), ('kabilin', 1), ('manupil', 1), ('oooops', 1), ('pasukob', 1)]...\n",
      "[2022-09-26 16:25:34,523] keeping 2000000 tokens which were in no less than 0 and no more than 3160000 (=100.0%) documents\n",
      "[2022-09-26 16:25:38,581] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:25:38,651] adding document #3160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:26:11,617] discarding 31678 tokens: [('nolanriverbridge', 1), ('gagliani', 1), ('antiforma', 1), ('granitoides', 1), ('gravimétrico', 1), ('montancil', 1), ('tamuja', 1), ('tances', 1), ('globularly', 1), ('leptinidae', 1)]...\n",
      "[2022-09-26 16:26:11,619] keeping 2000000 tokens which were in no less than 0 and no more than 3170000 (=100.0%) documents\n",
      "[2022-09-26 16:26:17,215] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:26:17,318] adding document #3170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:26:48,668] discarding 35971 tokens: [('shriral', 1), ('tulajā', 1), ('turajā', 1), ('ugrachanda', 1), ('vaiṣṇavī', 1), ('vīriṇī', 1), ('ćaṇḍika', 1), ('ćāmuṇḍā', 1), ('abilawa', 1), ('adityaredhaya', 1)]...\n",
      "[2022-09-26 16:26:48,670] keeping 2000000 tokens which were in no less than 0 and no more than 3180000 (=100.0%) documents\n",
      "[2022-09-26 16:26:52,913] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:26:52,983] adding document #3180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:27:25,444] discarding 31905 tokens: [('shaugran', 1), ('ceiad', 1), ('shelembe', 1), ('deeltijdfeminisme', 1), ('eerlijke', 1), ('hulpbetoon', 1), ('schaamte', 1), ('tendeloo', 1), ('vlijtige', 1), ('vrouwenarbeidloten', 1)]...\n",
      "[2022-09-26 16:27:25,447] keeping 2000000 tokens which were in no less than 0 and no more than 3190000 (=100.0%) documents\n",
      "[2022-09-26 16:27:31,361] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:27:31,462] adding document #3190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:28:06,080] discarding 32002 tokens: [('pissboy', 1), ('centralbahnplatz', 1), ('jesenik', 1), ('moravskoslezsky', 1), ('mscb', 1), ('opavou', 1), ('valšov', 1), ('vmoravskoslezském', 1), ('würbenthal', 1), ('alpinizmu', 1)]...\n",
      "[2022-09-26 16:28:06,081] keeping 2000000 tokens which were in no less than 0 and no more than 3200000 (=100.0%) documents\n",
      "[2022-09-26 16:28:11,577] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:28:11,680] adding document #3200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:28:50,549] discarding 35217 tokens: [('khufwy', 1), ('nebetu', 1), ('templeheka', 1), ('ḥk', 1), ('menchit', 1), ('satjet', 1), ('satjit', 1), ('anoukis', 1), ('anouké', 1), ('anucis', 1)]...\n",
      "[2022-09-26 16:28:50,551] keeping 2000000 tokens which were in no less than 0 and no more than 3210000 (=100.0%) documents\n",
      "[2022-09-26 16:28:56,029] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:28:56,132] adding document #3210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:29:27,655] discarding 32090 tokens: [('altmejd', 1), ('bazzo', 1), ('dutrizac', 1), ('guénette', 1), ('akkent', 1), ('chamob', 1), ('gyanvihar', 1), ('panchmarhi', 1), ('fuerststift', 1), ('komádi', 1)]...\n",
      "[2022-09-26 16:29:27,656] keeping 2000000 tokens which were in no less than 0 and no more than 3220000 (=100.0%) documents\n",
      "[2022-09-26 16:29:33,115] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:29:33,217] adding document #3220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:30:02,353] discarding 34370 tokens: [('bhaudhara', 1), ('cumuri', 1), ('dakṣiṇā', 1), ('daśagvas', 1), ('ekashtaka', 1), ('emuṣa', 1), ('grishti', 1), ('hárī', 1), ('iminā', 1), ('indradhanus', 1)]...\n",
      "[2022-09-26 16:30:02,355] keeping 2000000 tokens which were in no less than 0 and no more than 3230000 (=100.0%) documents\n",
      "[2022-09-26 16:30:06,422] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:30:06,491] adding document #3230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:30:37,994] discarding 38118 tokens: [('ffectively', 1), ('bargstall', 1), ('bissee', 1), ('borgdorf', 1), ('böhnhusen', 1), ('friedrichsgraben', 1), ('friedrichsholm', 1), ('grevenkrug', 1), ('mielkendorf', 1), ('mühbrook', 1)]...\n",
      "[2022-09-26 16:30:37,996] keeping 2000000 tokens which were in no less than 0 and no more than 3240000 (=100.0%) documents\n",
      "[2022-09-26 16:30:42,109] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:30:42,180] adding document #3240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:31:19,028] discarding 40737 tokens: [('pontsuspendutardes', 1), ('apedomēn', 1), ('apodōsomai', 1), ('condairc', 1), ('cowish', 1), ('derḱ₂', 1), ('eirēka', 1), ('eirēmai', 1), ('elekhthēn', 1), ('eleksa', 1)]...\n",
      "[2022-09-26 16:31:19,030] keeping 2000000 tokens which were in no less than 0 and no more than 3250000 (=100.0%) documents\n",
      "[2022-09-26 16:31:23,128] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:31:23,200] adding document #3250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:31:53,747] discarding 35388 tokens: [('ːna', 1), ('ːnastɛ', 1), ('ːra', 1), ('ːrat', 1), ('ːrjɛ', 1), ('ːɳa', 1), ('blapp', 1), ('fs_plugin_cache', 1), ('fs_plugin_encrypt', 1), ('hsparwed', 1)]...\n",
      "[2022-09-26 16:31:53,750] keeping 2000000 tokens which were in no less than 0 and no more than 3260000 (=100.0%) documents\n",
      "[2022-09-26 16:31:59,259] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:31:59,363] adding document #3260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:32:30,557] discarding 35344 tokens: [('dolabello', 1), ('eaugoras', 1), ('leucimme', 1), ('luciuis', 1), ('milawanda', 1), ('peloponnessian', 1), ('phyrrhias', 1), ('sindlike', 1), ('syracue', 1), ('vensontio', 1)]...\n",
      "[2022-09-26 16:32:30,562] keeping 2000000 tokens which were in no less than 0 and no more than 3270000 (=100.0%) documents\n",
      "[2022-09-26 16:32:34,815] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:32:34,885] adding document #3270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:33:06,714] discarding 36483 tokens: [('agenorides', 1), ('aionian', 1), ('ereneia', 1), ('straighcut', 1), ('tootled', 1), ('metionadae', 1), ('μητίονος', 1), ('μητίων', 1), ('πάτση', 1), ('donnot', 1)]...\n",
      "[2022-09-26 16:33:06,716] keeping 2000000 tokens which were in no less than 0 and no more than 3280000 (=100.0%) documents\n",
      "[2022-09-26 16:33:11,692] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:33:11,762] adding document #3280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:33:42,986] discarding 29635 tokens: [('γόητα', 1), ('ζέμελεν', 1), ('dentellets', 1), ('iambadoule', 1), ('iambadule', 1), ('sbelsurd', 1), ('svelsurdus', 1), ('yambadula', 1), ('zbelsourdos', 1), ('zbelthiourdes', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:33:42,988] keeping 2000000 tokens which were in no less than 0 and no more than 3290000 (=100.0%) documents\n",
      "[2022-09-26 16:33:48,583] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:33:48,685] adding document #3290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:34:21,013] discarding 36724 tokens: [('catwalked', 1), ('cgdt', 1), ('haweya', 1), ('pambili', 1), ('kakouros', 1), ('tilstonebank', 1), ('albez', 1), ('alron', 1), ('dafana', 1), ('helperon', 1)]...\n",
      "[2022-09-26 16:34:21,015] keeping 2000000 tokens which were in no less than 0 and no more than 3300000 (=100.0%) documents\n",
      "[2022-09-26 16:34:25,046] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:34:25,116] adding document #3300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:34:58,532] discarding 39165 tokens: [('kadhalare', 1), ('madheswaran', 1), ('poonthendrale', 1), ('poovukkul', 1), ('puthaiyal', 1), ('velaiyadu', 1), ('kandathu', 1), ('mayyama', 1), ('parvathiy', 1), ('pathimoonam', 1)]...\n",
      "[2022-09-26 16:34:58,534] keeping 2000000 tokens which were in no less than 0 and no more than 3310000 (=100.0%) documents\n",
      "[2022-09-26 16:35:04,005] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:35:04,108] adding document #3310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:35:34,153] discarding 34276 tokens: [('kudamati', 1), ('ganovex', 1), ('paleopacific', 1), ('pedalspiel', 1), ('coeruleata', 1), ('cosmopepla', 1), ('cruciaria', 1), ('intergressus', 1), ('lintneriana', 1), ('uhleri', 1)]...\n",
      "[2022-09-26 16:35:34,155] keeping 2000000 tokens which were in no less than 0 and no more than 3320000 (=100.0%) documents\n",
      "[2022-09-26 16:35:38,923] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:35:38,994] adding document #3320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:36:08,687] discarding 31158 tokens: [('abbotioannis', 1), ('agrimisargos', 1), ('balourdosnikolaos', 1), ('chaldeosioannis', 1), ('christeasdimitrios', 1), ('daispavlos', 1), ('dekavalasnikolaos', 1), ('diomatarasevangelos', 1), ('dolaspetros', 1), ('drivaskonstantinos', 1)]...\n",
      "[2022-09-26 16:36:08,688] keeping 2000000 tokens which were in no less than 0 and no more than 3330000 (=100.0%) documents\n",
      "[2022-09-26 16:36:14,291] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:36:14,392] adding document #3330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:36:44,121] discarding 38879 tokens: [('drui', 1), ('voate', 1), ('aliez', 1), ('asseylum', 1), ('barouhcruz', 1), ('blackschleger', 1), ('cruheteo', 1), ('cyua', 1), ('darzana', 1), ('dioskuria', 1)]...\n",
      "[2022-09-26 16:36:44,122] keeping 2000000 tokens which were in no less than 0 and no more than 3340000 (=100.0%) documents\n",
      "[2022-09-26 16:36:48,169] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:36:48,240] adding document #3340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:37:17,736] discarding 33169 tokens: [('sretnom', 1), ('zvijezdom', 1), ('cymet', 1), ('flashgamm', 1), ('joonam', 1), ('nabood', 1), ('radiojavan', 1), ('zarebin', 1), ('aydinlar', 1), ('bossae', 1)]...\n",
      "[2022-09-26 16:37:17,738] keeping 2000000 tokens which were in no less than 0 and no more than 3350000 (=100.0%) documents\n",
      "[2022-09-26 16:37:23,331] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:37:23,434] adding document #3350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:37:56,100] discarding 32947 tokens: [('euhm', 1), ('rivioli', 1), ('beircheart', 1), ('cagley', 1), ('childrenis', 1), ('deckner', 1), ('gallavich', 1), ('gehlfuss', 1), ('goreshter', 1), ('hermiker', 1)]...\n",
      "[2022-09-26 16:37:56,103] keeping 2000000 tokens which were in no less than 0 and no more than 3360000 (=100.0%) documents\n",
      "[2022-09-26 16:38:01,623] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:38:01,726] adding document #3360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:38:32,311] discarding 35259 tokens: [('whorrongary', 1), ('worongary', 1), ('jalaiah', 1), ('kritical', 1), ('reazy', 1), ('arxius', 1), ('dokumentart', 1), ('forumdocbh', 1), ('kaleidoscopy', 1), ('niedermayrite', 1)]...\n",
      "[2022-09-26 16:38:32,313] keeping 2000000 tokens which were in no less than 0 and no more than 3370000 (=100.0%) documents\n",
      "[2022-09-26 16:38:37,347] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:38:37,420] adding document #3370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:39:08,776] discarding 35105 tokens: [('trumpsc', 1), ('boelling', 1), ('artesanas', 1), ('comunitarias', 1), ('coopsalinas', 1), ('fugjs', 1), ('funconquerucom', 1), ('funorsal', 1), ('producoop', 1), ('salinerito', 1)]...\n",
      "[2022-09-26 16:39:08,778] keeping 2000000 tokens which were in no less than 0 and no more than 3380000 (=100.0%) documents\n",
      "[2022-09-26 16:39:14,436] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:39:14,539] adding document #3380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:39:46,344] discarding 36502 tokens: [('chrappan', 1), ('heverin', 1), ('milansky', 1), ('zilisnky', 1), ('kalloriyin', 1), ('lingeswaran', 1), ('paati', 1), ('sandhanapandi', 1), ('shanmugapandi', 1), ('vellaipandi', 1)]...\n",
      "[2022-09-26 16:39:46,346] keeping 2000000 tokens which were in no less than 0 and no more than 3390000 (=100.0%) documents\n",
      "[2022-09-26 16:39:52,037] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:39:52,139] adding document #3390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:40:21,194] discarding 40892 tokens: [('menschenversand', 1), ('nortrud', 1), ('pogatschar', 1), ('silbentrennung', 1), ('wurlitz', 1), ('dermid', 1), ('balagana', 1), ('balambina', 1), ('bantalang', 1), ('herlang', 1)]...\n",
      "[2022-09-26 16:40:21,196] keeping 2000000 tokens which were in no less than 0 and no more than 3400000 (=100.0%) documents\n",
      "[2022-09-26 16:40:25,245] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:40:25,317] adding document #3400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:40:55,220] discarding 36920 tokens: [('actiom', 1), ('btishlyrics', 1), ('gazira', 1), ('hitlr', 1), ('welily', 1), ('obendoerfer', 1), ('modb', 1), ('فضل', 1), ('charldorp', 1), ('sasnev', 1)]...\n",
      "[2022-09-26 16:40:55,222] keeping 2000000 tokens which were in no less than 0 and no more than 3410000 (=100.0%) documents\n",
      "[2022-09-26 16:41:01,089] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:41:01,194] adding document #3410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:41:32,205] discarding 33722 tokens: [('ingorbernables', 1), ('ebijuro', 1), ('東京都立図書館', 1), ('演劇博物館', 1), ('立命館大学', 1), ('marfatia', 1), ('montagnde', 1), ('soliant', 1), ('hildah', 1), ('masalethulini', 1)]...\n",
      "[2022-09-26 16:41:32,207] keeping 2000000 tokens which were in no less than 0 and no more than 3420000 (=100.0%) documents\n",
      "[2022-09-26 16:41:37,716] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:41:37,819] adding document #3420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:42:08,323] discarding 31562 tokens: [('geofisika', 1), ('hidrología', 1), ('klimatologi', 1), ('meteorologi', 1), ('dianah', 1), ('poschman', 1), ('strathford', 1), ('tunell', 1), ('zdinak', 1), ('nachitoch', 1)]...\n",
      "[2022-09-26 16:42:08,326] keeping 2000000 tokens which were in no less than 0 and no more than 3430000 (=100.0%) documents\n",
      "[2022-09-26 16:42:14,214] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:42:14,361] adding document #3430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:42:46,129] discarding 33592 tokens: [('ösk', 1), ('sourtoe', 1), ('pivaronas', 1), ('yrkeskoler', 1), ('michingeo', 1), ('gioulekas', 1), ('bhrigurari', 1), ('náttmál', 1), ('romaniafranceswitzerlandgermany', 1), ('vackert', 1)]...\n",
      "[2022-09-26 16:42:46,131] keeping 2000000 tokens which were in no less than 0 and no more than 3440000 (=100.0%) documents\n",
      "[2022-09-26 16:42:51,643] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:42:51,747] adding document #3440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:43:27,406] discarding 36755 tokens: [('babhantoli', 1), ('bakhtaurganj', 1), ('barahkurwa', 1), ('bhikhanpura', 1), ('bhualpur', 1), ('gurbiswa', 1), ('kusiari', 1), ('grapozzo', 1), ('fungologiam', 1), ('reinshiella', 1)]...\n",
      "[2022-09-26 16:43:27,407] keeping 2000000 tokens which were in no less than 0 and no more than 3450000 (=100.0%) documents\n",
      "[2022-09-26 16:43:31,457] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:43:31,528] adding document #3450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:44:00,654] discarding 32416 tokens: [('μελάναιγις', 1), ('μόρυχος', 1), ('οἰνεύς', 1), ('περικιόνιος', 1), ('πιθοίγια', 1), ('σκυλλίτας', 1), ('συκίτης', 1), ('τhyiοn', 1), ('ταυροφάγος', 1), ('φαλλήν', 1)]...\n",
      "[2022-09-26 16:44:00,656] keeping 2000000 tokens which were in no less than 0 and no more than 3460000 (=100.0%) documents\n",
      "[2022-09-26 16:44:05,408] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:44:05,480] adding document #3460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:44:33,314] discarding 35698 tokens: [('milnesiidae', 1), ('parachaela', 1), ('pseudobiotus', 1), ('tardigrada§', 1), ('aminopentanedioate', 1), ('studiesthe', 1), ('amphiesmal', 1), ('amphitrophic', 1), ('cleptochloroplasts', 1), ('crepidoodinium', 1)]...\n",
      "[2022-09-26 16:44:33,316] keeping 2000000 tokens which were in no less than 0 and no more than 3470000 (=100.0%) documents\n",
      "[2022-09-26 16:44:38,774] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:44:38,876] adding document #3470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:45:09,043] discarding 34671 tokens: [('神奈川県教育委員会', 1), ('himetsugi', 1), ('okuyugawara', 1), ('alluvional', 1), ('gayatsu', 1), ('harisuribashi', 1), ('iijimagasaki', 1), ('sōtōzan', 1), ('ヶ谷', 1), ('okouzyou', 1)]...\n",
      "[2022-09-26 16:45:09,045] keeping 2000000 tokens which were in no less than 0 and no more than 3480000 (=100.0%) documents\n",
      "[2022-09-26 16:45:14,978] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:45:15,080] adding document #3480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:45:47,437] discarding 49098 tokens: [('sanzhangju', 1), ('wubeizhi', 1), ('三丈菊', 1), ('武備火龍經', 1), ('火戲略', 1), ('爆仗', 1), ('爆竿', 1), ('百丈蓮', 1), ('趙學敏', 1), ('phenylsalicylate', 1)]...\n",
      "[2022-09-26 16:45:47,439] keeping 2000000 tokens which were in no less than 0 and no more than 3490000 (=100.0%) documents\n",
      "[2022-09-26 16:45:53,041] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:45:53,148] adding document #3490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:46:24,580] discarding 32415 tokens: [('palladiopalazzojonesburlingon', 1), ('fuzilieers', 1), ('großwerft', 1), ('nordelbe', 1), ('norderloch', 1), ('fromsqlraw', 1), ('czaich', 1), ('deperras', 1), ('uralkodásának', 1), ('delimato', 1)]...\n",
      "[2022-09-26 16:46:24,581] keeping 2000000 tokens which were in no less than 0 and no more than 3500000 (=100.0%) documents\n",
      "[2022-09-26 16:46:28,742] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:46:28,812] adding document #3500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:47:00,253] discarding 32563 tokens: [('crandover', 1), ('lardendie', 1), ('larendie', 1), ('larondie', 1), ('parascandola', 1), ('thruout', 1), ('eòghain', 1), ('sliochd', 1), ('frodesen', 1), ('gartnere', 1)]...\n",
      "[2022-09-26 16:47:00,256] keeping 2000000 tokens which were in no less than 0 and no more than 3510000 (=100.0%) documents\n",
      "[2022-09-26 16:47:05,821] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:47:05,923] adding document #3510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:47:37,205] discarding 35053 tokens: [('geraniummaderense', 1), ('geranós', 1), ('wildgeranium', 1), ('approximatio', 1), ('binomii', 1), ('expansi', 1), ('favonios', 1), ('gallerey', 1), ('paraenthesis', 1), ('apothgecary', 1)]...\n",
      "[2022-09-26 16:47:37,207] keeping 2000000 tokens which were in no less than 0 and no more than 3520000 (=100.0%) documents\n",
      "[2022-09-26 16:47:43,023] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:47:43,127] adding document #3520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:48:12,494] discarding 31874 tokens: [('ehtnographical', 1), ('polyvocal', 1), ('proccalimed', 1), ('confora', 1), ('courtrey', 1), ('wylackie', 1), ('albasham', 1), ('alkhazam', 1), ('alshath', 1), ('asulamy', 1)]...\n",
      "[2022-09-26 16:48:12,496] keeping 2000000 tokens which were in no less than 0 and no more than 3530000 (=100.0%) documents\n",
      "[2022-09-26 16:48:17,968] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:48:18,071] adding document #3530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:48:47,044] discarding 33341 tokens: [('pitetta', 1), ('pletschuhorn', 1), ('ritord', 1), ('rogneux', 1), ('rossbode', 1), ('sesiajoch', 1), ('sparruhorn', 1), ('toûno', 1), ('tällihorn', 1), ('verosso', 1)]...\n",
      "[2022-09-26 16:48:47,047] keeping 2000000 tokens which were in no less than 0 and no more than 3540000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:48:51,130] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:48:51,201] adding document #3540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:49:19,335] discarding 31012 tokens: [('siddiki', 1), ('waddies', 1), ('yunkanjini', 1), ('raitel', 1), ('righttel', 1), ('marishal', 1), ('dailysportscar', 1), ('racesports', 1), ('adefisayo', 1), ('akinbile', 1)]...\n",
      "[2022-09-26 16:49:19,336] keeping 2000000 tokens which were in no less than 0 and no more than 3550000 (=100.0%) documents\n",
      "[2022-09-26 16:49:23,467] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:49:23,538] adding document #3550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:49:55,724] discarding 33487 tokens: [('mastixioids', 1), ('nyssoids', 1), ('sillyberry', 1), ('fgfy', 1), ('gfgx', 1), ('biophysika', 1), ('bʉd', 1), ('chariticas', 1), ('cuchanec', 1), ('dahaʉi', 1)]...\n",
      "[2022-09-26 16:49:55,726] keeping 2000000 tokens which were in no less than 0 and no more than 3560000 (=100.0%) documents\n",
      "[2022-09-26 16:50:01,306] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:50:01,409] adding document #3560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:50:31,920] discarding 34897 tokens: [('bwà', 1), ('bàdʊ', 1), ('bàlá', 1), ('bàlìyà', 1), ('bàlídú', 1), ('bàlɪ', 1), ('bàndá', 1), ('bànásá', 1), ('bànáá', 1), ('bànāā', 1)]...\n",
      "[2022-09-26 16:50:31,922] keeping 2000000 tokens which were in no less than 0 and no more than 3570000 (=100.0%) documents\n",
      "[2022-09-26 16:50:36,098] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:50:36,170] adding document #3570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:51:07,629] discarding 29466 tokens: [('aguibou', 1), ('panathiniakos', 1), ('thriumphic', 1), ('aitebaar', 1), ('banjaara', 1), ('bazain', 1), ('chahatain', 1), ('chahey', 1), ('chalawa', 1), ('chatharay', 1)]...\n",
      "[2022-09-26 16:51:07,631] keeping 2000000 tokens which were in no less than 0 and no more than 3580000 (=100.0%) documents\n",
      "[2022-09-26 16:51:13,249] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:51:13,351] adding document #3580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:51:46,380] discarding 31606 tokens: [('xingyong', 1), ('artschool', 1), ('barbiersbrug', 1), ('israelskade', 1), ('overhoeks', 1), ('terop', 1), ('catronio', 1), ('hindprints', 1), ('münchehagen', 1), ('הסרטן', 1)]...\n",
      "[2022-09-26 16:51:46,382] keeping 2000000 tokens which were in no less than 0 and no more than 3590000 (=100.0%) documents\n",
      "[2022-09-26 16:51:51,981] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:51:52,083] adding document #3590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:52:22,728] discarding 30878 tokens: [('corregiduria', 1), ('platanilla', 1), ('illiani', 1), ('ansuman', 1), ('vitmayer', 1), ('magnall', 1), ('torrisholme', 1), ('aroit', 1), ('bachsin', 1), ('bawalah', 1)]...\n",
      "[2022-09-26 16:52:22,730] keeping 2000000 tokens which were in no less than 0 and no more than 3600000 (=100.0%) documents\n",
      "[2022-09-26 16:52:26,836] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:52:26,907] adding document #3600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:52:57,338] discarding 35877 tokens: [('zmyrnaîos', 1), ('zmýrna', 1), ('binarytogray', 1), ('decimalof', 1), ('graytobinary', 1), ('keybounce', 1), ('prototalk', 1), ('togray', 1), ('balancedthe', 1), ('advancedtransit', 1)]...\n",
      "[2022-09-26 16:52:57,340] keeping 2000000 tokens which were in no less than 0 and no more than 3610000 (=100.0%) documents\n",
      "[2022-09-26 16:53:02,886] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:53:02,990] adding document #3610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:53:33,397] discarding 29466 tokens: [('fdba', 1), ('atınç', 1), ('bayrampaşaspor', 1), ('fleetcorp', 1), ('karaal', 1), ('kurumuş', 1), ('küçükköylü', 1), ('majorworx', 1), ('mogaz', 1), ('passolig', 1)]...\n",
      "[2022-09-26 16:53:33,398] keeping 2000000 tokens which were in no less than 0 and no more than 3620000 (=100.0%) documents\n",
      "[2022-09-26 16:53:38,743] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:53:38,847] adding document #3620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:54:06,340] discarding 28606 tokens: [('rosingana', 1), ('ichthyologyahead', 1), ('melanocheilus', 1), ('hellegård', 1), ('erivelto', 1), ('gubiani', 1), ('devanyi', 1), ('inequalitites', 1), ('podesva', 1), ('ndmappingorbit', 1)]...\n",
      "[2022-09-26 16:54:06,342] keeping 2000000 tokens which were in no less than 0 and no more than 3630000 (=100.0%) documents\n",
      "[2022-09-26 16:54:12,180] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:54:12,251] adding document #3630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:54:41,448] discarding 31770 tokens: [('mecandance', 1), ('supermartxé', 1), ('tilllate', 1), ('vorgias', 1), ('valdesius', 1), ('prakhongchit', 1), ('sülümenli', 1), ('jerusalém', 1), ('flvs', 1), ('industrilization', 1)]...\n",
      "[2022-09-26 16:54:41,450] keeping 2000000 tokens which were in no less than 0 and no more than 3640000 (=100.0%) documents\n",
      "[2022-09-26 16:54:46,692] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:54:46,763] adding document #3640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:55:12,476] discarding 29102 tokens: [('pandwa', 1), ('colunistas', 1), ('crpcom', 1), ('pontífica', 1), ('geofences', 1), ('choudhrie', 1), ('haagenrud', 1), ('buscas', 1), ('maltratadas', 1), ('kahike', 1)]...\n",
      "[2022-09-26 16:55:12,478] keeping 2000000 tokens which were in no less than 0 and no more than 3650000 (=100.0%) documents\n",
      "[2022-09-26 16:55:18,145] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:55:18,248] adding document #3650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:55:51,114] discarding 39090 tokens: [('cliftonky', 1), ('clubbist', 1), ('devadesatka', 1), ('devadesátka', 1), ('jestřábe', 1), ('junackou', 1), ('poklusu', 1), ('pokracuje', 1), ('rikitan', 1), ('sedmy', 1)]...\n",
      "[2022-09-26 16:55:51,116] keeping 2000000 tokens which were in no less than 0 and no more than 3660000 (=100.0%) documents\n",
      "[2022-09-26 16:55:56,837] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:55:56,942] adding document #3660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:56:28,065] discarding 31003 tokens: [('differentional', 1), ('kм', 1), ('parametry', 1), ('swepos', 1), ('tλa', 1), ('δia', 1), ('δṫa', 1), ('гj', 1), ('омв', 1), ('смв', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 16:56:28,067] keeping 2000000 tokens which were in no less than 0 and no more than 3670000 (=100.0%) documents\n",
      "[2022-09-26 16:56:32,164] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:56:32,235] adding document #3670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:57:02,867] discarding 30527 tokens: [('ʱəɾ', 1), ('populationt', 1), ('prognózy', 1), ('stárnoucí', 1), ('úmrtnosti', 1), ('kantadarsaner', 1), ('tatparyya', 1), ('albicollisdf', 1), ('mlongipennis', 1), ('şivanxapînok', 1)]...\n",
      "[2022-09-26 16:57:02,869] keeping 2000000 tokens which were in no less than 0 and no more than 3680000 (=100.0%) documents\n",
      "[2022-09-26 16:57:07,957] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:57:08,060] adding document #3680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:57:37,427] discarding 31757 tokens: [('xaferi', 1), ('reslant', 1), ('batracomiomachìa', 1), ('caduche', 1), ('esclude', 1), ('innumerabili', 1), ('irraggia', 1), ('paralipòmeni', 1), ('sensibil', 1), ('taldegardo', 1)]...\n",
      "[2022-09-26 16:57:37,428] keeping 2000000 tokens which were in no less than 0 and no more than 3690000 (=100.0%) documents\n",
      "[2022-09-26 16:57:41,573] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:57:41,645] adding document #3690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:58:09,701] discarding 31813 tokens: [('sankoukan', 1), ('ketenensi', 1), ('pampilonensis', 1), ('isocyclotron', 1), ('multimagnet', 1), ('gyradius', 1), ('hardshore', 1), ('medcu', 1), ('portlanda', 1), ('portlandlandmarks', 1)]...\n",
      "[2022-09-26 16:58:09,702] keeping 2000000 tokens which were in no less than 0 and no more than 3700000 (=100.0%) documents\n",
      "[2022-09-26 16:58:13,766] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:58:13,837] adding document #3700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:58:44,284] discarding 32931 tokens: [('vunnerius', 1), ('fengzong', 1), ('politiican', 1), ('bosphorean', 1), ('cclxix', 1), ('carateristical', 1), ('consularius', 1), ('lastoi', 1), ('palaeoveneti', 1), ('redentór', 1)]...\n",
      "[2022-09-26 16:58:44,285] keeping 2000000 tokens which were in no less than 0 and no more than 3710000 (=100.0%) documents\n",
      "[2022-09-26 16:58:48,431] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:58:48,504] adding document #3710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:59:31,107] discarding 33703 tokens: [('diplobrachia', 1), ('galathealinum', 1), ('heptobrachia', 1), ('lamellisabella', 1), ('nereilinum', 1), ('opisthosome', 1), ('paraescarpia', 1), ('polybrachia', 1), ('siboglinids', 1), ('siboglinoides', 1)]...\n",
      "[2022-09-26 16:59:31,109] keeping 2000000 tokens which were in no less than 0 and no more than 3720000 (=100.0%) documents\n",
      "[2022-09-26 16:59:36,606] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 16:59:36,710] adding document #3720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:00:09,527] discarding 35987 tokens: [('vlahvite', 1), ('гангстером', 1), ('literaturecollection', 1), ('philanthromathematics', 1), ('pskyscraper', 1), ('superlatrives', 1), ('vereton', 1), ('wchord', 1), ('minibust', 1), ('cyberleaf', 1)]...\n",
      "[2022-09-26 17:00:09,529] keeping 2000000 tokens which were in no less than 0 and no more than 3730000 (=100.0%) documents\n",
      "[2022-09-26 17:00:14,325] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:00:14,397] adding document #3730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:00:45,157] discarding 33643 tokens: [('beihuan', 1), ('深圳滨海大道', 1), ('滨海大道', 1), ('barbenson', 1), ('blackbeans', 1), ('breakfronts', 1), ('tadec', 1), ('toplights', 1), ('papillaire', 1), ('printanière', 1)]...\n",
      "[2022-09-26 17:00:45,159] keeping 2000000 tokens which were in no less than 0 and no more than 3740000 (=100.0%) documents\n",
      "[2022-09-26 17:00:50,665] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:00:50,767] adding document #3740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:01:17,508] discarding 37820 tokens: [('roshak', 1), ('torsoed', 1), ('yongjae', 1), ('corthron', 1), ('schankman', 1), ('berthame', 1), ('kensy', 1), ('weeka', 1), ('wuiswell', 1), ('classdojo', 1)]...\n",
      "[2022-09-26 17:01:17,510] keeping 2000000 tokens which were in no less than 0 and no more than 3750000 (=100.0%) documents\n",
      "[2022-09-26 17:01:21,785] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:01:21,859] adding document #3750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:02:00,753] discarding 31757 tokens: [('chlodna', 1), ('kurant', 1), ('prezerwatywy', 1), ('waggerman', 1), ('escheresque', 1), ('overpop', 1), ('chagoopa', 1), ('hawlsey', 1), ('nydiver', 1), ('slissate', 1)]...\n",
      "[2022-09-26 17:02:00,757] keeping 2000000 tokens which were in no less than 0 and no more than 3760000 (=100.0%) documents\n",
      "[2022-09-26 17:02:06,317] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:02:06,421] adding document #3760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:02:34,395] discarding 30618 tokens: [('māpou', 1), ('roantree', 1), ('ashenbach', 1), ('ketrick', 1), ('zeimer', 1), ('dantreume', 1), ('yancich', 1), ('beniko', 1), ('recine', 1), ('abercych', 1)]...\n",
      "[2022-09-26 17:02:34,397] keeping 2000000 tokens which were in no less than 0 and no more than 3770000 (=100.0%) documents\n",
      "[2022-09-26 17:02:40,000] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:02:40,105] adding document #3770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:03:10,915] discarding 33698 tokens: [('garinati', 1), ('juard', 1), ('baviana', 1), ('cafreriana', 1), ('marksi', 1), ('htudies', 1), ('sakiestewa', 1), ('hazeleye', 1), ('lonidaw', 1), ('olondaw', 1)]...\n",
      "[2022-09-26 17:03:10,917] keeping 2000000 tokens which were in no less than 0 and no more than 3780000 (=100.0%) documents\n",
      "[2022-09-26 17:03:16,424] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:03:16,527] adding document #3780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:03:42,892] discarding 28566 tokens: [('liscourt', 1), ('barleet', 1), ('amabele', 1), ('amagogotya', 1), ('amahleke', 1), ('amalinde', 1), ('amandlambe', 1), ('amandungwana', 1), ('amantinde', 1), ('amaqwathi', 1)]...\n",
      "[2022-09-26 17:03:42,894] keeping 2000000 tokens which were in no less than 0 and no more than 3790000 (=100.0%) documents\n",
      "[2022-09-26 17:03:48,438] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:03:48,542] adding document #3790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:04:16,666] discarding 34532 tokens: [('hakaritai', 1), ('hiruobi', 1), ('hōshanō', 1), ('jōshiki', 1), ('kankintōbō', 1), ('midarana', 1), ('nettai', 1), ('puchisuto', 1), ('shouin', 1), ('sodatete', 1)]...\n",
      "[2022-09-26 17:04:16,668] keeping 2000000 tokens which were in no less than 0 and no more than 3800000 (=100.0%) documents\n",
      "[2022-09-26 17:04:22,261] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:04:22,365] adding document #3800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:04:56,624] discarding 37452 tokens: [('faivel', 1), ('dopl', 1), ('katolícka', 1), ('adrean', 1), ('manantlán', 1), ('nigerrimum', 1), ('aarot', 1), ('tecneudosia', 1), ('gobbelschroy', 1), ('dipmac', 1)]...\n",
      "[2022-09-26 17:04:56,626] keeping 2000000 tokens which were in no less than 0 and no more than 3810000 (=100.0%) documents\n",
      "[2022-09-26 17:05:00,780] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:05:00,852] adding document #3810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:05:29,290] discarding 35102 tokens: [('istchenko', 1), ('lebedevii', 1), ('oricilla', 1), ('planatophyton', 1), ('protohyenia', 1), ('salopense', 1), ('sartilmania', 1), ('szaferi', 1), ('teruelia', 1), ('thursophyton', 1)]...\n",
      "[2022-09-26 17:05:29,292] keeping 2000000 tokens which were in no less than 0 and no more than 3820000 (=100.0%) documents\n",
      "[2022-09-26 17:05:33,408] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:05:33,480] adding document #3820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:06:02,746] discarding 37047 tokens: [('ضامن', 1), ('stragmeister', 1), ('eletskiy', 1), ('karpachev', 1), ('qidian', 1), ('shangpu', 1), ('ltbc', 1), ('choppas', 1), ('lazurick', 1), ('bymell', 1)]...\n",
      "[2022-09-26 17:06:02,748] keeping 2000000 tokens which were in no less than 0 and no more than 3830000 (=100.0%) documents\n",
      "[2022-09-26 17:06:08,469] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:06:08,572] adding document #3830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:06:35,370] discarding 30327 tokens: [('rojpojchanarat', 1), ('thong_yib', 1), ('wangpaichitr', 1), ('auroleus', 1), ('elitea', 1), ('realpolitician', 1), ('schaacks', 1), ('dupacks', 1), ('so_debug', 1), ('alhambrismo', 1)]...\n",
      "[2022-09-26 17:06:35,371] keeping 2000000 tokens which were in no less than 0 and no more than 3840000 (=100.0%) documents\n",
      "[2022-09-26 17:06:39,487] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:06:39,558] adding document #3840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:07:08,972] discarding 39638 tokens: [('synoikize', 1), ('synoikized', 1), ('𒋫𒊒𒄿𒊭', 1), ('𒌷𒃾𒇻𒊭', 1), ('chocolart', 1), ('cottahaus', 1), ('dorfackerschule', 1), ('kotzte', 1), ('neckarfront', 1), ('nikolauslauf', 1)]...\n",
      "[2022-09-26 17:07:08,973] keeping 2000000 tokens which were in no less than 0 and no more than 3850000 (=100.0%) documents\n",
      "[2022-09-26 17:07:13,138] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:07:13,210] adding document #3850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:07:43,035] discarding 32318 tokens: [('karems', 1), ('klantorrs', 1), ('lemuran', 1), ('lemurans', 1), ('locots', 1), ('noweks', 1), ('palandor', 1), ('palandorians', 1), ('pterons', 1), ('quarlians', 1)]...\n",
      "[2022-09-26 17:07:43,037] keeping 2000000 tokens which were in no less than 0 and no more than 3860000 (=100.0%) documents\n",
      "[2022-09-26 17:07:49,016] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:07:49,129] adding document #3860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:08:19,976] discarding 31906 tokens: [('amphicrossinae', 1), ('apetinus', 1), ('calonecrinae', 1), ('cillaeinae', 1), ('cillaeopeplus', 1), ('conotelus', 1), ('cyrtostolus', 1), ('epuraeinae', 1), ('eunitidula', 1), ('eupetinus', 1)]...\n",
      "[2022-09-26 17:08:19,978] keeping 2000000 tokens which were in no less than 0 and no more than 3870000 (=100.0%) documents\n",
      "[2022-09-26 17:08:25,477] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:08:25,580] adding document #3870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:08:56,351] discarding 32749 tokens: [('šnḥ', 1), ('šānina', 1), ('apolle', 1), ('countyimgp', 1), ('saaremaasaaremaa', 1), ('tagalahtbaypanorama', 1), ('visitsaaremaa', 1), ('œselia', 1), ('simonology', 1), ('hanslam', 1)]...\n",
      "[2022-09-26 17:08:56,353] keeping 2000000 tokens which were in no less than 0 and no more than 3880000 (=100.0%) documents\n",
      "[2022-09-26 17:09:02,032] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:09:02,136] adding document #3880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:09:35,180] discarding 36117 tokens: [('bantfwabenkhosi', 1), ('swazibusiness', 1), ('lomashasha', 1), ('nertson', 1), ('fonono', 1), ('lindimpi', 1), ('ludlalukhala', 1), ('ndizimande', 1), ('phocweni', 1), ('centralswitzerland', 1)]...\n",
      "[2022-09-26 17:09:35,182] keeping 2000000 tokens which were in no less than 0 and no more than 3890000 (=100.0%) documents\n",
      "[2022-09-26 17:09:39,345] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:09:39,419] adding document #3890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:10:09,186] discarding 32781 tokens: [('achíeve', 1), ('bréaks', 1), ('béauty', 1), ('bíg', 1), ('bíllion', 1), ('bów', 1), ('chevalíer', 1), ('cáught', 1), ('dángerous', 1), ('dáuphin', 1)]...\n",
      "[2022-09-26 17:10:09,188] keeping 2000000 tokens which were in no less than 0 and no more than 3900000 (=100.0%) documents\n",
      "[2022-09-26 17:10:14,902] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:10:15,007] adding document #3900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:10:46,045] discarding 33813 tokens: [('ursăreasa', 1), ('zinti', 1), ('čergarja', 1), ('honeger', 1), ('angvikauto', 1), ('cottage_grove_dump_truck_', 1), ('lane_county', 1), ('liecense', 1), ('reoroyalevictoriaeight', 1), ('yehudon', 1)]...\n",
      "[2022-09-26 17:10:46,046] keeping 2000000 tokens which were in no less than 0 and no more than 3910000 (=100.0%) documents\n",
      "[2022-09-26 17:10:51,803] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:10:51,909] adding document #3910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:11:21,518] discarding 33762 tokens: [('vītam', 1), ('vẽa', 1), ('westernromance', 1), ('àbba', 1), ('àrtu', 1), ('àut', 1), ('ægoa', 1), ('çhantar', 1), ('çincue', 1), ('çiél', 1)]...\n",
      "[2022-09-26 17:11:21,520] keeping 2000000 tokens which were in no less than 0 and no more than 3920000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:11:25,712] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:11:25,784] adding document #3920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:11:59,529] discarding 33534 tokens: [('nemazene', 1), ('novaflurazine', 1), ('novoridazine', 1), ('orimon', 1), ('padophene', 1), ('penthazine', 1), ('phenegic', 1), ('phenovarm', 1), ('phenovis', 1), ('phenoxur', 1)]...\n",
      "[2022-09-26 17:11:59,531] keeping 2000000 tokens which were in no less than 0 and no more than 3930000 (=100.0%) documents\n",
      "[2022-09-26 17:12:05,093] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:12:05,197] adding document #3930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:12:35,047] discarding 34189 tokens: [('fürstenbergska', 1), ('grodparken', 1), ('herrgårdsvägen', 1), ('rotteros', 1), ('snödroppe', 1), ('snöklocka', 1), ('tidningsbiblioteket', 1), ('tjusning', 1), ('tjusningen', 1), ('vågenstjusninggbg', 1)]...\n",
      "[2022-09-26 17:12:35,049] keeping 2000000 tokens which were in no less than 0 and no more than 3940000 (=100.0%) documents\n",
      "[2022-09-26 17:12:40,998] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:12:41,104] adding document #3940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:13:10,547] discarding 30818 tokens: [('kawua', 1), ('lembomawo', 1), ('ranononcu', 1), ('hmod', 1), ('sindrum', 1), ('shmeir', 1), ('aiyya', 1), ('guneris', 1), ('janamadhya', 1), ('kewatillage', 1)]...\n",
      "[2022-09-26 17:13:10,550] keeping 2000000 tokens which were in no less than 0 and no more than 3950000 (=100.0%) documents\n",
      "[2022-09-26 17:13:15,890] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:13:15,963] adding document #3950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:13:49,017] discarding 36327 tokens: [('sicmu', 1), ('uckday', 1), ('upidstay', 1), ('utlerbay', 1), ('utway', 1), ('verlanned', 1), ('ciuchy', 1), ('czipsy', 1), ('geszeft', 1), ('japko', 1)]...\n",
      "[2022-09-26 17:13:49,019] keeping 2000000 tokens which were in no less than 0 and no more than 3960000 (=100.0%) documents\n",
      "[2022-09-26 17:13:54,699] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:13:54,804] adding document #3960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:14:24,361] discarding 29939 tokens: [('budogosch', 1), ('nicolskoye', 1), ('pontonnaya', 1), ('pontonny', 1), ('pontonnyy', 1), ('rybatskoe', 1), ('rybatskoye', 1), ('volkhovstroy', 1), ('gautot', 1), ('neverlnd', 1)]...\n",
      "[2022-09-26 17:14:24,363] keeping 2000000 tokens which were in no less than 0 and no more than 3970000 (=100.0%) documents\n",
      "[2022-09-26 17:14:29,914] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:14:30,018] adding document #3970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:15:01,684] discarding 33377 tokens: [('ǐɗá', 1), ('ɓaɓa', 1), ('ɓɪj', 1), ('ɔnyo', 1), ('ɔnɑ', 1), ('ɔɲ', 1), ('ɔɲɔ', 1), ('ɛmmɛ', 1), ('ɲabi', 1), ('ɲabire', 1)]...\n",
      "[2022-09-26 17:15:01,686] keeping 2000000 tokens which were in no less than 0 and no more than 3980000 (=100.0%) documents\n",
      "[2022-09-26 17:15:07,315] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:15:07,388] adding document #3980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:15:39,570] discarding 33338 tokens: [('億兆京', 1), ('億兆垓', 1), ('億垓', 1), ('兆京', 1), ('兆垓', 1), ('萬京', 1), ('萬億京', 1), ('萬億兆', 1), ('萬億兆京', 1), ('萬兆', 1)]...\n",
      "[2022-09-26 17:15:39,572] keeping 2000000 tokens which were in no less than 0 and no more than 3990000 (=100.0%) documents\n",
      "[2022-09-26 17:15:43,670] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:15:43,741] adding document #3990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:16:16,753] discarding 33915 tokens: [('salasaga', 1), ('xǐxīn', 1), ('mitual', 1), ('altapointe', 1), ('alwayshd', 1), ('cowbellians', 1), ('manicles', 1), ('precipes', 1), ('wyems', 1), ('yoaks', 1)]...\n",
      "[2022-09-26 17:16:16,755] keeping 2000000 tokens which were in no less than 0 and no more than 4000000 (=100.0%) documents\n",
      "[2022-09-26 17:16:20,908] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:16:20,979] adding document #4000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:16:51,111] discarding 33971 tokens: [('polyadenylyl', 1), ('distinctivity', 1), ('minestrare', 1), ('pulticulae', 1), ('espherical', 1), ('ggrl', 1), ('ghouloon', 1), ('marsexpedition', 1), ('militaritarized', 1), ('soukyougurentai', 1)]...\n",
      "[2022-09-26 17:16:51,113] keeping 2000000 tokens which were in no less than 0 and no more than 4010000 (=100.0%) documents\n",
      "[2022-09-26 17:16:55,416] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:16:55,489] adding document #4010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:17:26,708] discarding 35003 tokens: [('humilita', 1), ('schödlberger', 1), ('mlukhia', 1), ('molokia', 1), ('datehis', 1), ('infuriat', 1), ('marcbloch', 1), ('microméga', 1), ('ministérialité', 1), ('rankianer', 1)]...\n",
      "[2022-09-26 17:17:26,711] keeping 2000000 tokens which were in no less than 0 and no more than 4020000 (=100.0%) documents\n",
      "[2022-09-26 17:17:32,281] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:17:32,385] adding document #4020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:18:04,686] discarding 37010 tokens: [('dicotyledoneous', 1), ('memecyclaceae', 1), ('guttales', 1), ('huales', 1), ('parietales§', 1), ('phylétique', 1), ('violoid', 1), ('zeylanol', 1), ('zdenkowski', 1), ('chromatides', 1)]...\n",
      "[2022-09-26 17:18:04,688] keeping 2000000 tokens which were in no less than 0 and no more than 4030000 (=100.0%) documents\n",
      "[2022-09-26 17:18:10,005] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:18:10,080] adding document #4030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:18:44,946] discarding 35524 tokens: [('slicky', 1), ('aufrichtung', 1), ('bachereau', 1), ('darstellungsweisen', 1), ('frankurt', 1), ('fügung', 1), ('geselschap', 1), ('historienbild', 1), ('kaiserproklamation', 1), ('kaisertums', 1)]...\n",
      "[2022-09-26 17:18:44,948] keeping 2000000 tokens which were in no less than 0 and no more than 4040000 (=100.0%) documents\n",
      "[2022-09-26 17:18:50,545] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:18:50,650] adding document #4040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:19:29,835] discarding 56829 tokens: [('hamadryados', 1), ('hamlinensis', 1), ('handgenae', 1), ('grubbylea', 1), ('hebetika', 1), ('heilprinii', 1), ('helikum', 1), ('hemicythere', 1), ('farryn', 1), ('hendryana', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:19:29,837] keeping 2000000 tokens which were in no less than 0 and no more than 4050000 (=100.0%) documents\n",
      "[2022-09-26 17:19:35,562] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:19:35,676] adding document #4050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:20:08,911] discarding 33891 tokens: [('moovs', 1), ('lgpled', 1), ('llgpl', 1), ('gavenlock', 1), ('kibbleplex', 1), ('gopex', 1), ('spehalski', 1), ('zeptowatts', 1), ('pardesand', 1), ('parádeisoswas', 1)]...\n",
      "[2022-09-26 17:20:08,913] keeping 2000000 tokens which were in no less than 0 and no more than 4060000 (=100.0%) documents\n",
      "[2022-09-26 17:20:13,472] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:20:13,544] adding document #4060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:20:42,969] discarding 31702 tokens: [('despiteful', 1), ('extinguisheth', 1), ('gormandy', 1), ('gossip#4§', 1), ('malignity§', 1), ('reinforceor', 1), ('kleftogiorgos', 1), ('auburnerf', 1), ('duncaethal', 1), ('fantasticheskii', 1)]...\n",
      "[2022-09-26 17:20:42,971] keeping 2000000 tokens which were in no less than 0 and no more than 4070000 (=100.0%) documents\n",
      "[2022-09-26 17:20:47,163] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:20:47,236] adding document #4070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:21:18,223] discarding 32614 tokens: [('gsstc', 1), ('guldkusten', 1), ('guldkyst', 1), ('kɛntɛn', 1), ('muuston', 1), ('nsaduaso', 1), ('nwontoma', 1), ('oceanonly', 1), ('olnini', 1), ('tendamba', 1)]...\n",
      "[2022-09-26 17:21:18,225] keeping 2000000 tokens which were in no less than 0 and no more than 4080000 (=100.0%) documents\n",
      "[2022-09-26 17:21:22,599] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:21:22,670] adding document #4080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:21:57,333] discarding 41383 tokens: [('tietze_genus_', 1), ('tietze_moebius', 1), ('visual_proof_mutually_touching_solids', 1), ('amanguchi', 1), ('barazeuz', 1), ('batpal', 1), ('coulao', 1), ('fransiku', 1), ('malyapore', 1), ('multao', 1)]...\n",
      "[2022-09-26 17:21:57,336] keeping 2000000 tokens which were in no less than 0 and no more than 4090000 (=100.0%) documents\n",
      "[2022-09-26 17:22:03,030] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:22:03,136] adding document #4090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:22:34,148] discarding 34699 tokens: [('omikyō', 1), ('shishihito', 1), ('unakata', 1), ('宍人梶媛娘', 1), ('忍壁皇子', 1), ('新田部皇子', 1), ('泊瀬部皇女', 1), ('田形皇女', 1), ('磯城皇子', 1), ('胸形尼子娘', 1)]...\n",
      "[2022-09-26 17:22:34,150] keeping 2000000 tokens which were in no less than 0 and no more than 4100000 (=100.0%) documents\n",
      "[2022-09-26 17:22:39,843] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:22:39,948] adding document #4100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:23:10,513] discarding 39987 tokens: [('empedoklēs', 1), ('opsopaus', 1), ('orphicsing', 1), ('cladothamneae', 1), ('diplarcheae', 1), ('kosteltsky', 1), ('prionotaceae', 1), ('rhododendroideae', 1), ('wittsteinioideae', 1), ('nonabstract', 1)]...\n",
      "[2022-09-26 17:23:10,515] keeping 2000000 tokens which were in no less than 0 and no more than 4110000 (=100.0%) documents\n",
      "[2022-09-26 17:23:16,411] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:23:16,517] adding document #4110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:23:47,697] discarding 33367 tokens: [('simplehtmldom', 1), ('agaruna', 1), ('coextending', 1), ('crystalsdmt', 1), ('inihibitors', 1), ('intrastability', 1), ('neuropharmocology', 1), ('nigerine', 1), ('transmethylated', 1), ('danælog', 1)]...\n",
      "[2022-09-26 17:23:47,699] keeping 2000000 tokens which were in no less than 0 and no more than 4120000 (=100.0%) documents\n",
      "[2022-09-26 17:23:53,283] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:23:53,387] adding document #4120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:24:23,833] discarding 42539 tokens: [('duduko', 1), ('musabimana', 1), ('nyakwana', 1), ('ndabala', 1), ('manvu', 1), ('kasomeno', 1), ('yjmyry', 1), ('conflictogenic', 1), ('dictatogenic', 1), ('kisenye', 1)]...\n",
      "[2022-09-26 17:24:23,835] keeping 2000000 tokens which were in no less than 0 and no more than 4130000 (=100.0%) documents\n",
      "[2022-09-26 17:24:28,135] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:24:28,241] adding document #4130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:24:58,868] discarding 36211 tokens: [('eeengw', 1), ('enèki', 1), ('ezzaħ', 1), ('eːj', 1), ('faanqʼw', 1), ('fadhiiso', 1), ('faɴqʼu', 1), ('fereèyi', 1), ('ferèyi', 1), ('furdaa', 1)]...\n",
      "[2022-09-26 17:24:58,870] keeping 2000000 tokens which were in no less than 0 and no more than 4140000 (=100.0%) documents\n",
      "[2022-09-26 17:25:03,705] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:25:03,778] adding document #4140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:25:45,592] discarding 53433 tokens: [('tamburlane', 1), ('burlandier', 1), ('gsvans', 1), ('jphd', 1), ('spēlynks', 1), ('wetsocks', 1), ('corrasional', 1), ('stygophile', 1), ('stygoxene', 1), ('bìsòng', 1)]...\n",
      "[2022-09-26 17:25:45,593] keeping 2000000 tokens which were in no less than 0 and no more than 4150000 (=100.0%) documents\n",
      "[2022-09-26 17:25:51,263] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:25:51,373] adding document #4150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:26:23,113] discarding 42398 tokens: [('bashkatov', 1), ('bogatchova', 1), ('desilvia', 1), ('elzermann', 1), ('fswc', 1), ('goldenbook', 1), ('goldsmithg', 1), ('jähnichen', 1), ('kotriaga', 1), ('krisitn', 1)]...\n",
      "[2022-09-26 17:26:23,115] keeping 2000000 tokens which were in no less than 0 and no more than 4160000 (=100.0%) documents\n",
      "[2022-09-26 17:26:28,236] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:26:28,309] adding document #4160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:27:04,852] discarding 33838 tokens: [('mahassine', 1), ('mouhidine', 1), ('uralp', 1), ('pehlevi', 1), ('fredh', 1), ('kywebwe', 1), ('miniority', 1), ('jananiponnambalamsendrayan', 1), ('brunoa', 1), ('harlson', 1)]...\n",
      "[2022-09-26 17:27:04,854] keeping 2000000 tokens which were in no less than 0 and no more than 4170000 (=100.0%) documents\n",
      "[2022-09-26 17:27:09,073] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:27:09,144] adding document #4170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:27:43,879] discarding 33981 tokens: [('tlemcin', 1), ('天福鎮寶', 1), ('兑', 1), ('nageyari', 1), ('glukhie', 1), ('inscriptionin', 1), ('hímer', 1), ('masciopinto', 1), ('nambotha', 1), ('levat', 1)]...\n",
      "[2022-09-26 17:27:43,881] keeping 2000000 tokens which were in no less than 0 and no more than 4180000 (=100.0%) documents\n",
      "[2022-09-26 17:27:48,154] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:27:48,227] adding document #4180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:28:20,818] discarding 33467 tokens: [('atthakanāgara', 1), ('belaṭṭhasīsa', 1), ('cityhence', 1), ('daśabāla', 1), ('deliverancewithout', 1), ('haimavāta', 1), ('kusināra', 1), ('kāliṅgabodhi', 1), ('mahākasapa', 1), ('mahīśasaka', 1)]...\n",
      "[2022-09-26 17:28:20,820] keeping 2000000 tokens which were in no less than 0 and no more than 4190000 (=100.0%) documents\n",
      "[2022-09-26 17:28:26,409] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:28:26,513] adding document #4190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:28:58,408] discarding 32452 tokens: [('hiyangthang', 1), ('lamkhai', 1), ('wabagai', 1), ('wrongspeak', 1), ('bjäresjö', 1), ('gierhi', 1), ('giardinelli', 1), ('mempo', 1), ('scened', 1), ('telecino', 1)]...\n",
      "[2022-09-26 17:28:58,411] keeping 2000000 tokens which were in no less than 0 and no more than 4200000 (=100.0%) documents\n",
      "[2022-09-26 17:29:04,201] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:29:04,306] adding document #4200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:29:34,576] discarding 33790 tokens: [('educationruth', 1), ('typicalalbedo', 1), ('aanother', 1), ('alabamo', 1), ('albama', 1), ('alebamon', 1), ('alibamou', 1), ('allibamou', 1), ('buddharaksa', 1), ('phoutthavihan', 1)]...\n",
      "[2022-09-26 17:29:34,578] keeping 2000000 tokens which were in no less than 0 and no more than 4210000 (=100.0%) documents\n",
      "[2022-09-26 17:29:38,992] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:29:39,105] adding document #4210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:30:09,910] discarding 36944 tokens: [('tarid', 2), ('yubazlan', 2), ('japari', 2), ('machanick', 2), ('mandokava', 2), ('piabié', 2), ('silga', 2), ('yemdaogo', 2), ('ncoakhoe', 2), ('ntcox', 2)]...\n",
      "[2022-09-26 17:30:09,912] keeping 2000000 tokens which were in no less than 0 and no more than 4220000 (=100.0%) documents\n",
      "[2022-09-26 17:30:15,644] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:30:15,749] adding document #4220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:30:52,713] discarding 28187 tokens: [('kozyri', 2), ('calamp', 2), ('echame', 2), ('zauberflote', 2), ('cwba', 2), ('rawiwan', 2), ('pseudomonadales', 2), ('adilkhanova', 2), ('adilya', 2), ('nurinisso', 2)]...\n",
      "[2022-09-26 17:30:52,715] keeping 2000000 tokens which were in no less than 0 and no more than 4230000 (=100.0%) documents\n",
      "[2022-09-26 17:30:58,653] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:30:58,757] adding document #4230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:31:30,385] discarding 31765 tokens: [('osmy', 2), ('brezitskyi', 2), ('ceipe', 2), ('chedryk', 2), ('mouget', 2), ('moël', 2), ('awardeddue', 2), ('akbarali', 2), ('anutaraporn', 2), ('baratchi', 2)]...\n",
      "[2022-09-26 17:31:30,387] keeping 2000000 tokens which were in no less than 0 and no more than 4240000 (=100.0%) documents\n",
      "[2022-09-26 17:31:34,638] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:31:34,711] adding document #4240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:32:17,498] discarding 34884 tokens: [('supermolina', 2), ('flobowling', 2), ('guineiske', 2), ('spezioli', 2), ('иогансон', 2), ('zhenu', 2), ('解琬', 2), ('遮努', 2), ('neietsu', 2), ('takekichi', 2)]...\n",
      "[2022-09-26 17:32:17,501] keeping 2000000 tokens which were in no less than 0 and no more than 4250000 (=100.0%) documents\n",
      "[2022-09-26 17:32:23,169] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:32:23,275] adding document #4250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:33:03,731] discarding 36442 tokens: [('superpattern', 2), ('dholshaji', 2), ('tuliya', 2), ('tournamen', 2), ('boztyh', 2), ('básztij', 2), ('básztélyi', 2), ('csabdi', 2), ('hosszúmező', 2), ('lapispatak', 2)]...\n",
      "[2022-09-26 17:33:03,733] keeping 2000000 tokens which were in no less than 0 and no more than 4260000 (=100.0%) documents\n",
      "[2022-09-26 17:33:08,028] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:33:08,101] adding document #4260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:33:41,380] discarding 30492 tokens: [('finishorder', 2), ('basipennis', 2), ('dohlmann', 2), ('xenelaphis', 2), ('domcke', 2), ('mondok', 2), ('kahanovychskyi', 2), ('stalinskyi', 2), ('verkhnya', 2), ('bagenbaggage', 2)]...\n",
      "[2022-09-26 17:33:41,381] keeping 2000000 tokens which were in no less than 0 and no more than 4270000 (=100.0%) documents\n",
      "[2022-09-26 17:33:45,535] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:33:45,606] adding document #4270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:34:17,231] discarding 32467 tokens: [('godholly', 2), ('chamarmazovich', 2), ('yaremich', 2), ('tvplus', 2), ('timefall', 2), ('wakahanada', 2), ('januza', 2), ('hortolomei', 2), ('šustić', 2), ('agolutin', 2)]...\n",
      "[2022-09-26 17:34:17,232] keeping 2000000 tokens which were in no less than 0 and no more than 4280000 (=100.0%) documents\n",
      "[2022-09-26 17:34:22,672] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:34:22,777] adding document #4280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:34:52,422] discarding 29397 tokens: [('moshkinstein', 2), ('hazelwinkel', 2), ('tidarat', 2), ('sangkapreecha', 2), ('kremberg', 2), ('handknattleik', 2), ('ruenes', 2), ('paraguy', 2), ('continto', 2), ('laneastie', 2)]...\n",
      "[2022-09-26 17:34:52,424] keeping 2000000 tokens which were in no less than 0 and no more than 4290000 (=100.0%) documents\n",
      "[2022-09-26 17:34:56,630] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:34:56,702] adding document #4290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:35:30,032] discarding 29857 tokens: [('cherfolio', 2), ('natalsya', 2), ('dedier', 2), ('favcars', 2), ('modulons', 2), ('cephalosome', 2), ('urosomal', 2), ('jewad', 2), ('bezawy', 2), ('dewidar', 2)]...\n",
      "[2022-09-26 17:35:30,034] keeping 2000000 tokens which were in no less than 0 and no more than 4300000 (=100.0%) documents\n",
      "[2022-09-26 17:35:35,981] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:35:36,086] adding document #4300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:36:07,286] discarding 32508 tokens: [('ohale', 2), ('okeoghene', 2), ('boattin', 2), ('cernoia', 2), ('ejangue', 2), ('volksernährung', 2), ('kaličanin', 2), ('kriznar', 2), ('pribanović', 2), ('tiringer', 2)]...\n",
      "[2022-09-26 17:36:07,288] keeping 2000000 tokens which were in no less than 0 and no more than 4310000 (=100.0%) documents\n",
      "[2022-09-26 17:36:13,199] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:36:13,304] adding document #4310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:36:44,376] discarding 33284 tokens: [('sonsavan', 2), ('vongxay', 2), ('wiphada', 2), ('xayavong', 2), ('ladsamee', 2), ('phaviset', 2), ('βυζαντινών', 2), ('riazat', 2), ('volkspflege', 2), ('illukka', 2)]...\n",
      "[2022-09-26 17:36:44,378] keeping 2000000 tokens which were in no less than 0 and no more than 4320000 (=100.0%) documents\n",
      "[2022-09-26 17:36:48,677] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:36:48,750] adding document #4320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:37:21,886] discarding 33709 tokens: [('inthaxara', 2), ('leptopsaltria', 2), ('leptosemia', 2), ('masamia', 2), ('minipomponia', 2), ('miniterpnosia', 2), ('neoterpnosia', 2), ('paranosia', 2), ('paratanna', 2), ('puranoides', 2)]...\n",
      "[2022-09-26 17:37:21,888] keeping 2000000 tokens which were in no less than 0 and no more than 4330000 (=100.0%) documents\n",
      "[2022-09-26 17:37:26,068] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:37:26,141] adding document #4330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:37:59,745] discarding 32658 tokens: [('lambug', 2), ('mokhadarat', 2), ('lōjabōn', 2), ('wodejebato', 2), ('telegdis', 2), ('lomeda', 2), ('mirond', 2), ('waskwei', 2), ('wunehikun', 2), ('maconde', 2)]...\n",
      "[2022-09-26 17:37:59,747] keeping 2000000 tokens which were in no less than 0 and no more than 4340000 (=100.0%) documents\n",
      "[2022-09-26 17:38:05,685] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:38:05,790] adding document #4340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:38:38,341] discarding 33169 tokens: [('hiatsintov', 2), ('rightrenaissance', 2), ('ecofilter', 2), ('ilonis', 2), ('društvenu', 2), ('crussell', 2), ('tillside', 2), ('thangesvari', 2), ('reconocer', 2), ('zulj', 2)]...\n",
      "[2022-09-26 17:38:38,343] keeping 2000000 tokens which were in no less than 0 and no more than 4350000 (=100.0%) documents\n",
      "[2022-09-26 17:38:42,578] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:38:42,652] adding document #4350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:39:14,132] discarding 29044 tokens: [('atulkar', 2), ('bhendia', 2), ('gurubux', 2), ('hadma', 2), ('hiraram', 2), ('lochanlal', 2), ('muthambigai', 2), ('tridenskaya', 2), ('bhawanilal', 2), ('deokaran', 2)]...\n",
      "[2022-09-26 17:39:14,136] keeping 2000000 tokens which were in no less than 0 and no more than 4360000 (=100.0%) documents\n",
      "[2022-09-26 17:39:19,798] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:39:19,902] adding document #4360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:39:52,340] discarding 34965 tokens: [('bomayé', 2), ('jahyanai', 2), ('jaymax', 2), ('kayz', 2), ('matéta', 2), ('équilibré', 2), ('iraee', 2), ('abudi', 2), ('blumenauensis', 2), ('desenderi', 2)]...\n",
      "[2022-09-26 17:39:52,342] keeping 2000000 tokens which were in no less than 0 and no more than 4370000 (=100.0%) documents\n",
      "[2022-09-26 17:39:56,592] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:39:56,667] adding document #4370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:40:32,056] discarding 33932 tokens: [('gourlayi', 2), ('atrisquama', 2), ('denotatus', 2), ('propodealis', 2), ('ramicornis', 2), ('นกห', 2), ('acolhuus', 2), ('nahuus', 2), ('rhysodesmus', 2), ('zendalus', 2)]...\n",
      "[2022-09-26 17:40:32,058] keeping 2000000 tokens which were in no less than 0 and no more than 4380000 (=100.0%) documents\n",
      "[2022-09-26 17:40:36,323] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:40:36,397] adding document #4380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:41:13,540] discarding 33983 tokens: [('heteromyia', 2), ('levanidovae', 2), ('naraiensis', 2), ('zapekina', 2), ('gallowich', 2), ('dogueti', 2), ('letzneri', 2), ('henschii', 2), ('vesiculata', 2), ('dissipatus', 2)]...\n",
      "[2022-09-26 17:41:13,542] keeping 2000000 tokens which were in no less than 0 and no more than 4390000 (=100.0%) documents\n",
      "[2022-09-26 17:41:19,168] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:41:19,274] adding document #4390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:41:52,260] discarding 32302 tokens: [('inflatipes', 2), ('labrosa', 2), ('longicostalis', 2), ('luteiclava', 2), ('malayae', 2), ('nocturnalis', 2), ('orientata', 2), ('pabloi', 2), ('pachydactyla', 2), ('pallidifemur', 2)]...\n",
      "[2022-09-26 17:41:52,262] keeping 2000000 tokens which were in no less than 0 and no more than 4400000 (=100.0%) documents\n",
      "[2022-09-26 17:41:58,345] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:41:58,452] adding document #4400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:42:32,540] discarding 87100 tokens: [('corcyrensis', 2), ('hulstaerti', 2), ('madronensis', 2), ('tornatus', 2), ('arizonus', 2), ('glabromaculatus', 2), ('nigroantennatus', 2), ('abidula', 2), ('abrary', 2), ('adzman', 2)]...\n",
      "[2022-09-26 17:42:32,542] keeping 2000000 tokens which were in no less than 0 and no more than 4410000 (=100.0%) documents\n",
      "[2022-09-26 17:42:36,895] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:42:36,983] adding document #4410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:43:15,098] discarding 39133 tokens: [('马艺瑄', 2), ('张思丽', 2), ('bossaura', 2), ('nwidobie', 2), ('blachs', 2), ('ciceron', 2), ('αργυρόπουλος', 2), ('μαυρουδής', 2), ('μεταξάς', 2), ('παπαρρηγόπουλος', 2)]...\n",
      "[2022-09-26 17:43:15,100] keeping 2000000 tokens which were in no less than 0 and no more than 4420000 (=100.0%) documents\n",
      "[2022-09-26 17:43:19,290] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:43:19,370] adding document #4420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:43:55,140] discarding 34939 tokens: [('puszka', 2), ('pařeniště', 2), ('ditylus', 2), ('manvanthara', 2), ('priddeth', 2), ('vučkovič', 2), ('jingoushan', 2), ('yanjiawan', 2), ('jonjic', 2), ('zhaojiachong', 2)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:43:55,142] keeping 2000000 tokens which were in no less than 0 and no more than 4430000 (=100.0%) documents\n",
      "[2022-09-26 17:43:59,492] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:43:59,572] adding document #4430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:44:36,353] discarding 31554 tokens: [('pokotciminikew', 2), ('pandab', 2), ('slatkog', 2), ('gallers', 2), ('itreni', 2), ('trasformate', 2), ('alguse', 2), ('puigserver', 2), ('吶喊', 2), ('jazzarine', 2)]...\n",
      "[2022-09-26 17:44:36,355] keeping 2000000 tokens which were in no less than 0 and no more than 4440000 (=100.0%) documents\n",
      "[2022-09-26 17:44:40,589] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:44:40,667] adding document #4440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:45:23,382] discarding 34152 tokens: [('мореходная', 2), ('сивуч', 2), ('lutogniew', 2), ('lagti', 2), ('穷追不舍', 2), ('boldecker', 2), ('ichgala', 2), ('naringala', 2), ('assalat', 2), ('bourdarie', 2)]...\n",
      "[2022-09-26 17:45:23,384] keeping 2000000 tokens which were in no less than 0 and no more than 4450000 (=100.0%) documents\n",
      "[2022-09-26 17:45:28,959] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:45:29,039] adding document #4450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:46:02,581] discarding 30559 tokens: [('hemiacridinae', 2), ('hieroglyphini', 2), ('albuleț', 2), ('makropoulou', 2), ('pogorevici', 2), ('bellinvs', 2), ('tetramethylcyclopropylfentanyl', 2), ('benzoylfentanyl', 2), ('phenylfentanyl', 2), ('gölitz', 2)]...\n",
      "[2022-09-26 17:46:02,583] keeping 2000000 tokens which were in no less than 0 and no more than 4460000 (=100.0%) documents\n",
      "[2022-09-26 17:46:06,864] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:46:06,942] adding document #4460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:46:40,997] discarding 34281 tokens: [('leardini', 2), ('nizhnikova', 2), ('sanpiter', 2), ('altaghullen', 2), ('radipedia', 2), ('excrucians', 2), ('pseudoneuroptera', 2), ('insara', 2), ('henrylygus', 2), ('ultranubilus', 2)]...\n",
      "[2022-09-26 17:46:40,999] keeping 2000000 tokens which were in no less than 0 and no more than 4470000 (=100.0%) documents\n",
      "[2022-09-26 17:46:45,798] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:46:45,877] adding document #4470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:47:16,771] discarding 31136 tokens: [('apoderaeocoris', 2), ('cimicicapsus', 2), ('cimidaeorus', 2), ('deraeocapsus', 2), ('diplozona', 2), ('eurybrochis', 2), ('fingulus', 2), ('kalamemiris', 2), ('klopicoris', 2), ('platycapsus', 2)]...\n",
      "[2022-09-26 17:47:16,773] keeping 2000000 tokens which were in no less than 0 and no more than 4480000 (=100.0%) documents\n",
      "[2022-09-26 17:47:21,188] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:47:21,267] adding document #4480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:47:54,896] discarding 34149 tokens: [('markfant', 2), ('sekersöz', 2), ('泉洞', 2), ('dalebura', 2), ('价川', 2), ('개천', 2), ('korvasova', 2), ('zentgericht', 2), ('repiquet', 2), ('julaolinja', 2)]...\n",
      "[2022-09-26 17:47:54,898] keeping 2000000 tokens which were in no less than 0 and no more than 4490000 (=100.0%) documents\n",
      "[2022-09-26 17:47:59,199] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:47:59,279] adding document #4490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:48:33,533] discarding 33880 tokens: [('bunthomarra', 2), ('eromarra', 2), ('murgoin', 2), ('thiralla', 2), ('wonkamurra', 2), ('toimo', 2), ('妥羅', 2), ('建州女真', 2), ('錫寶齊篇古', 2), ('maslaňák', 2)]...\n",
      "[2022-09-26 17:48:33,535] keeping 2000000 tokens which were in no less than 0 and no more than 4500000 (=100.0%) documents\n",
      "[2022-09-26 17:48:39,241] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:48:39,354] adding document #4500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:49:12,314] discarding 36834 tokens: [('flygfältsarbetskompani', 2), ('fältet', 2), ('gunnarn', 2), ('hasslösa', 2), ('kommandocentral', 2), ('krigsflygbas', 2), ('krigsflygbaser', 2), ('skyddskompani', 2), ('stabskompani', 2), ('stationskompani', 2)]...\n",
      "[2022-09-26 17:49:12,316] keeping 2000000 tokens which were in no less than 0 and no more than 4510000 (=100.0%) documents\n",
      "[2022-09-26 17:49:18,073] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:49:18,188] adding document #4510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:49:49,126] discarding 30744 tokens: [('ngulipartu', 2), ('ghazawi', 2), ('jarira', 2), ('งไงก', 2), ('tjiwali', 2), ('wanaeka', 2), ('kukuruba', 2), ('ngururrpa', 2), ('peedona', 2), ('molião', 2)]...\n",
      "[2022-09-26 17:49:49,129] keeping 2000000 tokens which were in no less than 0 and no more than 4520000 (=100.0%) documents\n",
      "[2022-09-26 17:49:54,837] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:49:54,950] adding document #4520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:50:25,431] discarding 31190 tokens: [('keshwari', 2), ('zōwa', 2), ('況斎雑話', 2), ('難波江', 2), ('baszko', 2), ('gryglas', 2), ('hakimova', 2), ('神宮文庫', 2), ('oftob', 2), ('chharwa', 2)]...\n",
      "[2022-09-26 17:50:25,433] keeping 2000000 tokens which were in no less than 0 and no more than 4530000 (=100.0%) documents\n",
      "[2022-09-26 17:50:29,820] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:50:29,900] adding document #4530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:51:00,969] discarding 33336 tokens: [('famitsin', 2), ('hominsky', 2), ('mekenzi', 2), ('petropavlovkaya', 2), ('svedomskiy', 2), ('vilimovich', 2), ('грейг', 2), ('bhaiwala', 2), ('azkorbebeitia', 2), ('eccho', 2)]...\n",
      "[2022-09-26 17:51:00,972] keeping 2000000 tokens which were in no less than 0 and no more than 4540000 (=100.0%) documents\n",
      "[2022-09-26 17:51:06,776] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:51:06,889] adding document #4540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:51:38,387] discarding 37441 tokens: [('gharamyat', 2), ('lemaza', 2), ('mafish', 2), ('tafahom', 2), ('telmiza', 2), ('maghoula', 2), ('maww', 2), ('kommunalgemeinde', 2), ('ostermarsch', 2), ('westermarsch', 2)]...\n",
      "[2022-09-26 17:51:38,389] keeping 2000000 tokens which were in no less than 0 and no more than 4550000 (=100.0%) documents\n",
      "[2022-09-26 17:51:42,632] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:51:42,712] adding document #4550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 17:52:15,904] discarding 36839 tokens: [('climacoidea', 2), ('concordatus', 2), ('dichroplax', 2), ('gausapatum', 2), ('gemmulatum', 2), ('hanetia', 2), ('hyalinonetrion', 2), ('iselica', 2), ('loveorum', 2), ('mcnowni', 2)]...\n",
      "[2022-09-26 17:52:15,906] keeping 2000000 tokens which were in no less than 0 and no more than 4560000 (=100.0%) documents\n",
      "[2022-09-26 17:52:21,976] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:52:22,092] adding document #4560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:52:56,663] discarding 38489 tokens: [('angelinensis', 2), ('anterproductus', 2), ('apocynophyllum', 2), ('calaminthus', 2), ('catocopis', 2), ('centropomus', 2), ('cochlodesma', 2), ('coffeyi', 2), ('cudahyensis', 2), ('curtivallum', 2)]...\n",
      "[2022-09-26 17:52:56,665] keeping 2000000 tokens which were in no less than 0 and no more than 4570000 (=100.0%) documents\n",
      "[2022-09-26 17:53:02,863] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:53:02,978] adding document #4570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:53:34,345] discarding 32740 tokens: [('izetti', 2), ('laticuneus', 2), ('lativertebralis', 2), ('leipsanolestes', 2), ('leptomylus', 2), ('lesquereuxiana', 2), ('limacioides', 2), ('lithopsis', 2), ('litoyoderimys', 2), ('loveina', 2)]...\n",
      "[2022-09-26 17:53:34,347] keeping 2000000 tokens which were in no less than 0 and no more than 4580000 (=100.0%) documents\n",
      "[2022-09-26 17:53:40,366] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:53:40,480] adding document #4580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:54:20,329] discarding 44102 tokens: [('crittendenia', 2), ('cypellospongia', 2), ('daphnogenoides', 2), ('elegantinia', 2), ('eotriassica', 2), ('fimbriartis', 2), ('geltena', 2), ('idahocolumbites', 2), ('iqualadelphis', 2), ('kashmirites', 2)]...\n",
      "[2022-09-26 17:54:20,331] keeping 2000000 tokens which were in no less than 0 and no more than 4590000 (=100.0%) documents\n",
      "[2022-09-26 17:54:24,686] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:54:24,767] adding document #4590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:54:58,746] discarding 37048 tokens: [('stromatotrypa', 2), ('subgibbosa', 2), ('synchirocrinus', 2), ('tumulosus', 2), ('wellerites', 2), ('whitspakia', 2), ('alleghanensis', 2), ('ankoura', 2), ('anoplea', 2), ('bellastriatum', 2)]...\n",
      "[2022-09-26 17:54:58,748] keeping 2000000 tokens which were in no less than 0 and no more than 4600000 (=100.0%) documents\n",
      "[2022-09-26 17:55:04,580] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:55:04,695] adding document #4600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:55:40,254] discarding 36059 tokens: [('acheilops', 2), ('actinodesma', 2), ('aechminaria', 2), ('alaionema', 2), ('aneurophyton', 2), ('apiculiretusispora', 2), ('arcarius', 2), ('arkonensis', 2), ('auroraspora', 2), ('bactrocrinites', 2)]...\n",
      "[2022-09-26 17:55:40,256] keeping 2000000 tokens which were in no less than 0 and no more than 4610000 (=100.0%) documents\n",
      "[2022-09-26 17:55:45,969] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:55:46,083] adding document #4610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:56:21,603] discarding 36854 tokens: [('hierogramma', 2), ('hudsonospongia', 2), ('kendrickensis', 2), ('lanthanaster', 2), ('laphamoceras', 2), ('lindsleyi', 2), ('listrochiton', 2), ('maysvillensis', 2), ('mespilocrinus', 2), ('metichthyocrinus', 2)]...\n",
      "[2022-09-26 17:56:21,606] keeping 2000000 tokens which were in no less than 0 and no more than 4620000 (=100.0%) documents\n",
      "[2022-09-26 17:56:27,413] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:56:27,527] adding document #4620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:57:00,696] discarding 36805 tokens: [('medullare', 2), ('menoeidina', 2), ('mesocyridira', 2), ('mesoleptostrophia', 2), ('miamoceras', 2), ('millepuntata', 2), ('mixoneura', 2), ('modulatum', 2), ('monodechenella', 2), ('monotrypella', 2)]...\n",
      "[2022-09-26 17:57:00,697] keeping 2000000 tokens which were in no less than 0 and no more than 4630000 (=100.0%) documents\n",
      "[2022-09-26 17:57:06,456] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:57:06,569] adding document #4630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:57:38,719] discarding 39033 tokens: [('cardiothyris', 2), ('cerauromerus', 2), ('cervifurca', 2), ('chalonerii', 2), ('channahonensis', 2), ('cheliphlebia', 2), ('chicagoensis', 2), ('clathrocrinus', 2), ('colchesterensis', 2), ('columinisporites', 2)]...\n",
      "[2022-09-26 17:57:38,721] keeping 2000000 tokens which were in no less than 0 and no more than 4640000 (=100.0%) documents\n",
      "[2022-09-26 17:57:43,083] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:57:43,163] adding document #4640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:58:23,722] discarding 31960 tokens: [('thysanophyllum', 2), ('tostonia', 2), ('trabeculites', 2), ('trigonicus', 2), ('tubulara', 2), ('verneuilia', 2), ('viriatellina', 2), ('vostokovae', 2), ('wellsvillensis', 2), ('wilberrya', 2)]...\n",
      "[2022-09-26 17:58:23,725] keeping 2000000 tokens which were in no less than 0 and no more than 4650000 (=100.0%) documents\n",
      "[2022-09-26 17:58:28,378] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:58:28,456] adding document #4650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:59:00,126] discarding 33873 tokens: [('diakui', 2), ('ritharrngu', 2), ('edizioa', 2), ('uzei', 2), ('krutoholova', 2), ('hoondert', 2), ('kandikuppam', 2), ('kurumber', 2), ('adultolescence', 2), ('iturburu', 2)]...\n",
      "[2022-09-26 17:59:00,128] keeping 2000000 tokens which were in no less than 0 and no more than 4660000 (=100.0%) documents\n",
      "[2022-09-26 17:59:04,390] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:59:04,469] adding document #4660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:59:36,195] discarding 31476 tokens: [('ferrío', 2), ('bromophenolosus', 2), ('kartacze', 2), ('ziemniaczana', 2), ('ballenita', 2), ('minuche', 2), ('murgueito', 2), ('nimbriotis', 2), ('colebrooke_family', 2), ('londonaye', 2)]...\n",
      "[2022-09-26 17:59:36,203] keeping 2000000 tokens which were in no less than 0 and no more than 4670000 (=100.0%) documents\n",
      "[2022-09-26 17:59:41,853] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 17:59:41,965] adding document #4670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:00:19,565] discarding 33510 tokens: [('salzlecke', 2), ('schmiedsberger', 2), ('matakichi', 2), ('orenochi', 2), ('taninnochi', 2), ('xctc', 2), ('mayota', 2), ('boolas', 2), ('학동', 2), ('plymouth_pa_armory_', 2)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 18:00:19,567] keeping 2000000 tokens which were in no less than 0 and no more than 4680000 (=100.0%) documents\n",
      "[2022-09-26 18:00:25,280] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:00:25,394] adding document #4680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:00:56,560] discarding 34487 tokens: [('kirola', 2), ('staffeldtsgate', 2), ('rumaniamania', 2), ('špralja', 2), ('presbyt', 2), ('lefaurichon', 2), ('besttone', 2), ('chilreu', 2), ('osenins', 2), ('ozhiganova', 2)]...\n",
      "[2022-09-26 18:00:56,563] keeping 2000000 tokens which were in no less than 0 and no more than 4690000 (=100.0%) documents\n",
      "[2022-09-26 18:01:00,891] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:01:00,971] adding document #4690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:01:30,215] discarding 35662 tokens: [('verntallat', 2), ('bilsa', 2), ('tashobya', 2), ('kashaija', 2), ('mimikatz', 2), ('butochiyn', 2), ('epizooty', 2), ('netzen', 2), ('novelletta', 2), ('asheervadam', 2)]...\n",
      "[2022-09-26 18:01:30,217] keeping 2000000 tokens which were in no less than 0 and no more than 4700000 (=100.0%) documents\n",
      "[2022-09-26 18:01:35,880] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:01:35,994] adding document #4700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:02:07,186] discarding 37400 tokens: [('steamchicken', 2), ('tzfoni', 2), ('baizetown', 2), ('montegano', 2), ('dignelis', 2), ('shanira', 2), ('taymí', 2), ('latalladi', 2), ('sarybel', 2), ('anicequol', 2)]...\n",
      "[2022-09-26 18:02:07,188] keeping 2000000 tokens which were in no less than 0 and no more than 4710000 (=100.0%) documents\n",
      "[2022-09-26 18:02:13,149] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:02:13,264] adding document #4710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:02:43,030] discarding 34212 tokens: [('agbekorun', 2), ('ajígun', 2), ('alárá', 2), ('ará', 2), ('ayajo', 2), ('modulua', 2), ('ojopagogo', 2), ('olutipin', 2), ('ìlárá', 2), ('gramaphon', 2)]...\n",
      "[2022-09-26 18:02:43,032] keeping 2000000 tokens which were in no less than 0 and no more than 4720000 (=100.0%) documents\n",
      "[2022-09-26 18:02:48,794] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:02:48,910] adding document #4720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:03:17,558] discarding 32767 tokens: [('brodding', 1), ('burncastle', 1), ('dolmoune', 1), ('excecution', 1), ('encurtidos', 1), ('unbottled', 1), ('hoglen', 1), ('honlulu', 1), ('pamanian', 1), ('reichhoff', 1)]...\n",
      "[2022-09-26 18:03:17,560] keeping 2000000 tokens which were in no less than 0 and no more than 4730000 (=100.0%) documents\n",
      "[2022-09-26 18:03:23,609] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:03:23,725] adding document #4730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...)\n",
      "[2022-09-26 18:03:26,890] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-26 18:03:27,372] built Dictionary(2001512 unique tokens: ['ability', 'ability#1§', 'able#0', 'abolish', 'abolition§']...) from 4730463 documents (total 1993339819 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# loc = 'num'|'lr'|'ent'\n",
    "# pos = True|False\n",
    "# download latest wiki dump\n",
    "#w.download_wiki_dump('en', WIKIXML)\n",
    "\n",
    "# parse wiki dump\n",
    "wiki_sentences = w.WikiSentences(WIKIXML, 'en',lower=True) # Orignal\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='EM',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='DEP',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNS',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNSEM',lower=True,pos=False,loc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T17:03:29.125388Z",
     "start_time": "2022-09-26T17:03:27.377671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the data\n"
     ]
    }
   ],
   "source": [
    "#sv.save(wiki_sentences,\"wiki_sentences_pos_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_pos\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_loc\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences\") # orignal\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep2\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_uns\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_unsem\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_em\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_wnet\")\n",
    "sv.save(wiki_sentences,\"wiki_sentences_wnet_noun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phrase mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:50:46.815932Z",
     "start_time": "2021-05-29T18:50:46.810808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T22:24:31.858045Z",
     "start_time": "2021-05-29T18:51:21.445049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences, min_count=100, threshold=1)\n",
    "frozen_phrases = phrases.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:41:54.697897Z",
     "start_time": "2021-05-30T11:41:38.608475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(phrases,\"gensim_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:06:13.397797Z",
     "start_time": "2020-03-16T22:06:13.394080Z"
    }
   },
   "source": [
    "# Train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T17:03:31.086301Z",
     "start_time": "2022-09-26T17:03:29.127200Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentences = sv.load(\"wiki_sentences_no\")\n",
    "#temp_sens are cased!!\n",
    "#sentences = sv.load(\"temp_sens\")\n",
    " \n",
    "#sentences = sv.load(\"wiki_sentences\") #Normal sentences using wiki_old.py\n",
    "\n",
    "#Wiki_Sentences_SP are cased\n",
    "#sentences = sv.load(\"Wiki_Sentences_SP\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_loc\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_sp\") #New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_pos\") # not to be used\n",
    "#sentences = sv.load(\"Wiki_sentences_pos_sample\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent\") # New\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent_sample\") # New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_dep\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_dep2\") #New\n",
    "\n",
    "#wiki english sample Cased \n",
    "#sentences = sv.load(\"Wiki_sentences_sp_sample\")\n",
    "#sentences = sv.load(\"wiki_sentences_uns\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_unsem\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_em\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_wnet\") #New\n",
    "sentences = sv.load(\"wiki_sentences_wnet_noun\") #New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T17:03:42.932622Z",
     "start_time": "2022-09-26T17:03:31.089314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism#0', 'political#1§', 'philosophy', 'movement', 'sceptical', 'authority#4§', 'reject', 'involuntary', 'coercive', 'form#11§', 'hierarchy', 'anarchism#0', 'call#24', 'abolition§', 'state§', 'hold#15', 'unnecessary', 'undesirable', 'harmful', 'historically', 'left#7', 'wing', 'movement#3§', 'placed', 'farthest', 'left', 'political§', 'spectrum', 'usually', 'described', 'alongside', 'libertarian#0§', 'marxism', 'libertarian#0§', 'wing', 'libertarian#0§', 'socialism#0§', 'socialist#0', 'movement', 'ha', 'strong', 'historical', 'association#0§', 'anti', 'capitalism', 'socialism#0§', 'human', 'lived', 'society#1§', 'without', 'formal', 'hierarchy#1', 'long#5', 'establishment#2', 'formal#3', 'state#3§', 'realm', 'empire#1', 'rise#25§', 'organised', 'hierarchical', 'body§', 'scepticism', 'toward', 'authority#4§', 'also', 'rose#18', 'th', 'century', 'self', 'conscious', 'political#1§', 'movement#3', 'emerged', 'latter', 'half#1', 'th', 'first#11§', 'decade', 'th', 'century', 'anarchist', 'movement', 'flourished', 'part§', 'world#5', 'significant', 'role#1', 'worker', 'struggle', 'emancipation', 'various', 'anarchist', 'school#4§', 'thought#2', 'formed', 'period#0§', 'anarchist', 'taken', 'part']\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    print(sent[:100])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T17:03:42.938601Z",
     "start_time": "2022-09-26T17:03:42.934712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length of token: 1\n"
     ]
    }
   ],
   "source": [
    "#sentences = wiki_sentences\n",
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:19:47.558649Z",
     "start_time": "2022-09-26T17:03:42.939992Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 18:03:42,951] Training model wordnet\n",
      "[2022-09-26 18:03:42,958] collecting all words and their counts\n",
      "[2022-09-26 18:03:50,141] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "[2022-09-26 18:04:46,699] PROGRESS: at sentence #10000, processed 22225402 words, keeping 554977 word types\n",
      "[2022-09-26 18:05:33,875] PROGRESS: at sentence #20000, processed 41943306 words, keeping 772018 word types\n",
      "[2022-09-26 18:06:14,601] PROGRESS: at sentence #30000, processed 58290377 words, keeping 933211 word types\n",
      "[2022-09-26 18:06:52,715] PROGRESS: at sentence #40000, processed 72364821 words, keeping 1068453 word types\n",
      "[2022-09-26 18:07:21,324] PROGRESS: at sentence #50000, processed 82317533 words, keeping 1150408 word types\n",
      "[2022-09-26 18:07:37,730] PROGRESS: at sentence #60000, processed 87762646 words, keeping 1171155 word types\n",
      "[2022-09-26 18:07:53,807] PROGRESS: at sentence #70000, processed 92716088 words, keeping 1189915 word types\n",
      "[2022-09-26 18:08:07,940] PROGRESS: at sentence #80000, processed 97063053 words, keeping 1205627 word types\n",
      "[2022-09-26 18:08:41,933] PROGRESS: at sentence #90000, processed 109857833 words, keeping 1305340 word types\n",
      "[2022-09-26 18:09:18,156] PROGRESS: at sentence #100000, processed 123311531 words, keeping 1419477 word types\n",
      "[2022-09-26 18:09:52,017] PROGRESS: at sentence #110000, processed 135129496 words, keeping 1512304 word types\n",
      "[2022-09-26 18:10:22,678] PROGRESS: at sentence #120000, processed 146492745 words, keeping 1604555 word types\n",
      "[2022-09-26 18:10:53,317] PROGRESS: at sentence #130000, processed 157162167 words, keeping 1677746 word types\n",
      "[2022-09-26 18:11:26,218] PROGRESS: at sentence #140000, processed 168503353 words, keeping 1768322 word types\n",
      "[2022-09-26 18:11:56,051] PROGRESS: at sentence #150000, processed 178576587 words, keeping 1855202 word types\n",
      "[2022-09-26 18:12:29,791] PROGRESS: at sentence #160000, processed 189059349 words, keeping 1937990 word types\n",
      "[2022-09-26 18:12:59,751] PROGRESS: at sentence #170000, processed 198799150 words, keeping 2006792 word types\n",
      "[2022-09-26 18:13:26,981] PROGRESS: at sentence #180000, processed 207991006 words, keeping 2067960 word types\n",
      "[2022-09-26 18:13:53,746] PROGRESS: at sentence #190000, processed 216518004 words, keeping 2127165 word types\n",
      "[2022-09-26 18:14:21,520] PROGRESS: at sentence #200000, processed 225298324 words, keeping 2190236 word types\n",
      "[2022-09-26 18:14:49,277] PROGRESS: at sentence #210000, processed 233853089 words, keeping 2242911 word types\n",
      "[2022-09-26 18:15:15,951] PROGRESS: at sentence #220000, processed 242379346 words, keeping 2298612 word types\n",
      "[2022-09-26 18:15:40,907] PROGRESS: at sentence #230000, processed 250583985 words, keeping 2353629 word types\n",
      "[2022-09-26 18:16:07,945] PROGRESS: at sentence #240000, processed 258809291 words, keeping 2401517 word types\n",
      "[2022-09-26 18:16:35,937] PROGRESS: at sentence #250000, processed 267116715 words, keeping 2451810 word types\n",
      "[2022-09-26 18:17:03,201] PROGRESS: at sentence #260000, processed 274875375 words, keeping 2497074 word types\n",
      "[2022-09-26 18:17:40,682] PROGRESS: at sentence #270000, processed 282548869 words, keeping 2547095 word types\n",
      "[2022-09-26 18:18:05,414] PROGRESS: at sentence #280000, processed 289952551 words, keeping 2595215 word types\n",
      "[2022-09-26 18:18:30,497] PROGRESS: at sentence #290000, processed 297453149 words, keeping 2648507 word types\n",
      "[2022-09-26 18:18:55,016] PROGRESS: at sentence #300000, processed 304509322 words, keeping 2697251 word types\n",
      "[2022-09-26 18:19:21,488] PROGRESS: at sentence #310000, processed 311746079 words, keeping 2747772 word types\n",
      "[2022-09-26 18:19:45,848] PROGRESS: at sentence #320000, processed 318838780 words, keeping 2789828 word types\n",
      "[2022-09-26 18:20:10,171] PROGRESS: at sentence #330000, processed 325580253 words, keeping 2828629 word types\n",
      "[2022-09-26 18:20:34,656] PROGRESS: at sentence #340000, processed 332540140 words, keeping 2864207 word types\n",
      "[2022-09-26 18:20:57,672] PROGRESS: at sentence #350000, processed 339237281 words, keeping 2899318 word types\n",
      "[2022-09-26 18:21:21,477] PROGRESS: at sentence #360000, processed 345756712 words, keeping 2934926 word types\n",
      "[2022-09-26 18:21:45,158] PROGRESS: at sentence #370000, processed 352324329 words, keeping 2973815 word types\n",
      "[2022-09-26 18:22:12,965] PROGRESS: at sentence #380000, processed 358941214 words, keeping 3007601 word types\n",
      "[2022-09-26 18:22:35,329] PROGRESS: at sentence #390000, processed 365295812 words, keeping 3042820 word types\n",
      "[2022-09-26 18:22:59,434] PROGRESS: at sentence #400000, processed 371730311 words, keeping 3082342 word types\n",
      "[2022-09-26 18:23:22,812] PROGRESS: at sentence #410000, processed 378071195 words, keeping 3120844 word types\n",
      "[2022-09-26 18:23:45,346] PROGRESS: at sentence #420000, processed 384384853 words, keeping 3153658 word types\n",
      "[2022-09-26 18:24:10,553] PROGRESS: at sentence #430000, processed 390853567 words, keeping 3190194 word types\n",
      "[2022-09-26 18:24:34,426] PROGRESS: at sentence #440000, processed 397022042 words, keeping 3225472 word types\n",
      "[2022-09-26 18:24:58,485] PROGRESS: at sentence #450000, processed 403316708 words, keeping 3262213 word types\n",
      "[2022-09-26 18:25:19,982] PROGRESS: at sentence #460000, processed 409138434 words, keeping 3290680 word types\n",
      "[2022-09-26 18:25:41,063] PROGRESS: at sentence #470000, processed 415005516 words, keeping 3325917 word types\n",
      "[2022-09-26 18:26:03,226] PROGRESS: at sentence #480000, processed 421056967 words, keeping 3356804 word types\n",
      "[2022-09-26 18:26:26,273] PROGRESS: at sentence #490000, processed 427135130 words, keeping 3390320 word types\n",
      "[2022-09-26 18:26:51,438] PROGRESS: at sentence #500000, processed 433174784 words, keeping 3422800 word types\n",
      "[2022-09-26 18:27:14,615] PROGRESS: at sentence #510000, processed 439109641 words, keeping 3454926 word types\n",
      "[2022-09-26 18:27:37,191] PROGRESS: at sentence #520000, processed 444941134 words, keeping 3485590 word types\n",
      "[2022-09-26 18:27:59,997] PROGRESS: at sentence #530000, processed 450658717 words, keeping 3520214 word types\n",
      "[2022-09-26 18:28:22,997] PROGRESS: at sentence #540000, processed 456438290 words, keeping 3549926 word types\n",
      "[2022-09-26 18:28:45,132] PROGRESS: at sentence #550000, processed 462093942 words, keeping 3579888 word types\n",
      "[2022-09-26 18:29:06,318] PROGRESS: at sentence #560000, processed 467664999 words, keeping 3607247 word types\n",
      "[2022-09-26 18:29:27,744] PROGRESS: at sentence #570000, processed 472993364 words, keeping 3632735 word types\n",
      "[2022-09-26 18:29:49,919] PROGRESS: at sentence #580000, processed 478670944 words, keeping 3663107 word types\n",
      "[2022-09-26 18:30:11,573] PROGRESS: at sentence #590000, processed 484030392 words, keeping 3692703 word types\n",
      "[2022-09-26 18:30:34,753] PROGRESS: at sentence #600000, processed 489467500 words, keeping 3721480 word types\n",
      "[2022-09-26 18:30:55,508] PROGRESS: at sentence #610000, processed 494586432 words, keeping 3749164 word types\n",
      "[2022-09-26 18:31:16,828] PROGRESS: at sentence #620000, processed 499869590 words, keeping 3777719 word types\n",
      "[2022-09-26 18:31:40,501] PROGRESS: at sentence #630000, processed 505012029 words, keeping 3805519 word types\n",
      "[2022-09-26 18:32:01,648] PROGRESS: at sentence #640000, processed 510132221 words, keeping 3831246 word types\n",
      "[2022-09-26 18:32:20,652] PROGRESS: at sentence #650000, processed 515061701 words, keeping 3867466 word types\n",
      "[2022-09-26 18:32:40,134] PROGRESS: at sentence #660000, processed 520021433 words, keeping 3893512 word types\n",
      "[2022-09-26 18:33:01,476] PROGRESS: at sentence #670000, processed 525012797 words, keeping 3917718 word types\n",
      "[2022-09-26 18:33:22,082] PROGRESS: at sentence #680000, processed 529979631 words, keeping 3941903 word types\n",
      "[2022-09-26 18:33:43,728] PROGRESS: at sentence #690000, processed 535082089 words, keeping 3984924 word types\n",
      "[2022-09-26 18:34:07,023] PROGRESS: at sentence #700000, processed 540118642 words, keeping 4010404 word types\n",
      "[2022-09-26 18:34:29,444] PROGRESS: at sentence #710000, processed 545134541 words, keeping 4032048 word types\n",
      "[2022-09-26 18:34:54,909] PROGRESS: at sentence #720000, processed 550340584 words, keeping 4057071 word types\n",
      "[2022-09-26 18:35:18,383] PROGRESS: at sentence #730000, processed 555342651 words, keeping 4082742 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 18:35:39,479] PROGRESS: at sentence #740000, processed 560241434 words, keeping 4105918 word types\n",
      "[2022-09-26 18:36:00,566] PROGRESS: at sentence #750000, processed 565232033 words, keeping 4132424 word types\n",
      "[2022-09-26 18:36:22,336] PROGRESS: at sentence #760000, processed 569992094 words, keeping 4155256 word types\n",
      "[2022-09-26 18:36:41,509] PROGRESS: at sentence #770000, processed 574696074 words, keeping 4179004 word types\n",
      "[2022-09-26 18:37:02,576] PROGRESS: at sentence #780000, processed 579515950 words, keeping 4206208 word types\n",
      "[2022-09-26 18:37:27,337] PROGRESS: at sentence #790000, processed 584518584 words, keeping 4230054 word types\n",
      "[2022-09-26 18:37:48,429] PROGRESS: at sentence #800000, processed 589205793 words, keeping 4252916 word types\n",
      "[2022-09-26 18:38:08,024] PROGRESS: at sentence #810000, processed 593965467 words, keeping 4277987 word types\n",
      "[2022-09-26 18:38:28,397] PROGRESS: at sentence #820000, processed 598640393 words, keeping 4301697 word types\n",
      "[2022-09-26 18:38:48,565] PROGRESS: at sentence #830000, processed 603420779 words, keeping 4325507 word types\n",
      "[2022-09-26 18:39:10,162] PROGRESS: at sentence #840000, processed 608173752 words, keeping 4349937 word types\n",
      "[2022-09-26 18:39:28,081] PROGRESS: at sentence #850000, processed 612733687 words, keeping 4372738 word types\n",
      "[2022-09-26 18:39:48,341] PROGRESS: at sentence #860000, processed 617356251 words, keeping 4393486 word types\n",
      "[2022-09-26 18:40:11,611] PROGRESS: at sentence #870000, processed 622102176 words, keeping 4419308 word types\n",
      "[2022-09-26 18:40:33,344] PROGRESS: at sentence #880000, processed 626811145 words, keeping 4440107 word types\n",
      "[2022-09-26 18:40:54,912] PROGRESS: at sentence #890000, processed 631498441 words, keeping 4462227 word types\n",
      "[2022-09-26 18:41:15,571] PROGRESS: at sentence #900000, processed 636040676 words, keeping 4486327 word types\n",
      "[2022-09-26 18:41:35,565] PROGRESS: at sentence #910000, processed 640513571 words, keeping 4506592 word types\n",
      "[2022-09-26 18:41:58,032] PROGRESS: at sentence #920000, processed 645230382 words, keeping 4526931 word types\n",
      "[2022-09-26 18:42:20,187] PROGRESS: at sentence #930000, processed 649938292 words, keeping 4551875 word types\n",
      "[2022-09-26 18:42:39,483] PROGRESS: at sentence #940000, processed 654218846 words, keeping 4574888 word types\n",
      "[2022-09-26 18:43:01,559] PROGRESS: at sentence #950000, processed 658681612 words, keeping 4608291 word types\n",
      "[2022-09-26 18:43:21,918] PROGRESS: at sentence #960000, processed 663092887 words, keeping 4633240 word types\n",
      "[2022-09-26 18:43:42,428] PROGRESS: at sentence #970000, processed 667586598 words, keeping 4659660 word types\n",
      "[2022-09-26 18:44:01,527] PROGRESS: at sentence #980000, processed 671895231 words, keeping 4681783 word types\n",
      "[2022-09-26 18:44:21,652] PROGRESS: at sentence #990000, processed 676196050 words, keeping 4707125 word types\n",
      "[2022-09-26 18:44:39,636] PROGRESS: at sentence #1000000, processed 680100705 words, keeping 4736163 word types\n",
      "[2022-09-26 18:45:01,420] PROGRESS: at sentence #1010000, processed 684572046 words, keeping 4762796 word types\n",
      "[2022-09-26 18:45:22,708] PROGRESS: at sentence #1020000, processed 688858346 words, keeping 4785524 word types\n",
      "[2022-09-26 18:45:44,478] PROGRESS: at sentence #1030000, processed 693013146 words, keeping 4804940 word types\n",
      "[2022-09-26 18:46:04,248] PROGRESS: at sentence #1040000, processed 697167972 words, keeping 4824916 word types\n",
      "[2022-09-26 18:46:23,505] PROGRESS: at sentence #1050000, processed 701234684 words, keeping 4843093 word types\n",
      "[2022-09-26 18:46:43,642] PROGRESS: at sentence #1060000, processed 705581106 words, keeping 4863087 word types\n",
      "[2022-09-26 18:47:02,364] PROGRESS: at sentence #1070000, processed 709406545 words, keeping 4881982 word types\n",
      "[2022-09-26 18:47:21,399] PROGRESS: at sentence #1080000, processed 713434299 words, keeping 4902603 word types\n",
      "[2022-09-26 18:47:40,966] PROGRESS: at sentence #1090000, processed 717454859 words, keeping 4923223 word types\n",
      "[2022-09-26 18:47:59,229] PROGRESS: at sentence #1100000, processed 721378347 words, keeping 4942722 word types\n",
      "[2022-09-26 18:48:19,675] PROGRESS: at sentence #1110000, processed 725691109 words, keeping 4961707 word types\n",
      "[2022-09-26 18:48:39,533] PROGRESS: at sentence #1120000, processed 729822226 words, keeping 4979775 word types\n",
      "[2022-09-26 18:49:00,502] PROGRESS: at sentence #1130000, processed 733868900 words, keeping 4999261 word types\n",
      "[2022-09-26 18:49:21,314] PROGRESS: at sentence #1140000, processed 737956650 words, keeping 5019575 word types\n",
      "[2022-09-26 18:49:40,918] PROGRESS: at sentence #1150000, processed 741915228 words, keeping 5038472 word types\n",
      "[2022-09-26 18:50:00,067] PROGRESS: at sentence #1160000, processed 745994482 words, keeping 5061129 word types\n",
      "[2022-09-26 18:50:19,903] PROGRESS: at sentence #1170000, processed 750027273 words, keeping 5079140 word types\n",
      "[2022-09-26 18:50:39,175] PROGRESS: at sentence #1180000, processed 754009847 words, keeping 5098720 word types\n",
      "[2022-09-26 18:50:58,304] PROGRESS: at sentence #1190000, processed 757836954 words, keeping 5122692 word types\n",
      "[2022-09-26 18:51:20,635] PROGRESS: at sentence #1200000, processed 762109132 words, keeping 5140280 word types\n",
      "[2022-09-26 18:51:40,666] PROGRESS: at sentence #1210000, processed 766078015 words, keeping 5159418 word types\n",
      "[2022-09-26 18:52:03,240] PROGRESS: at sentence #1220000, processed 770137758 words, keeping 5177551 word types\n",
      "[2022-09-26 18:52:24,090] PROGRESS: at sentence #1230000, processed 774158691 words, keeping 5194377 word types\n",
      "[2022-09-26 18:52:45,218] PROGRESS: at sentence #1240000, processed 778196573 words, keeping 5212021 word types\n",
      "[2022-09-26 18:53:05,310] PROGRESS: at sentence #1250000, processed 782219922 words, keeping 5230290 word types\n",
      "[2022-09-26 18:53:27,007] PROGRESS: at sentence #1260000, processed 786475447 words, keeping 5249370 word types\n",
      "[2022-09-26 18:53:48,675] PROGRESS: at sentence #1270000, processed 790602432 words, keeping 5270774 word types\n",
      "[2022-09-26 18:54:09,796] PROGRESS: at sentence #1280000, processed 794706125 words, keeping 5289184 word types\n",
      "[2022-09-26 18:54:29,529] PROGRESS: at sentence #1290000, processed 798676315 words, keeping 5313414 word types\n",
      "[2022-09-26 18:54:48,629] PROGRESS: at sentence #1300000, processed 802490517 words, keeping 5329730 word types\n",
      "[2022-09-26 18:55:08,415] PROGRESS: at sentence #1310000, processed 806413569 words, keeping 5347621 word types\n",
      "[2022-09-26 18:55:28,259] PROGRESS: at sentence #1320000, processed 810428371 words, keeping 5367141 word types\n",
      "[2022-09-26 18:55:46,400] PROGRESS: at sentence #1330000, processed 814232832 words, keeping 5383888 word types\n",
      "[2022-09-26 18:56:07,646] PROGRESS: at sentence #1340000, processed 818250133 words, keeping 5404364 word types\n",
      "[2022-09-26 18:56:29,800] PROGRESS: at sentence #1350000, processed 822284675 words, keeping 5423161 word types\n",
      "[2022-09-26 18:56:49,990] PROGRESS: at sentence #1360000, processed 826187223 words, keeping 5443345 word types\n",
      "[2022-09-26 18:57:10,452] PROGRESS: at sentence #1370000, processed 830074016 words, keeping 5466393 word types\n",
      "[2022-09-26 18:57:30,188] PROGRESS: at sentence #1380000, processed 833854613 words, keeping 5484256 word types\n",
      "[2022-09-26 18:57:49,579] PROGRESS: at sentence #1390000, processed 837830677 words, keeping 5502437 word types\n",
      "[2022-09-26 18:58:10,768] PROGRESS: at sentence #1400000, processed 841873808 words, keeping 5519745 word types\n",
      "[2022-09-26 18:58:32,893] PROGRESS: at sentence #1410000, processed 846519058 words, keeping 5538254 word types\n",
      "[2022-09-26 18:58:55,535] PROGRESS: at sentence #1420000, processed 850566719 words, keeping 5553945 word types\n",
      "[2022-09-26 18:59:16,869] PROGRESS: at sentence #1430000, processed 854492939 words, keeping 5571751 word types\n",
      "[2022-09-26 18:59:38,183] PROGRESS: at sentence #1440000, processed 858708932 words, keeping 5589170 word types\n",
      "[2022-09-26 18:59:59,050] PROGRESS: at sentence #1450000, processed 862367445 words, keeping 5605674 word types\n",
      "[2022-09-26 19:00:15,075] PROGRESS: at sentence #1460000, processed 865711104 words, keeping 5621584 word types\n",
      "[2022-09-26 19:00:37,095] PROGRESS: at sentence #1470000, processed 869634068 words, keeping 5642036 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 19:00:58,359] PROGRESS: at sentence #1480000, processed 873553867 words, keeping 5658283 word types\n",
      "[2022-09-26 19:01:17,648] PROGRESS: at sentence #1490000, processed 877320658 words, keeping 5674167 word types\n",
      "[2022-09-26 19:01:38,301] PROGRESS: at sentence #1500000, processed 880903221 words, keeping 5692822 word types\n",
      "[2022-09-26 19:01:59,227] PROGRESS: at sentence #1510000, processed 884553019 words, keeping 5709449 word types\n",
      "[2022-09-26 19:02:17,945] PROGRESS: at sentence #1520000, processed 887829588 words, keeping 5723912 word types\n",
      "[2022-09-26 19:02:37,000] PROGRESS: at sentence #1530000, processed 890984556 words, keeping 5736179 word types\n",
      "[2022-09-26 19:02:57,533] PROGRESS: at sentence #1540000, processed 894572785 words, keeping 5751515 word types\n",
      "[2022-09-26 19:03:18,942] PROGRESS: at sentence #1550000, processed 898200468 words, keeping 5770490 word types\n",
      "[2022-09-26 19:03:39,071] PROGRESS: at sentence #1560000, processed 901869151 words, keeping 5790358 word types\n",
      "[2022-09-26 19:04:00,418] PROGRESS: at sentence #1570000, processed 905690654 words, keeping 5810838 word types\n",
      "[2022-09-26 19:04:23,170] PROGRESS: at sentence #1580000, processed 909276563 words, keeping 5826429 word types\n",
      "[2022-09-26 19:04:45,675] PROGRESS: at sentence #1590000, processed 912942198 words, keeping 5845661 word types\n",
      "[2022-09-26 19:05:05,609] PROGRESS: at sentence #1600000, processed 916525601 words, keeping 5860274 word types\n",
      "[2022-09-26 19:05:27,576] PROGRESS: at sentence #1610000, processed 920283892 words, keeping 5877118 word types\n",
      "[2022-09-26 19:05:49,419] PROGRESS: at sentence #1620000, processed 923939235 words, keeping 5892885 word types\n",
      "[2022-09-26 19:06:09,050] PROGRESS: at sentence #1630000, processed 927459477 words, keeping 5915612 word types\n",
      "[2022-09-26 19:06:30,994] PROGRESS: at sentence #1640000, processed 931085341 words, keeping 5938783 word types\n",
      "[2022-09-26 19:06:50,228] PROGRESS: at sentence #1650000, processed 934808091 words, keeping 5952218 word types\n",
      "[2022-09-26 19:07:11,862] PROGRESS: at sentence #1660000, processed 938702091 words, keeping 5968802 word types\n",
      "[2022-09-26 19:07:36,202] PROGRESS: at sentence #1670000, processed 942588390 words, keeping 5985205 word types\n",
      "[2022-09-26 19:07:56,060] PROGRESS: at sentence #1680000, processed 946164182 words, keeping 6003487 word types\n",
      "[2022-09-26 19:08:15,199] PROGRESS: at sentence #1690000, processed 949595094 words, keeping 6017684 word types\n",
      "[2022-09-26 19:08:36,608] PROGRESS: at sentence #1700000, processed 952707250 words, keeping 6039811 word types\n",
      "[2022-09-26 19:09:01,096] PROGRESS: at sentence #1710000, processed 956116844 words, keeping 6066279 word types\n",
      "[2022-09-26 19:09:22,996] PROGRESS: at sentence #1720000, processed 959762408 words, keeping 6088449 word types\n",
      "[2022-09-26 19:09:43,831] PROGRESS: at sentence #1730000, processed 963304948 words, keeping 6104718 word types\n",
      "[2022-09-26 19:10:06,015] PROGRESS: at sentence #1740000, processed 966857153 words, keeping 6120660 word types\n",
      "[2022-09-26 19:10:28,402] PROGRESS: at sentence #1750000, processed 970461761 words, keeping 6136454 word types\n",
      "[2022-09-26 19:10:52,436] PROGRESS: at sentence #1760000, processed 974110953 words, keeping 6150996 word types\n",
      "[2022-09-26 19:11:15,167] PROGRESS: at sentence #1770000, processed 977760911 words, keeping 6166901 word types\n",
      "[2022-09-26 19:11:41,732] PROGRESS: at sentence #1780000, processed 981587715 words, keeping 6185586 word types\n",
      "[2022-09-26 19:12:10,223] PROGRESS: at sentence #1790000, processed 985259048 words, keeping 6199925 word types\n",
      "[2022-09-26 19:12:44,132] PROGRESS: at sentence #1800000, processed 988805810 words, keeping 6212576 word types\n",
      "[2022-09-26 19:13:05,654] PROGRESS: at sentence #1810000, processed 992442244 words, keeping 6227304 word types\n",
      "[2022-09-26 19:13:26,270] PROGRESS: at sentence #1820000, processed 995956001 words, keeping 6241561 word types\n",
      "[2022-09-26 19:13:46,991] PROGRESS: at sentence #1830000, processed 999663310 words, keeping 6258572 word types\n",
      "[2022-09-26 19:14:06,176] PROGRESS: at sentence #1840000, processed 1003289660 words, keeping 6272254 word types\n",
      "[2022-09-26 19:14:26,004] PROGRESS: at sentence #1850000, processed 1006940968 words, keeping 6285626 word types\n",
      "[2022-09-26 19:14:44,721] PROGRESS: at sentence #1860000, processed 1010355382 words, keeping 6299942 word types\n",
      "[2022-09-26 19:15:06,621] PROGRESS: at sentence #1870000, processed 1014038506 words, keeping 6314707 word types\n",
      "[2022-09-26 19:15:28,165] PROGRESS: at sentence #1880000, processed 1017568023 words, keeping 6329086 word types\n",
      "[2022-09-26 19:15:49,017] PROGRESS: at sentence #1890000, processed 1021054049 words, keeping 6344308 word types\n",
      "[2022-09-26 19:16:11,722] PROGRESS: at sentence #1900000, processed 1024825150 words, keeping 6359747 word types\n",
      "[2022-09-26 19:16:33,490] PROGRESS: at sentence #1910000, processed 1028324989 words, keeping 6374047 word types\n",
      "[2022-09-26 19:16:53,913] PROGRESS: at sentence #1920000, processed 1031844835 words, keeping 6389181 word types\n",
      "[2022-09-26 19:17:15,820] PROGRESS: at sentence #1930000, processed 1035463989 words, keeping 6419419 word types\n",
      "[2022-09-26 19:17:35,928] PROGRESS: at sentence #1940000, processed 1038763448 words, keeping 6432197 word types\n",
      "[2022-09-26 19:18:02,767] PROGRESS: at sentence #1950000, processed 1043008551 words, keeping 6449030 word types\n",
      "[2022-09-26 19:18:26,711] PROGRESS: at sentence #1960000, processed 1046660241 words, keeping 6464096 word types\n",
      "[2022-09-26 19:18:50,602] PROGRESS: at sentence #1970000, processed 1050654885 words, keeping 6479269 word types\n",
      "[2022-09-26 19:19:13,640] PROGRESS: at sentence #1980000, processed 1054446170 words, keeping 6495518 word types\n",
      "[2022-09-26 19:19:38,331] PROGRESS: at sentence #1990000, processed 1058076256 words, keeping 6513467 word types\n",
      "[2022-09-26 19:20:06,636] PROGRESS: at sentence #2000000, processed 1061774732 words, keeping 6530066 word types\n",
      "[2022-09-26 19:20:29,505] PROGRESS: at sentence #2010000, processed 1065280048 words, keeping 6545918 word types\n",
      "[2022-09-26 19:20:50,562] PROGRESS: at sentence #2020000, processed 1068744639 words, keeping 6559085 word types\n",
      "[2022-09-26 19:21:10,350] PROGRESS: at sentence #2030000, processed 1072347676 words, keeping 6575325 word types\n",
      "[2022-09-26 19:21:32,441] PROGRESS: at sentence #2040000, processed 1075907244 words, keeping 6593023 word types\n",
      "[2022-09-26 19:21:53,430] PROGRESS: at sentence #2050000, processed 1079531952 words, keeping 6608945 word types\n",
      "[2022-09-26 19:22:16,689] PROGRESS: at sentence #2060000, processed 1083218311 words, keeping 6627356 word types\n",
      "[2022-09-26 19:22:39,600] PROGRESS: at sentence #2070000, processed 1086965673 words, keeping 6642719 word types\n",
      "[2022-09-26 19:23:04,687] PROGRESS: at sentence #2080000, processed 1090765418 words, keeping 6658100 word types\n",
      "[2022-09-26 19:23:26,389] PROGRESS: at sentence #2090000, processed 1094360943 words, keeping 6674299 word types\n",
      "[2022-09-26 19:23:49,148] PROGRESS: at sentence #2100000, processed 1098119479 words, keeping 6693872 word types\n",
      "[2022-09-26 19:24:11,584] PROGRESS: at sentence #2110000, processed 1101802383 words, keeping 6709637 word types\n",
      "[2022-09-26 19:24:33,795] PROGRESS: at sentence #2120000, processed 1105233721 words, keeping 6727474 word types\n",
      "[2022-09-26 19:24:54,053] PROGRESS: at sentence #2130000, processed 1108409150 words, keeping 6739182 word types\n",
      "[2022-09-26 19:25:13,455] PROGRESS: at sentence #2140000, processed 1111774961 words, keeping 6755376 word types\n",
      "[2022-09-26 19:25:34,660] PROGRESS: at sentence #2150000, processed 1115134853 words, keeping 6785006 word types\n",
      "[2022-09-26 19:25:58,122] PROGRESS: at sentence #2160000, processed 1118779214 words, keeping 6801746 word types\n",
      "[2022-09-26 19:26:19,947] PROGRESS: at sentence #2170000, processed 1122165401 words, keeping 6818227 word types\n",
      "[2022-09-26 19:26:42,432] PROGRESS: at sentence #2180000, processed 1125750565 words, keeping 6835485 word types\n",
      "[2022-09-26 19:27:02,243] PROGRESS: at sentence #2190000, processed 1129345674 words, keeping 6848532 word types\n",
      "[2022-09-26 19:27:22,901] PROGRESS: at sentence #2200000, processed 1132818328 words, keeping 6862863 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 19:27:46,090] PROGRESS: at sentence #2210000, processed 1136525524 words, keeping 6876439 word types\n",
      "[2022-09-26 19:28:11,302] PROGRESS: at sentence #2220000, processed 1140118919 words, keeping 6893091 word types\n",
      "[2022-09-26 19:28:43,531] PROGRESS: at sentence #2230000, processed 1143988367 words, keeping 6909300 word types\n",
      "[2022-09-26 19:29:10,169] PROGRESS: at sentence #2240000, processed 1147606940 words, keeping 6924994 word types\n",
      "[2022-09-26 19:29:31,034] PROGRESS: at sentence #2250000, processed 1150948558 words, keeping 6948961 word types\n",
      "[2022-09-26 19:29:54,313] PROGRESS: at sentence #2260000, processed 1154576694 words, keeping 6965543 word types\n",
      "[2022-09-26 19:30:16,398] PROGRESS: at sentence #2270000, processed 1158112213 words, keeping 6983738 word types\n",
      "[2022-09-26 19:30:37,605] PROGRESS: at sentence #2280000, processed 1161618690 words, keeping 6999651 word types\n",
      "[2022-09-26 19:30:59,358] PROGRESS: at sentence #2290000, processed 1165220484 words, keeping 7014835 word types\n",
      "[2022-09-26 19:31:23,681] PROGRESS: at sentence #2300000, processed 1168976713 words, keeping 7030764 word types\n",
      "[2022-09-26 19:31:45,388] PROGRESS: at sentence #2310000, processed 1172621742 words, keeping 7046288 word types\n",
      "[2022-09-26 19:32:09,020] PROGRESS: at sentence #2320000, processed 1176076958 words, keeping 7066020 word types\n",
      "[2022-09-26 19:32:30,757] PROGRESS: at sentence #2330000, processed 1179751565 words, keeping 7081799 word types\n",
      "[2022-09-26 19:32:52,151] PROGRESS: at sentence #2340000, processed 1183199266 words, keeping 7097650 word types\n",
      "[2022-09-26 19:33:14,316] PROGRESS: at sentence #2350000, processed 1186820777 words, keeping 7113827 word types\n",
      "[2022-09-26 19:33:37,572] PROGRESS: at sentence #2360000, processed 1190818253 words, keeping 7126798 word types\n",
      "[2022-09-26 19:33:58,607] PROGRESS: at sentence #2370000, processed 1194298072 words, keeping 7140003 word types\n",
      "[2022-09-26 19:34:23,823] PROGRESS: at sentence #2380000, processed 1197889779 words, keeping 7154531 word types\n",
      "[2022-09-26 19:34:46,282] PROGRESS: at sentence #2390000, processed 1201638427 words, keeping 7170114 word types\n",
      "[2022-09-26 19:35:05,841] PROGRESS: at sentence #2400000, processed 1204971125 words, keeping 7186609 word types\n",
      "[2022-09-26 19:35:26,647] PROGRESS: at sentence #2410000, processed 1208507332 words, keeping 7202764 word types\n",
      "[2022-09-26 19:35:48,824] PROGRESS: at sentence #2420000, processed 1211981447 words, keeping 7217474 word types\n",
      "[2022-09-26 19:36:10,376] PROGRESS: at sentence #2430000, processed 1215654459 words, keeping 7234230 word types\n",
      "[2022-09-26 19:36:31,753] PROGRESS: at sentence #2440000, processed 1219220477 words, keeping 7249705 word types\n",
      "[2022-09-26 19:36:51,752] PROGRESS: at sentence #2450000, processed 1222546065 words, keeping 7262357 word types\n",
      "[2022-09-26 19:37:11,554] PROGRESS: at sentence #2460000, processed 1225801562 words, keeping 7275060 word types\n",
      "[2022-09-26 19:37:34,898] PROGRESS: at sentence #2470000, processed 1229066789 words, keeping 7288958 word types\n",
      "[2022-09-26 19:37:54,426] PROGRESS: at sentence #2480000, processed 1232462622 words, keeping 7302109 word types\n",
      "[2022-09-26 19:38:16,991] PROGRESS: at sentence #2490000, processed 1236155246 words, keeping 7359780 word types\n",
      "[2022-09-26 19:38:42,751] PROGRESS: at sentence #2500000, processed 1239830985 words, keeping 7378303 word types\n",
      "[2022-09-26 19:39:06,749] PROGRESS: at sentence #2510000, processed 1243410409 words, keeping 7397196 word types\n",
      "[2022-09-26 19:39:29,316] PROGRESS: at sentence #2520000, processed 1246789793 words, keeping 7413841 word types\n",
      "[2022-09-26 19:39:51,230] PROGRESS: at sentence #2530000, processed 1250315357 words, keeping 7430406 word types\n",
      "[2022-09-26 19:40:11,992] PROGRESS: at sentence #2540000, processed 1253718400 words, keeping 7443658 word types\n",
      "[2022-09-26 19:40:31,390] PROGRESS: at sentence #2550000, processed 1257095546 words, keeping 7457764 word types\n",
      "[2022-09-26 19:40:59,881] PROGRESS: at sentence #2560000, processed 1260453226 words, keeping 7474190 word types\n",
      "[2022-09-26 19:41:25,997] PROGRESS: at sentence #2570000, processed 1263956905 words, keeping 7492652 word types\n",
      "[2022-09-26 19:41:50,953] PROGRESS: at sentence #2580000, processed 1267594828 words, keeping 7522424 word types\n",
      "[2022-09-26 19:42:14,236] PROGRESS: at sentence #2590000, processed 1271100424 words, keeping 7537666 word types\n",
      "[2022-09-26 19:42:37,014] PROGRESS: at sentence #2600000, processed 1274522225 words, keeping 7552164 word types\n",
      "[2022-09-26 19:42:57,486] PROGRESS: at sentence #2610000, processed 1277703963 words, keeping 7564529 word types\n",
      "[2022-09-26 19:43:17,712] PROGRESS: at sentence #2620000, processed 1280714813 words, keeping 7576267 word types\n",
      "[2022-09-26 19:43:49,275] PROGRESS: at sentence #2630000, processed 1284048547 words, keeping 7589107 word types\n",
      "[2022-09-26 19:44:22,164] PROGRESS: at sentence #2640000, processed 1287710950 words, keeping 7605868 word types\n",
      "[2022-09-26 19:44:44,951] PROGRESS: at sentence #2650000, processed 1291209533 words, keeping 7622868 word types\n",
      "[2022-09-26 19:45:09,781] PROGRESS: at sentence #2660000, processed 1294832625 words, keeping 7639312 word types\n",
      "[2022-09-26 19:45:31,373] PROGRESS: at sentence #2670000, processed 1298396244 words, keeping 7654104 word types\n",
      "[2022-09-26 19:45:53,850] PROGRESS: at sentence #2680000, processed 1301903548 words, keeping 7669218 word types\n",
      "[2022-09-26 19:46:16,486] PROGRESS: at sentence #2690000, processed 1306854968 words, keeping 7688471 word types\n",
      "[2022-09-26 19:46:38,565] PROGRESS: at sentence #2700000, processed 1310462941 words, keeping 7704301 word types\n",
      "[2022-09-26 19:46:59,354] PROGRESS: at sentence #2710000, processed 1314022398 words, keeping 7720725 word types\n",
      "[2022-09-26 19:47:24,096] PROGRESS: at sentence #2720000, processed 1317677932 words, keeping 7738316 word types\n",
      "[2022-09-26 19:47:45,349] PROGRESS: at sentence #2730000, processed 1321398546 words, keeping 7753893 word types\n",
      "[2022-09-26 19:48:07,163] PROGRESS: at sentence #2740000, processed 1325089869 words, keeping 7768697 word types\n",
      "[2022-09-26 19:48:30,040] PROGRESS: at sentence #2750000, processed 1328784593 words, keeping 7784656 word types\n",
      "[2022-09-26 19:48:55,506] PROGRESS: at sentence #2760000, processed 1332454953 words, keeping 7799224 word types\n",
      "[2022-09-26 19:49:19,833] PROGRESS: at sentence #2770000, processed 1336063795 words, keeping 7815105 word types\n",
      "[2022-09-26 19:49:43,578] PROGRESS: at sentence #2780000, processed 1339556046 words, keeping 7831316 word types\n",
      "[2022-09-26 19:50:07,577] PROGRESS: at sentence #2790000, processed 1343057442 words, keeping 7847443 word types\n",
      "[2022-09-26 19:50:30,557] PROGRESS: at sentence #2800000, processed 1346444713 words, keeping 7867397 word types\n",
      "[2022-09-26 19:50:54,603] PROGRESS: at sentence #2810000, processed 1349893580 words, keeping 7882645 word types\n",
      "[2022-09-26 19:51:17,097] PROGRESS: at sentence #2820000, processed 1353088219 words, keeping 7896777 word types\n",
      "[2022-09-26 19:51:41,819] PROGRESS: at sentence #2830000, processed 1356606228 words, keeping 7912864 word types\n",
      "[2022-09-26 19:52:06,561] PROGRESS: at sentence #2840000, processed 1360103585 words, keeping 7929433 word types\n",
      "[2022-09-26 19:52:29,782] PROGRESS: at sentence #2850000, processed 1363599337 words, keeping 7945651 word types\n",
      "[2022-09-26 19:52:55,546] PROGRESS: at sentence #2860000, processed 1367196138 words, keeping 7961774 word types\n",
      "[2022-09-26 19:53:17,918] PROGRESS: at sentence #2870000, processed 1370665735 words, keeping 7976218 word types\n",
      "[2022-09-26 19:53:40,400] PROGRESS: at sentence #2880000, processed 1374208465 words, keeping 7991967 word types\n",
      "[2022-09-26 19:54:05,171] PROGRESS: at sentence #2890000, processed 1377860557 words, keeping 8005861 word types\n",
      "[2022-09-26 19:54:26,919] PROGRESS: at sentence #2900000, processed 1381403690 words, keeping 8020784 word types\n",
      "[2022-09-26 19:54:47,274] PROGRESS: at sentence #2910000, processed 1384817930 words, keeping 8034061 word types\n",
      "[2022-09-26 19:55:11,474] PROGRESS: at sentence #2920000, processed 1388432175 words, keeping 8048137 word types\n",
      "[2022-09-26 19:55:37,664] PROGRESS: at sentence #2930000, processed 1391897502 words, keeping 8062755 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 19:56:00,748] PROGRESS: at sentence #2940000, processed 1395535912 words, keeping 8077021 word types\n",
      "[2022-09-26 19:56:28,919] PROGRESS: at sentence #2950000, processed 1399068831 words, keeping 8094368 word types\n",
      "[2022-09-26 19:56:51,849] PROGRESS: at sentence #2960000, processed 1402587199 words, keeping 8109056 word types\n",
      "[2022-09-26 19:57:15,494] PROGRESS: at sentence #2970000, processed 1406168781 words, keeping 8125619 word types\n",
      "[2022-09-26 19:57:39,534] PROGRESS: at sentence #2980000, processed 1409767963 words, keeping 8140770 word types\n",
      "[2022-09-26 19:58:03,434] PROGRESS: at sentence #2990000, processed 1413254212 words, keeping 8152521 word types\n",
      "[2022-09-26 19:58:28,508] PROGRESS: at sentence #3000000, processed 1416750382 words, keeping 8167407 word types\n",
      "[2022-09-26 19:58:50,684] PROGRESS: at sentence #3010000, processed 1419970080 words, keeping 8180914 word types\n",
      "[2022-09-26 19:59:14,193] PROGRESS: at sentence #3020000, processed 1423371147 words, keeping 8196172 word types\n",
      "[2022-09-26 19:59:38,513] PROGRESS: at sentence #3030000, processed 1426678972 words, keeping 8208249 word types\n",
      "[2022-09-26 20:00:03,077] PROGRESS: at sentence #3040000, processed 1430181285 words, keeping 8227087 word types\n",
      "[2022-09-26 20:00:25,525] PROGRESS: at sentence #3050000, processed 1433480444 words, keeping 8242248 word types\n",
      "[2022-09-26 20:00:51,181] PROGRESS: at sentence #3060000, processed 1436952996 words, keeping 8256010 word types\n",
      "[2022-09-26 20:01:15,520] PROGRESS: at sentence #3070000, processed 1440276893 words, keeping 8270209 word types\n",
      "[2022-09-26 20:01:43,320] PROGRESS: at sentence #3080000, processed 1444006980 words, keeping 8284715 word types\n",
      "[2022-09-26 20:02:07,221] PROGRESS: at sentence #3090000, processed 1447611358 words, keeping 8299467 word types\n",
      "[2022-09-26 20:02:33,159] PROGRESS: at sentence #3100000, processed 1451211332 words, keeping 8313603 word types\n",
      "[2022-09-26 20:03:03,731] PROGRESS: at sentence #3110000, processed 1454982588 words, keeping 8328373 word types\n",
      "[2022-09-26 20:03:29,636] PROGRESS: at sentence #3120000, processed 1458755395 words, keeping 8343421 word types\n",
      "[2022-09-26 20:03:53,329] PROGRESS: at sentence #3130000, processed 1462224949 words, keeping 8356452 word types\n",
      "[2022-09-26 20:04:20,392] PROGRESS: at sentence #3140000, processed 1465897550 words, keeping 8371632 word types\n",
      "[2022-09-26 20:04:48,885] PROGRESS: at sentence #3150000, processed 1469563659 words, keeping 8384301 word types\n",
      "[2022-09-26 20:05:13,530] PROGRESS: at sentence #3160000, processed 1472973817 words, keeping 8397573 word types\n",
      "[2022-09-26 20:05:38,684] PROGRESS: at sentence #3170000, processed 1476560379 words, keeping 8410113 word types\n",
      "[2022-09-26 20:06:01,784] PROGRESS: at sentence #3180000, processed 1480180878 words, keeping 8424492 word types\n",
      "[2022-09-26 20:06:27,678] PROGRESS: at sentence #3190000, processed 1483859816 words, keeping 8437056 word types\n",
      "[2022-09-26 20:06:54,801] PROGRESS: at sentence #3200000, processed 1487635858 words, keeping 8449780 word types\n",
      "[2022-09-26 20:07:26,458] PROGRESS: at sentence #3210000, processed 1491566182 words, keeping 8464426 word types\n",
      "[2022-09-26 20:07:51,028] PROGRESS: at sentence #3220000, processed 1495257595 words, keeping 8477500 word types\n",
      "[2022-09-26 20:08:15,253] PROGRESS: at sentence #3230000, processed 1498791148 words, keeping 8492150 word types\n",
      "[2022-09-26 20:08:38,824] PROGRESS: at sentence #3240000, processed 1502200211 words, keeping 8506667 word types\n",
      "[2022-09-26 20:09:04,393] PROGRESS: at sentence #3250000, processed 1505682298 words, keeping 8524382 word types\n",
      "[2022-09-26 20:09:28,399] PROGRESS: at sentence #3260000, processed 1509103246 words, keeping 8539484 word types\n",
      "[2022-09-26 20:09:51,018] PROGRESS: at sentence #3270000, processed 1512660258 words, keeping 8552570 word types\n",
      "[2022-09-26 20:10:15,300] PROGRESS: at sentence #3280000, processed 1516072828 words, keeping 8566779 word types\n",
      "[2022-09-26 20:10:40,088] PROGRESS: at sentence #3290000, processed 1519516280 words, keeping 8578327 word types\n",
      "[2022-09-26 20:11:06,396] PROGRESS: at sentence #3300000, processed 1523110372 words, keeping 8593472 word types\n",
      "[2022-09-26 20:11:32,264] PROGRESS: at sentence #3310000, processed 1526802233 words, keeping 8609279 word types\n",
      "[2022-09-26 20:11:55,587] PROGRESS: at sentence #3320000, processed 1530194346 words, keeping 8621563 word types\n",
      "[2022-09-26 20:12:18,455] PROGRESS: at sentence #3330000, processed 1533533242 words, keeping 8633150 word types\n",
      "[2022-09-26 20:12:48,931] PROGRESS: at sentence #3340000, processed 1537082285 words, keeping 8648823 word types\n",
      "[2022-09-26 20:13:10,743] PROGRESS: at sentence #3350000, processed 1540411115 words, keeping 8661332 word types\n",
      "[2022-09-26 20:13:35,050] PROGRESS: at sentence #3360000, processed 1543958816 words, keeping 8673441 word types\n",
      "[2022-09-26 20:13:58,444] PROGRESS: at sentence #3370000, processed 1547478289 words, keeping 8686985 word types\n",
      "[2022-09-26 20:14:22,569] PROGRESS: at sentence #3380000, processed 1551050947 words, keeping 8701023 word types\n",
      "[2022-09-26 20:14:47,477] PROGRESS: at sentence #3390000, processed 1554561527 words, keeping 8715851 word types\n",
      "[2022-09-26 20:15:10,569] PROGRESS: at sentence #3400000, processed 1557989064 words, keeping 8732655 word types\n",
      "[2022-09-26 20:15:32,351] PROGRESS: at sentence #3410000, processed 1561197160 words, keeping 8747381 word types\n",
      "[2022-09-26 20:15:56,423] PROGRESS: at sentence #3420000, processed 1564431480 words, keeping 8759780 word types\n",
      "[2022-09-26 20:16:19,127] PROGRESS: at sentence #3430000, processed 1567733581 words, keeping 8771700 word types\n",
      "[2022-09-26 20:16:42,744] PROGRESS: at sentence #3440000, processed 1571116894 words, keeping 8784428 word types\n",
      "[2022-09-26 20:17:12,766] PROGRESS: at sentence #3450000, processed 1574586610 words, keeping 8799419 word types\n",
      "[2022-09-26 20:17:34,051] PROGRESS: at sentence #3460000, processed 1577755423 words, keeping 8812306 word types\n",
      "[2022-09-26 20:17:54,751] PROGRESS: at sentence #3470000, processed 1581024765 words, keeping 8826923 word types\n",
      "[2022-09-26 20:18:16,988] PROGRESS: at sentence #3480000, processed 1584369679 words, keeping 8840570 word types\n",
      "[2022-09-26 20:18:43,920] PROGRESS: at sentence #3490000, processed 1587900671 words, keeping 8864470 word types\n",
      "[2022-09-26 20:19:07,931] PROGRESS: at sentence #3500000, processed 1591407095 words, keeping 8876343 word types\n",
      "[2022-09-26 20:19:31,712] PROGRESS: at sentence #3510000, processed 1594718091 words, keeping 8888657 word types\n",
      "[2022-09-26 20:19:55,859] PROGRESS: at sentence #3520000, processed 1598174706 words, keeping 8902921 word types\n",
      "[2022-09-26 20:20:18,717] PROGRESS: at sentence #3530000, processed 1601552386 words, keeping 8914619 word types\n",
      "[2022-09-26 20:20:40,214] PROGRESS: at sentence #3540000, processed 1605004635 words, keeping 8926406 word types\n",
      "[2022-09-26 20:21:03,231] PROGRESS: at sentence #3550000, processed 1608423229 words, keeping 8938307 word types\n",
      "[2022-09-26 20:21:27,305] PROGRESS: at sentence #3560000, processed 1611893018 words, keeping 8952142 word types\n",
      "[2022-09-26 20:21:51,513] PROGRESS: at sentence #3570000, processed 1615371871 words, keeping 8964261 word types\n",
      "[2022-09-26 20:22:16,215] PROGRESS: at sentence #3580000, processed 1618822943 words, keeping 8975070 word types\n",
      "[2022-09-26 20:22:42,131] PROGRESS: at sentence #3590000, processed 1622337550 words, keeping 8986899 word types\n",
      "[2022-09-26 20:23:06,353] PROGRESS: at sentence #3600000, processed 1625673729 words, keeping 8998288 word types\n",
      "[2022-09-26 20:23:29,529] PROGRESS: at sentence #3610000, processed 1628826583 words, keeping 9012647 word types\n",
      "[2022-09-26 20:23:54,292] PROGRESS: at sentence #3620000, processed 1631945846 words, keeping 9023368 word types\n",
      "[2022-09-26 20:24:15,193] PROGRESS: at sentence #3630000, processed 1634939357 words, keeping 9034834 word types\n",
      "[2022-09-26 20:24:36,805] PROGRESS: at sentence #3640000, processed 1637962579 words, keeping 9047240 word types\n",
      "[2022-09-26 20:24:56,546] PROGRESS: at sentence #3650000, processed 1640891558 words, keeping 9058459 word types\n",
      "[2022-09-26 20:25:22,064] PROGRESS: at sentence #3660000, processed 1644481742 words, keeping 9073724 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 20:25:46,359] PROGRESS: at sentence #3670000, processed 1647834352 words, keeping 9085541 word types\n",
      "[2022-09-26 20:26:11,739] PROGRESS: at sentence #3680000, processed 1651205762 words, keeping 9096864 word types\n",
      "[2022-09-26 20:26:34,390] PROGRESS: at sentence #3690000, processed 1654536101 words, keeping 9108940 word types\n",
      "[2022-09-26 20:26:56,267] PROGRESS: at sentence #3700000, processed 1657794552 words, keeping 9120865 word types\n",
      "[2022-09-26 20:27:20,420] PROGRESS: at sentence #3710000, processed 1661189683 words, keeping 9133764 word types\n",
      "[2022-09-26 20:27:49,531] PROGRESS: at sentence #3720000, processed 1664767763 words, keeping 9146786 word types\n",
      "[2022-09-26 20:28:14,016] PROGRESS: at sentence #3730000, processed 1668231750 words, keeping 9160389 word types\n",
      "[2022-09-26 20:28:37,053] PROGRESS: at sentence #3740000, processed 1671590727 words, keeping 9173529 word types\n",
      "[2022-09-26 20:28:58,870] PROGRESS: at sentence #3750000, processed 1674869710 words, keeping 9189436 word types\n",
      "[2022-09-26 20:29:31,500] PROGRESS: at sentence #3760000, processed 1678112567 words, keeping 9201593 word types\n",
      "[2022-09-26 20:29:53,200] PROGRESS: at sentence #3770000, processed 1681288150 words, keeping 9213258 word types\n",
      "[2022-09-26 20:30:17,321] PROGRESS: at sentence #3780000, processed 1684452284 words, keeping 9226886 word types\n",
      "[2022-09-26 20:30:44,406] PROGRESS: at sentence #3790000, processed 1687415222 words, keeping 9237332 word types\n",
      "[2022-09-26 20:31:05,473] PROGRESS: at sentence #3800000, processed 1690548874 words, keeping 9249992 word types\n",
      "[2022-09-26 20:31:32,410] PROGRESS: at sentence #3810000, processed 1693942817 words, keeping 9264327 word types\n",
      "[2022-09-26 20:31:55,284] PROGRESS: at sentence #3820000, processed 1697237238 words, keeping 9278218 word types\n",
      "[2022-09-26 20:32:16,714] PROGRESS: at sentence #3830000, processed 1700390084 words, keeping 9291740 word types\n",
      "[2022-09-26 20:32:38,381] PROGRESS: at sentence #3840000, processed 1703393069 words, keeping 9302988 word types\n",
      "[2022-09-26 20:33:02,324] PROGRESS: at sentence #3850000, processed 1706623354 words, keeping 9320372 word types\n",
      "[2022-09-26 20:33:24,702] PROGRESS: at sentence #3860000, processed 1709783329 words, keeping 9332668 word types\n",
      "[2022-09-26 20:33:49,624] PROGRESS: at sentence #3870000, processed 1712925858 words, keeping 9345218 word types\n",
      "[2022-09-26 20:34:13,851] PROGRESS: at sentence #3880000, processed 1716189260 words, keeping 9357348 word types\n",
      "[2022-09-26 20:34:39,814] PROGRESS: at sentence #3890000, processed 1719765909 words, keeping 9370935 word types\n",
      "[2022-09-26 20:35:03,090] PROGRESS: at sentence #3900000, processed 1722981903 words, keeping 9383450 word types\n",
      "[2022-09-26 20:35:26,007] PROGRESS: at sentence #3910000, processed 1726064631 words, keeping 9396401 word types\n",
      "[2022-09-26 20:35:49,423] PROGRESS: at sentence #3920000, processed 1729281928 words, keeping 9409242 word types\n",
      "[2022-09-26 20:36:16,223] PROGRESS: at sentence #3930000, processed 1732762199 words, keeping 9422280 word types\n",
      "[2022-09-26 20:36:40,459] PROGRESS: at sentence #3940000, processed 1735978465 words, keeping 9436122 word types\n",
      "[2022-09-26 20:37:02,879] PROGRESS: at sentence #3950000, processed 1739079826 words, keeping 9447170 word types\n",
      "[2022-09-26 20:37:28,547] PROGRESS: at sentence #3960000, processed 1742377150 words, keeping 9460504 word types\n",
      "[2022-09-26 20:37:51,598] PROGRESS: at sentence #3970000, processed 1745568881 words, keeping 9471306 word types\n",
      "[2022-09-26 20:38:17,200] PROGRESS: at sentence #3980000, processed 1748836196 words, keeping 9484403 word types\n",
      "[2022-09-26 20:38:42,229] PROGRESS: at sentence #3990000, processed 1752274171 words, keeping 9496609 word types\n",
      "[2022-09-26 20:39:08,202] PROGRESS: at sentence #4000000, processed 1755587991 words, keeping 9510179 word types\n",
      "[2022-09-26 20:39:31,347] PROGRESS: at sentence #4010000, processed 1758659650 words, keeping 9522839 word types\n",
      "[2022-09-26 20:39:55,430] PROGRESS: at sentence #4020000, processed 1761778215 words, keeping 9536263 word types\n",
      "[2022-09-26 20:40:21,308] PROGRESS: at sentence #4030000, processed 1765066298 words, keeping 9549347 word types\n",
      "[2022-09-26 20:40:47,200] PROGRESS: at sentence #4040000, processed 1768331169 words, keeping 9563168 word types\n",
      "[2022-09-26 20:41:19,462] PROGRESS: at sentence #4050000, processed 1771779584 words, keeping 9586248 word types\n",
      "[2022-09-26 20:41:46,769] PROGRESS: at sentence #4060000, processed 1775113175 words, keeping 9599338 word types\n",
      "[2022-09-26 20:42:10,444] PROGRESS: at sentence #4070000, processed 1778265350 words, keeping 9611105 word types\n",
      "[2022-09-26 20:42:35,118] PROGRESS: at sentence #4080000, processed 1781482697 words, keeping 9624394 word types\n",
      "[2022-09-26 20:43:02,152] PROGRESS: at sentence #4090000, processed 1784960784 words, keeping 9638241 word types\n",
      "[2022-09-26 20:43:26,018] PROGRESS: at sentence #4100000, processed 1788201490 words, keeping 9650302 word types\n",
      "[2022-09-26 20:43:49,239] PROGRESS: at sentence #4110000, processed 1791370794 words, keeping 9665485 word types\n",
      "[2022-09-26 20:44:13,272] PROGRESS: at sentence #4120000, processed 1794600088 words, keeping 9678897 word types\n",
      "[2022-09-26 20:44:36,941] PROGRESS: at sentence #4130000, processed 1797749781 words, keeping 9696306 word types\n",
      "[2022-09-26 20:45:02,079] PROGRESS: at sentence #4140000, processed 1801014246 words, keeping 9711259 word types\n",
      "[2022-09-26 20:45:28,518] PROGRESS: at sentence #4150000, processed 1804233104 words, keeping 9734198 word types\n",
      "[2022-09-26 20:45:53,750] PROGRESS: at sentence #4160000, processed 1807405750 words, keeping 9752211 word types\n",
      "[2022-09-26 20:46:22,989] PROGRESS: at sentence #4170000, processed 1810925205 words, keeping 9764926 word types\n",
      "[2022-09-26 20:46:50,036] PROGRESS: at sentence #4180000, processed 1814447937 words, keeping 9777352 word types\n",
      "[2022-09-26 20:47:15,333] PROGRESS: at sentence #4190000, processed 1817791853 words, keeping 9789305 word types\n",
      "[2022-09-26 20:47:39,815] PROGRESS: at sentence #4200000, processed 1821065818 words, keeping 9801071 word types\n",
      "[2022-09-26 20:48:03,916] PROGRESS: at sentence #4210000, processed 1824336422 words, keeping 9813855 word types\n",
      "[2022-09-26 20:48:28,248] PROGRESS: at sentence #4220000, processed 1827832427 words, keeping 9828928 word types\n",
      "[2022-09-26 20:48:57,808] PROGRESS: at sentence #4230000, processed 1831195932 words, keeping 9839472 word types\n",
      "[2022-09-26 20:49:24,627] PROGRESS: at sentence #4240000, processed 1834530254 words, keeping 9851755 word types\n",
      "[2022-09-26 20:50:07,442] PROGRESS: at sentence #4250000, processed 1838188630 words, keeping 9864380 word types\n",
      "[2022-09-26 20:50:38,795] PROGRESS: at sentence #4260000, processed 1841807007 words, keeping 9877783 word types\n",
      "[2022-09-26 20:51:06,360] PROGRESS: at sentence #4270000, processed 1845126466 words, keeping 9888847 word types\n",
      "[2022-09-26 20:51:31,879] PROGRESS: at sentence #4280000, processed 1848362663 words, keeping 9900203 word types\n",
      "[2022-09-26 20:51:54,403] PROGRESS: at sentence #4290000, processed 1851533385 words, keeping 9910467 word types\n",
      "[2022-09-26 20:52:20,844] PROGRESS: at sentence #4300000, processed 1854853334 words, keeping 9921037 word types\n",
      "[2022-09-26 20:52:44,836] PROGRESS: at sentence #4310000, processed 1858147455 words, keeping 9933179 word types\n",
      "[2022-09-26 20:53:09,941] PROGRESS: at sentence #4320000, processed 1861569283 words, keeping 9945520 word types\n",
      "[2022-09-26 20:53:36,429] PROGRESS: at sentence #4330000, processed 1864873684 words, keeping 9958052 word types\n",
      "[2022-09-26 20:54:04,041] PROGRESS: at sentence #4340000, processed 1868147735 words, keeping 9969374 word types\n",
      "[2022-09-26 20:54:30,639] PROGRESS: at sentence #4350000, processed 1871403935 words, keeping 9980967 word types\n",
      "[2022-09-26 20:54:54,454] PROGRESS: at sentence #4360000, processed 1874554712 words, keeping 9991081 word types\n",
      "[2022-09-26 20:55:18,648] PROGRESS: at sentence #4370000, processed 1877761330 words, keeping 10004036 word types\n",
      "[2022-09-26 20:55:46,101] PROGRESS: at sentence #4380000, processed 1881114032 words, keeping 10016782 word types\n",
      "[2022-09-26 20:56:13,424] PROGRESS: at sentence #4390000, processed 1884413002 words, keeping 10029736 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 20:56:40,073] PROGRESS: at sentence #4400000, processed 1887662242 words, keeping 10042141 word types\n",
      "[2022-09-26 20:57:08,807] PROGRESS: at sentence #4410000, processed 1891376063 words, keeping 10093978 word types\n",
      "[2022-09-26 20:57:39,133] PROGRESS: at sentence #4420000, processed 1894867412 words, keeping 10107574 word types\n",
      "[2022-09-26 20:58:07,732] PROGRESS: at sentence #4430000, processed 1898177609 words, keeping 10119711 word types\n",
      "[2022-09-26 20:58:36,490] PROGRESS: at sentence #4440000, processed 1901647529 words, keeping 10130744 word types\n",
      "[2022-09-26 20:59:10,859] PROGRESS: at sentence #4450000, processed 1905380846 words, keeping 10142381 word types\n",
      "[2022-09-26 20:59:37,854] PROGRESS: at sentence #4460000, processed 1908657844 words, keeping 10153861 word types\n",
      "[2022-09-26 21:00:02,774] PROGRESS: at sentence #4470000, processed 1911868096 words, keeping 10166638 word types\n",
      "[2022-09-26 21:00:26,501] PROGRESS: at sentence #4480000, processed 1915174304 words, keeping 10177726 word types\n",
      "[2022-09-26 21:00:54,756] PROGRESS: at sentence #4490000, processed 1918649545 words, keeping 10190017 word types\n",
      "[2022-09-26 21:01:21,355] PROGRESS: at sentence #4500000, processed 1922028921 words, keeping 10202748 word types\n",
      "[2022-09-26 21:01:47,027] PROGRESS: at sentence #4510000, processed 1925269576 words, keeping 10215267 word types\n",
      "[2022-09-26 21:02:10,620] PROGRESS: at sentence #4520000, processed 1928378745 words, keeping 10225946 word types\n",
      "[2022-09-26 21:02:34,669] PROGRESS: at sentence #4530000, processed 1931377356 words, keeping 10237453 word types\n",
      "[2022-09-26 21:02:58,789] PROGRESS: at sentence #4540000, processed 1934486438 words, keeping 10251019 word types\n",
      "[2022-09-26 21:03:23,746] PROGRESS: at sentence #4550000, processed 1937788972 words, keeping 10265925 word types\n",
      "[2022-09-26 21:03:49,428] PROGRESS: at sentence #4560000, processed 1941032517 words, keeping 10279467 word types\n",
      "[2022-09-26 21:04:16,956] PROGRESS: at sentence #4570000, processed 1944509354 words, keeping 10292305 word types\n",
      "[2022-09-26 21:04:40,957] PROGRESS: at sentence #4580000, processed 1947624881 words, keeping 10303889 word types\n",
      "[2022-09-26 21:05:13,333] PROGRESS: at sentence #4590000, processed 1951133443 words, keeping 10318485 word types\n",
      "[2022-09-26 21:05:40,725] PROGRESS: at sentence #4600000, processed 1954339107 words, keeping 10331825 word types\n",
      "[2022-09-26 21:06:09,352] PROGRESS: at sentence #4610000, processed 1957616905 words, keeping 10345415 word types\n",
      "[2022-09-26 21:06:37,179] PROGRESS: at sentence #4620000, processed 1960818778 words, keeping 10359586 word types\n",
      "[2022-09-26 21:07:02,724] PROGRESS: at sentence #4630000, processed 1963981571 words, keeping 10373553 word types\n",
      "[2022-09-26 21:07:27,207] PROGRESS: at sentence #4640000, processed 1967120324 words, keeping 10389215 word types\n",
      "[2022-09-26 21:07:50,375] PROGRESS: at sentence #4650000, processed 1970168974 words, keeping 10400010 word types\n",
      "[2022-09-26 21:08:15,886] PROGRESS: at sentence #4660000, processed 1973186652 words, keeping 10412618 word types\n",
      "[2022-09-26 21:08:41,162] PROGRESS: at sentence #4670000, processed 1976165322 words, keeping 10424374 word types\n",
      "[2022-09-26 21:09:11,810] PROGRESS: at sentence #4680000, processed 1979072664 words, keeping 10436751 word types\n",
      "[2022-09-26 21:09:35,703] PROGRESS: at sentence #4690000, processed 1981887687 words, keeping 10449584 word types\n",
      "[2022-09-26 21:09:57,431] PROGRESS: at sentence #4700000, processed 1984671704 words, keeping 10463540 word types\n",
      "[2022-09-26 21:10:22,670] PROGRESS: at sentence #4710000, processed 1987668419 words, keeping 10476188 word types\n",
      "[2022-09-26 21:10:46,454] PROGRESS: at sentence #4720000, processed 1990461749 words, keeping 10489148 word types\n",
      "[2022-09-26 21:11:10,020] PROGRESS: at sentence #4730000, processed 1993218622 words, keeping 10501206 word types\n",
      "[2022-09-26 21:11:13,258] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-26 21:11:13,912] collected 10501799 word types from a corpus of 1993339819 raw words and 4730463 sentences\n",
      "[2022-09-26 21:11:13,913] Loading a fresh vocabulary\n",
      "[2022-09-26 21:11:27,438] effective_min_count=5 retains 2673717 unique words (25% of original 10501799, drops 7828082)\n",
      "[2022-09-26 21:11:27,439] effective_min_count=5 leaves 1981428619 word corpus (99% of original 1993339819, drops 11911200)\n",
      "[2022-09-26 21:11:35,909] deleting the raw counts dictionary of 10501799 items\n",
      "[2022-09-26 21:11:36,485] sample=0.001 downsamples 3 most-common words\n",
      "[2022-09-26 21:11:36,486] downsampling leaves estimated 1956830255 word corpus (98.8% of prior 1981428619)\n",
      "[2022-09-26 21:11:50,174] estimated required memory for 2673717 words and 300 dimensions: 7753779300 bytes\n",
      "[2022-09-26 21:11:50,176] resetting layer weights\n",
      "[2022-09-26 21:28:15,383] training model with 40 workers on 2673717 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "[2022-09-26 21:31:58,259] EPOCHs No. 1 - PROGRESS: at 1.00% examples, 398577 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:33:43,883] EPOCHs No. 1 - PROGRESS: at 2.00% examples, 387924 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:36:16,174] EPOCHs No. 1 - PROGRESS: at 3.00% examples, 385142 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:38:34,484] EPOCHs No. 1 - PROGRESS: at 4.00% examples, 376921 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:40:42,919] EPOCHs No. 1 - PROGRESS: at 5.00% examples, 368935 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:42:55,501] EPOCHs No. 1 - PROGRESS: at 6.00% examples, 357670 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:44:49,864] EPOCHs No. 1 - PROGRESS: at 7.00% examples, 351753 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:46:42,142] EPOCHs No. 1 - PROGRESS: at 8.00% examples, 345921 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:48:30,561] EPOCHs No. 1 - PROGRESS: at 9.00% examples, 341144 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:50:23,107] EPOCHs No. 1 - PROGRESS: at 10.00% examples, 334716 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:52:12,219] EPOCHs No. 1 - PROGRESS: at 11.00% examples, 329732 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:53:57,334] EPOCHs No. 1 - PROGRESS: at 12.00% examples, 325271 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:55:36,375] EPOCHs No. 1 - PROGRESS: at 13.00% examples, 321714 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:57:15,008] EPOCHs No. 1 - PROGRESS: at 14.00% examples, 317776 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 21:58:53,766] EPOCHs No. 1 - PROGRESS: at 15.00% examples, 314162 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:00:37,760] EPOCHs No. 1 - PROGRESS: at 16.00% examples, 309966 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:02:17,378] EPOCHs No. 1 - PROGRESS: at 17.00% examples, 306372 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:03:50,940] EPOCHs No. 1 - PROGRESS: at 18.00% examples, 303757 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:05:29,800] EPOCHs No. 1 - PROGRESS: at 19.00% examples, 300575 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-26 22:07:05,913] EPOCHs No. 1 - PROGRESS: at 20.00% examples, 297713 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:08:40,715] EPOCHs No. 1 - PROGRESS: at 21.00% examples, 294842 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:10:13,220] EPOCHs No. 1 - PROGRESS: at 22.00% examples, 292255 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:11:42,065] EPOCHs No. 1 - PROGRESS: at 23.00% examples, 289889 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:13:17,397] EPOCHs No. 1 - PROGRESS: at 24.00% examples, 287130 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:14:47,543] EPOCHs No. 1 - PROGRESS: at 25.00% examples, 284837 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:16:24,573] EPOCHs No. 1 - PROGRESS: at 26.00% examples, 282124 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:18:03,069] EPOCHs No. 1 - PROGRESS: at 27.00% examples, 279577 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:19:34,505] EPOCHs No. 1 - PROGRESS: at 28.00% examples, 277491 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:21:09,096] EPOCHs No. 1 - PROGRESS: at 29.00% examples, 275308 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-26 22:22:46,226] EPOCHs No. 1 - PROGRESS: at 30.00% examples, 273315 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:24:21,413] EPOCHs No. 1 - PROGRESS: at 31.00% examples, 271129 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:25:58,305] EPOCHs No. 1 - PROGRESS: at 32.00% examples, 268833 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:27:29,249] EPOCHs No. 1 - PROGRESS: at 33.00% examples, 266709 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:29:06,876] EPOCHs No. 1 - PROGRESS: at 34.00% examples, 264486 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:30:42,303] EPOCHs No. 1 - PROGRESS: at 35.00% examples, 262490 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:32:21,086] EPOCHs No. 1 - PROGRESS: at 36.00% examples, 260282 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:34:02,442] EPOCHs No. 1 - PROGRESS: at 37.00% examples, 258012 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:36:06,041] EPOCHs No. 1 - PROGRESS: at 38.00% examples, 254610 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:37:41,149] EPOCHs No. 1 - PROGRESS: at 39.00% examples, 253036 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:39:16,209] EPOCHs No. 1 - PROGRESS: at 40.00% examples, 251470 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:40:54,210] EPOCHs No. 1 - PROGRESS: at 41.00% examples, 249789 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:42:45,162] EPOCHs No. 1 - PROGRESS: at 42.00% examples, 247871 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:44:27,406] EPOCHs No. 1 - PROGRESS: at 43.00% examples, 246138 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:46:13,552] EPOCHs No. 1 - PROGRESS: at 44.00% examples, 244435 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:47:52,692] EPOCHs No. 1 - PROGRESS: at 45.00% examples, 243000 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:49:34,060] EPOCHs No. 1 - PROGRESS: at 46.00% examples, 241403 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:51:21,324] EPOCHs No. 1 - PROGRESS: at 47.00% examples, 239752 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:53:10,014] EPOCHs No. 1 - PROGRESS: at 48.00% examples, 238095 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:54:53,075] EPOCHs No. 1 - PROGRESS: at 49.00% examples, 236780 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:56:33,423] EPOCHs No. 1 - PROGRESS: at 50.00% examples, 235662 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:58:12,745] EPOCHs No. 1 - PROGRESS: at 51.00% examples, 234531 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 22:59:46,403] EPOCHs No. 1 - PROGRESS: at 52.00% examples, 233612 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:01:37,268] EPOCHs No. 1 - PROGRESS: at 53.00% examples, 232053 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:03:16,524] EPOCHs No. 1 - PROGRESS: at 54.00% examples, 230973 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:05:09,247] EPOCHs No. 1 - PROGRESS: at 55.00% examples, 229426 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:07:00,493] EPOCHs No. 1 - PROGRESS: at 56.00% examples, 227849 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:08:45,129] EPOCHs No. 1 - PROGRESS: at 57.00% examples, 227046 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:10:26,883] EPOCHs No. 1 - PROGRESS: at 58.00% examples, 226201 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:12:14,585] EPOCHs No. 1 - PROGRESS: at 59.00% examples, 225101 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:14:02,015] EPOCHs No. 1 - PROGRESS: at 60.00% examples, 223932 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:15:53,208] EPOCHs No. 1 - PROGRESS: at 61.00% examples, 222764 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:17:41,331] EPOCHs No. 1 - PROGRESS: at 62.00% examples, 221725 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:19:29,849] EPOCHs No. 1 - PROGRESS: at 63.00% examples, 220733 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:21:18,391] EPOCHs No. 1 - PROGRESS: at 64.00% examples, 219630 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:23:10,856] EPOCHs No. 1 - PROGRESS: at 65.00% examples, 218480 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:25:11,697] EPOCHs No. 1 - PROGRESS: at 66.00% examples, 217311 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:27:09,127] EPOCHs No. 1 - PROGRESS: at 67.00% examples, 216182 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:29:07,255] EPOCHs No. 1 - PROGRESS: at 68.00% examples, 215261 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:30:54,687] EPOCHs No. 1 - PROGRESS: at 69.00% examples, 214420 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:32:52,715] EPOCHs No. 1 - PROGRESS: at 70.00% examples, 213360 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:34:36,688] EPOCHs No. 1 - PROGRESS: at 71.00% examples, 212642 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:36:23,775] EPOCHs No. 1 - PROGRESS: at 72.00% examples, 211891 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:38:13,583] EPOCHs No. 1 - PROGRESS: at 73.00% examples, 210988 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:39:58,076] EPOCHs No. 1 - PROGRESS: at 74.00% examples, 210291 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:41:45,284] EPOCHs No. 1 - PROGRESS: at 75.00% examples, 209554 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:43:36,512] EPOCHs No. 1 - PROGRESS: at 76.00% examples, 208770 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:45:17,436] EPOCHs No. 1 - PROGRESS: at 77.00% examples, 208031 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:47:00,827] EPOCHs No. 1 - PROGRESS: at 78.00% examples, 207415 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:48:49,780] EPOCHs No. 1 - PROGRESS: at 79.00% examples, 206718 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:50:42,037] EPOCHs No. 1 - PROGRESS: at 80.00% examples, 205835 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:52:29,522] EPOCHs No. 1 - PROGRESS: at 81.00% examples, 205077 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:54:15,610] EPOCHs No. 1 - PROGRESS: at 82.00% examples, 204360 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:56:07,002] EPOCHs No. 1 - PROGRESS: at 83.00% examples, 203620 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:57:55,223] EPOCHs No. 1 - PROGRESS: at 84.00% examples, 202917 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-26 23:59:46,007] EPOCHs No. 1 - PROGRESS: at 85.00% examples, 202184 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:01:47,575] EPOCHs No. 1 - PROGRESS: at 86.00% examples, 201270 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:03:37,851] EPOCHs No. 1 - PROGRESS: at 87.00% examples, 200608 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:05:31,007] EPOCHs No. 1 - PROGRESS: at 88.00% examples, 199863 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:07:27,004] EPOCHs No. 1 - PROGRESS: at 89.00% examples, 199163 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:09:38,608] EPOCHs No. 1 - PROGRESS: at 90.00% examples, 198234 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:11:31,021] EPOCHs No. 1 - PROGRESS: at 91.00% examples, 197595 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:13:26,282] EPOCHs No. 1 - PROGRESS: at 92.00% examples, 196925 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:15:34,198] EPOCHs No. 1 - PROGRESS: at 93.00% examples, 196002 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:17:47,251] EPOCHs No. 1 - PROGRESS: at 94.00% examples, 195172 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:19:44,221] EPOCHs No. 1 - PROGRESS: at 95.00% examples, 194524 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:21:35,637] EPOCHs No. 1 - PROGRESS: at 96.00% examples, 193931 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:23:34,248] EPOCHs No. 1 - PROGRESS: at 97.00% examples, 193303 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:25:34,952] EPOCHs No. 1 - PROGRESS: at 98.00% examples, 192582 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:27:29,838] EPOCHs No. 1 - PROGRESS: at 99.00% examples, 191878 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:29:16,536] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-27 00:29:17,251] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-27 00:29:17,312] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-27 00:29:17,313] worker thread finished; awaiting finish of 37 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 00:29:17,313] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-27 00:29:17,314] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-27 00:29:17,314] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-27 00:29:17,315] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-27 00:29:17,315] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-27 00:29:17,316] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-27 00:29:17,316] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-27 00:29:17,317] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-27 00:29:17,317] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-27 00:29:17,318] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-27 00:29:17,318] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-27 00:29:17,318] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-27 00:29:17,319] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-27 00:29:17,319] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-27 00:29:17,320] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-27 00:29:17,320] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-27 00:29:17,321] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-27 00:29:17,321] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-27 00:29:17,322] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-27 00:29:17,322] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-27 00:29:17,323] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-27 00:29:17,323] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-27 00:29:17,324] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-27 00:29:17,324] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-27 00:29:17,324] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-27 00:29:17,325] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-27 00:29:17,325] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-27 00:29:17,326] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-27 00:29:17,326] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-27 00:29:17,327] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-27 00:29:17,327] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-27 00:29:17,328] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-27 00:29:17,328] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-27 00:29:17,329] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-27 00:29:17,329] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-27 00:29:17,329] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-27 00:29:17,344] EPOCHs No. 1 - PROGRESS: at 100.00% examples, 191256 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 00:29:17,345] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-27 00:29:17,345] EPOCH - 1 : training on 1993339819 raw words (2077412282 effective words) took 10861.9s, 191256 effective words/s\n",
      "[2022-09-27 00:33:12,169] EPOCHs No. 2 - PROGRESS: at 1.00% examples, 378313 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:34:57,050] EPOCHs No. 2 - PROGRESS: at 2.00% examples, 375259 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:37:27,817] EPOCHs No. 2 - PROGRESS: at 3.00% examples, 377549 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:39:43,716] EPOCHs No. 2 - PROGRESS: at 4.00% examples, 372553 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:41:50,717] EPOCHs No. 2 - PROGRESS: at 5.00% examples, 366083 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:44:06,039] EPOCHs No. 2 - PROGRESS: at 6.00% examples, 354224 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:46:02,606] EPOCHs No. 2 - PROGRESS: at 7.00% examples, 347965 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:47:57,690] EPOCHs No. 2 - PROGRESS: at 8.00% examples, 341730 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:49:46,672] EPOCHs No. 2 - PROGRESS: at 9.00% examples, 337221 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:51:35,775] EPOCHs No. 2 - PROGRESS: at 10.00% examples, 332042 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:53:24,862] EPOCHs No. 2 - PROGRESS: at 11.00% examples, 327302 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:55:08,889] EPOCHs No. 2 - PROGRESS: at 12.00% examples, 323274 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:56:49,929] EPOCHs No. 2 - PROGRESS: at 13.00% examples, 319460 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 00:58:29,780] EPOCHs No. 2 - PROGRESS: at 14.00% examples, 315455 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:00:08,305] EPOCHs No. 2 - PROGRESS: at 15.00% examples, 312029 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:01:51,427] EPOCHs No. 2 - PROGRESS: at 16.00% examples, 308111 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:03:29,669] EPOCHs No. 2 - PROGRESS: at 17.00% examples, 304832 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:05:04,206] EPOCHs No. 2 - PROGRESS: at 18.00% examples, 302159 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:06:42,868] EPOCHs No. 2 - PROGRESS: at 19.00% examples, 299090 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:08:20,707] EPOCHs No. 2 - PROGRESS: at 20.00% examples, 296085 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:09:58,860] EPOCHs No. 2 - PROGRESS: at 21.00% examples, 292889 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:11:33,292] EPOCHs No. 2 - PROGRESS: at 22.00% examples, 290161 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-27 01:13:02,633] EPOCHs No. 2 - PROGRESS: at 23.00% examples, 287836 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:14:36,593] EPOCHs No. 2 - PROGRESS: at 24.00% examples, 285312 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:16:07,008] EPOCHs No. 2 - PROGRESS: at 25.00% examples, 283063 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:17:44,304] EPOCHs No. 2 - PROGRESS: at 26.00% examples, 280400 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:19:22,302] EPOCHs No. 2 - PROGRESS: at 27.00% examples, 277965 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:20:56,154] EPOCHs No. 2 - PROGRESS: at 28.00% examples, 275729 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:22:33,325] EPOCHs No. 2 - PROGRESS: at 29.00% examples, 273397 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:24:07,694] EPOCHs No. 2 - PROGRESS: at 30.00% examples, 271695 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 01:25:41,093] EPOCHs No. 2 - PROGRESS: at 31.00% examples, 269710 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:27:17,060] EPOCHs No. 2 - PROGRESS: at 32.00% examples, 267537 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:28:50,257] EPOCHs No. 2 - PROGRESS: at 33.00% examples, 265288 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:30:29,320] EPOCHs No. 2 - PROGRESS: at 34.00% examples, 263012 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:32:07,200] EPOCHs No. 2 - PROGRESS: at 35.00% examples, 260894 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:33:46,661] EPOCHs No. 2 - PROGRESS: at 36.00% examples, 258695 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:35:28,315] EPOCHs No. 2 - PROGRESS: at 37.00% examples, 256459 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:37:28,264] EPOCHs No. 2 - PROGRESS: at 38.00% examples, 253349 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:39:04,480] EPOCHs No. 2 - PROGRESS: at 39.00% examples, 251745 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:40:40,256] EPOCHs No. 2 - PROGRESS: at 40.00% examples, 250174 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:42:22,522] EPOCHs No. 2 - PROGRESS: at 41.00% examples, 248289 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:44:13,737] EPOCHs No. 2 - PROGRESS: at 42.00% examples, 246404 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:45:57,117] EPOCHs No. 2 - PROGRESS: at 43.00% examples, 244654 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:47:40,361] EPOCHs No. 2 - PROGRESS: at 44.00% examples, 243144 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 01:49:19,234] EPOCHs No. 2 - PROGRESS: at 45.00% examples, 241757 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:50:57,205] EPOCHs No. 2 - PROGRESS: at 46.00% examples, 240360 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:52:43,540] EPOCHs No. 2 - PROGRESS: at 47.00% examples, 238782 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:54:37,766] EPOCHs No. 2 - PROGRESS: at 48.00% examples, 236897 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:56:19,176] EPOCHs No. 2 - PROGRESS: at 49.00% examples, 235686 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:57:59,048] EPOCHs No. 2 - PROGRESS: at 50.00% examples, 234615 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 01:59:39,067] EPOCHs No. 2 - PROGRESS: at 51.00% examples, 233478 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:01:13,698] EPOCHs No. 2 - PROGRESS: at 52.00% examples, 232540 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:02:57,457] EPOCHs No. 2 - PROGRESS: at 53.00% examples, 231301 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:04:37,279] EPOCHs No. 2 - PROGRESS: at 54.00% examples, 230215 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:06:28,157] EPOCHs No. 2 - PROGRESS: at 55.00% examples, 228760 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:08:25,289] EPOCHs No. 2 - PROGRESS: at 56.00% examples, 226975 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:10:11,719] EPOCHs No. 2 - PROGRESS: at 57.00% examples, 226123 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:11:54,958] EPOCHs No. 2 - PROGRESS: at 58.00% examples, 225242 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:13:42,608] EPOCHs No. 2 - PROGRESS: at 59.00% examples, 224165 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:15:28,674] EPOCHs No. 2 - PROGRESS: at 60.00% examples, 223064 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:17:17,220] EPOCHs No. 2 - PROGRESS: at 61.00% examples, 222007 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:19:05,399] EPOCHs No. 2 - PROGRESS: at 62.00% examples, 220981 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:20:53,206] EPOCHs No. 2 - PROGRESS: at 63.00% examples, 220028 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:22:40,932] EPOCHs No. 2 - PROGRESS: at 64.00% examples, 218966 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:24:32,295] EPOCHs No. 2 - PROGRESS: at 65.00% examples, 217865 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:26:35,894] EPOCHs No. 2 - PROGRESS: at 66.00% examples, 216625 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:28:32,743] EPOCHs No. 2 - PROGRESS: at 67.00% examples, 215528 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:30:33,446] EPOCHs No. 2 - PROGRESS: at 68.00% examples, 214544 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:32:21,993] EPOCHs No. 2 - PROGRESS: at 69.00% examples, 213685 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:34:13,641] EPOCHs No. 2 - PROGRESS: at 70.00% examples, 212821 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:35:57,299] EPOCHs No. 2 - PROGRESS: at 71.00% examples, 212121 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:37:45,686] EPOCHs No. 2 - PROGRESS: at 72.00% examples, 211343 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:39:37,084] EPOCHs No. 2 - PROGRESS: at 73.00% examples, 210407 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:41:21,736] EPOCHs No. 2 - PROGRESS: at 74.00% examples, 209715 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:43:04,849] EPOCHs No. 2 - PROGRESS: at 75.00% examples, 209094 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:45:02,525] EPOCHs No. 2 - PROGRESS: at 76.00% examples, 208153 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:46:43,795] EPOCHs No. 2 - PROGRESS: at 77.00% examples, 207416 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:48:25,483] EPOCHs No. 2 - PROGRESS: at 78.00% examples, 206852 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:50:14,931] EPOCHs No. 2 - PROGRESS: at 79.00% examples, 206152 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:52:07,554] EPOCHs No. 2 - PROGRESS: at 80.00% examples, 205270 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:53:47,977] EPOCHs No. 2 - PROGRESS: at 81.00% examples, 204687 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:55:29,567] EPOCHs No. 2 - PROGRESS: at 82.00% examples, 204081 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:57:19,874] EPOCHs No. 2 - PROGRESS: at 83.00% examples, 203370 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 02:59:08,003] EPOCHs No. 2 - PROGRESS: at 84.00% examples, 202673 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:00:58,822] EPOCHs No. 2 - PROGRESS: at 85.00% examples, 201943 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:03:01,267] EPOCHs No. 2 - PROGRESS: at 86.00% examples, 201014 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:05:01,566] EPOCHs No. 2 - PROGRESS: at 87.00% examples, 200141 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:06:52,144] EPOCHs No. 2 - PROGRESS: at 88.00% examples, 199457 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:08:47,935] EPOCHs No. 2 - PROGRESS: at 89.00% examples, 198768 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:10:58,681] EPOCHs No. 2 - PROGRESS: at 90.00% examples, 197865 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:12:52,330] EPOCHs No. 2 - PROGRESS: at 91.00% examples, 197206 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:14:47,855] EPOCHs No. 2 - PROGRESS: at 92.00% examples, 196537 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:16:43,593] EPOCHs No. 2 - PROGRESS: at 93.00% examples, 195857 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:18:56,705] EPOCHs No. 2 - PROGRESS: at 94.00% examples, 195029 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:20:54,722] EPOCHs No. 2 - PROGRESS: at 95.00% examples, 194363 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:22:46,711] EPOCHs No. 2 - PROGRESS: at 96.00% examples, 193761 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:24:46,673] EPOCHs No. 2 - PROGRESS: at 97.00% examples, 193111 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:26:48,302] EPOCHs No. 2 - PROGRESS: at 98.00% examples, 192376 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:28:53,121] EPOCHs No. 2 - PROGRESS: at 99.00% examples, 191498 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:30:40,012] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-27 03:30:41,680] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-27 03:30:41,725] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-27 03:30:41,729] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-27 03:30:41,730] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-27 03:30:41,730] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-27 03:30:41,731] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-27 03:30:41,731] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-27 03:30:41,732] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-27 03:30:41,732] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-27 03:30:41,733] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-27 03:30:41,733] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-27 03:30:41,734] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-27 03:30:41,734] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-27 03:30:41,735] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-27 03:30:41,735] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-27 03:30:41,736] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-27 03:30:41,736] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-27 03:30:41,737] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-27 03:30:41,737] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-27 03:30:41,738] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-27 03:30:41,738] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-27 03:30:41,739] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-27 03:30:41,739] worker thread finished; awaiting finish of 17 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 03:30:41,740] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-27 03:30:41,740] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-27 03:30:41,741] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-27 03:30:41,741] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-27 03:30:41,742] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-27 03:30:41,743] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-27 03:30:41,743] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-27 03:30:41,744] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-27 03:30:41,744] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-27 03:30:41,745] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-27 03:30:41,745] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-27 03:30:41,746] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-27 03:30:41,746] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-27 03:30:41,747] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-27 03:30:41,747] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-27 03:30:41,748] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-27 03:30:41,756] EPOCHs No. 2 - PROGRESS: at 100.00% examples, 190862 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 03:30:41,756] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-27 03:30:41,757] EPOCH - 2 : training on 1993339819 raw words (2077412282 effective words) took 10884.4s, 190862 effective words/s\n",
      "[2022-09-27 03:34:26,077] EPOCHs No. 3 - PROGRESS: at 1.00% examples, 395932 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:36:13,837] EPOCHs No. 3 - PROGRESS: at 2.00% examples, 383808 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:38:48,085] EPOCHs No. 3 - PROGRESS: at 3.00% examples, 380761 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:41:08,244] EPOCHs No. 3 - PROGRESS: at 4.00% examples, 372481 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:43:17,886] EPOCHs No. 3 - PROGRESS: at 5.00% examples, 364745 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:45:39,033] EPOCHs No. 3 - PROGRESS: at 6.00% examples, 350832 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:47:38,106] EPOCHs No. 3 - PROGRESS: at 7.00% examples, 344166 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:49:32,614] EPOCHs No. 3 - PROGRESS: at 8.00% examples, 338551 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:51:24,695] EPOCHs No. 3 - PROGRESS: at 9.00% examples, 333527 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:53:14,167] EPOCHs No. 3 - PROGRESS: at 10.00% examples, 328608 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:55:08,488] EPOCHs No. 3 - PROGRESS: at 11.00% examples, 323013 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:56:56,289] EPOCHs No. 3 - PROGRESS: at 12.00% examples, 318553 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 03:58:41,328] EPOCHs No. 3 - PROGRESS: at 13.00% examples, 314326 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 04:00:23,801] EPOCHs No. 3 - PROGRESS: at 14.00% examples, 310223 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:02:08,496] EPOCHs No. 3 - PROGRESS: at 15.00% examples, 306111 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:03:56,969] EPOCHs No. 3 - PROGRESS: at 16.00% examples, 301758 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:05:40,297] EPOCHs No. 3 - PROGRESS: at 17.00% examples, 298118 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 04:07:20,253] EPOCHs No. 3 - PROGRESS: at 18.00% examples, 295061 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:09:02,241] EPOCHs No. 3 - PROGRESS: at 19.00% examples, 291943 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:10:42,645] EPOCHs No. 3 - PROGRESS: at 20.00% examples, 288989 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:12:18,991] EPOCHs No. 3 - PROGRESS: at 21.00% examples, 286353 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:13:58,114] EPOCHs No. 3 - PROGRESS: at 22.00% examples, 283409 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:15:30,773] EPOCHs No. 3 - PROGRESS: at 23.00% examples, 281014 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:17:08,396] EPOCHs No. 3 - PROGRESS: at 24.00% examples, 278411 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:18:41,662] EPOCHs No. 3 - PROGRESS: at 25.00% examples, 276159 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:20:23,650] EPOCHs No. 3 - PROGRESS: at 26.00% examples, 273353 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:22:07,286] EPOCHs No. 3 - PROGRESS: at 27.00% examples, 270712 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:23:44,682] EPOCHs No. 3 - PROGRESS: at 28.00% examples, 268442 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:25:25,324] EPOCHs No. 3 - PROGRESS: at 29.00% examples, 266104 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:27:04,229] EPOCHs No. 3 - PROGRESS: at 30.00% examples, 264295 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:28:42,230] EPOCHs No. 3 - PROGRESS: at 31.00% examples, 262214 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:30:25,023] EPOCHs No. 3 - PROGRESS: at 32.00% examples, 259805 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:32:00,068] EPOCHs No. 3 - PROGRESS: at 33.00% examples, 257686 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:33:44,327] EPOCHs No. 3 - PROGRESS: at 34.00% examples, 255321 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:35:22,776] EPOCHs No. 3 - PROGRESS: at 35.00% examples, 253420 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:37:05,748] EPOCHs No. 3 - PROGRESS: at 36.00% examples, 251248 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:38:51,523] EPOCHs No. 3 - PROGRESS: at 37.00% examples, 249009 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 04:41:02,452] EPOCHs No. 3 - PROGRESS: at 38.00% examples, 245559 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:42:43,466] EPOCHs No. 3 - PROGRESS: at 39.00% examples, 243906 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:44:24,900] EPOCHs No. 3 - PROGRESS: at 40.00% examples, 242242 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:46:08,221] EPOCHs No. 3 - PROGRESS: at 41.00% examples, 240539 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:48:04,721] EPOCHs No. 3 - PROGRESS: at 42.00% examples, 238625 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:49:53,925] EPOCHs No. 3 - PROGRESS: at 43.00% examples, 236808 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:51:43,565] EPOCHs No. 3 - PROGRESS: at 44.00% examples, 235202 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:53:32,698] EPOCHs No. 3 - PROGRESS: at 45.00% examples, 233535 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:55:17,132] EPOCHs No. 3 - PROGRESS: at 46.00% examples, 232048 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:57:09,951] EPOCHs No. 3 - PROGRESS: at 47.00% examples, 230406 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 04:59:06,929] EPOCHs No. 3 - PROGRESS: at 48.00% examples, 228646 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:00:55,068] EPOCHs No. 3 - PROGRESS: at 49.00% examples, 227349 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:02:40,439] EPOCHs No. 3 - PROGRESS: at 50.00% examples, 226240 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:04:26,150] EPOCHs No. 3 - PROGRESS: at 51.00% examples, 225064 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:06:05,270] EPOCHs No. 3 - PROGRESS: at 52.00% examples, 224123 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:07:57,399] EPOCHs No. 3 - PROGRESS: at 53.00% examples, 222758 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:09:48,840] EPOCHs No. 3 - PROGRESS: at 54.00% examples, 221422 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:11:47,740] EPOCHs No. 3 - PROGRESS: at 55.00% examples, 219891 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:13:45,850] EPOCHs No. 3 - PROGRESS: at 56.00% examples, 218307 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:15:37,973] EPOCHs No. 3 - PROGRESS: at 57.00% examples, 217437 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:17:24,226] EPOCHs No. 3 - PROGRESS: at 58.00% examples, 216628 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:19:16,484] EPOCHs No. 3 - PROGRESS: at 59.00% examples, 215581 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 05:21:09,739] EPOCHs No. 3 - PROGRESS: at 60.00% examples, 214426 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:23:03,168] EPOCHs No. 3 - PROGRESS: at 61.00% examples, 213393 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:25:02,731] EPOCHs No. 3 - PROGRESS: at 62.00% examples, 212190 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:26:57,570] EPOCHs No. 3 - PROGRESS: at 63.00% examples, 211197 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:28:51,040] EPOCHs No. 3 - PROGRESS: at 64.00% examples, 210142 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:30:49,905] EPOCHs No. 3 - PROGRESS: at 65.00% examples, 209003 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:32:57,960] EPOCHs No. 3 - PROGRESS: at 66.00% examples, 207836 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:35:02,993] EPOCHs No. 3 - PROGRESS: at 67.00% examples, 206693 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:37:13,689] EPOCHs No. 3 - PROGRESS: at 68.00% examples, 205619 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:39:10,294] EPOCHs No. 3 - PROGRESS: at 69.00% examples, 204706 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:41:08,683] EPOCHs No. 3 - PROGRESS: at 70.00% examples, 203830 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:43:01,356] EPOCHs No. 3 - PROGRESS: at 71.00% examples, 203046 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:45:01,310] EPOCHs No. 3 - PROGRESS: at 72.00% examples, 202133 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:46:59,245] EPOCHs No. 3 - PROGRESS: at 73.00% examples, 201202 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:48:53,555] EPOCHs No. 3 - PROGRESS: at 74.00% examples, 200423 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:50:44,695] EPOCHs No. 3 - PROGRESS: at 75.00% examples, 199752 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:52:43,733] EPOCHs No. 3 - PROGRESS: at 76.00% examples, 198950 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:54:32,366] EPOCHs No. 3 - PROGRESS: at 77.00% examples, 198184 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:56:22,312] EPOCHs No. 3 - PROGRESS: at 78.00% examples, 197565 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 05:58:19,123] EPOCHs No. 3 - PROGRESS: at 79.00% examples, 196847 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:00:18,772] EPOCHs No. 3 - PROGRESS: at 80.00% examples, 195968 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:02:07,953] EPOCHs No. 3 - PROGRESS: at 81.00% examples, 195325 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:03:58,689] EPOCHs No. 3 - PROGRESS: at 82.00% examples, 194657 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:06:05,530] EPOCHs No. 3 - PROGRESS: at 83.00% examples, 193746 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:08:01,828] EPOCHs No. 3 - PROGRESS: at 84.00% examples, 193024 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:10:00,669] EPOCHs No. 3 - PROGRESS: at 85.00% examples, 192279 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:12:10,562] EPOCHs No. 3 - PROGRESS: at 86.00% examples, 191369 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:14:10,246] EPOCHs No. 3 - PROGRESS: at 87.00% examples, 190667 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:16:11,953] EPOCHs No. 3 - PROGRESS: at 88.00% examples, 189908 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:18:14,847] EPOCHs No. 3 - PROGRESS: at 89.00% examples, 189228 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:20:40,309] EPOCHs No. 3 - PROGRESS: at 90.00% examples, 188218 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:22:42,059] EPOCHs No. 3 - PROGRESS: at 91.00% examples, 187550 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:24:45,232] EPOCHs No. 3 - PROGRESS: at 92.00% examples, 186883 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:26:51,521] EPOCHs No. 3 - PROGRESS: at 93.00% examples, 186156 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:29:25,730] EPOCHs No. 3 - PROGRESS: at 94.00% examples, 185124 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:31:31,942] EPOCHs No. 3 - PROGRESS: at 95.00% examples, 184460 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:33:30,702] EPOCHs No. 3 - PROGRESS: at 96.00% examples, 183876 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:35:42,703] EPOCHs No. 3 - PROGRESS: at 97.00% examples, 183167 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:37:55,584] EPOCHs No. 3 - PROGRESS: at 98.00% examples, 182395 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:39:57,712] EPOCHs No. 3 - PROGRESS: at 99.00% examples, 181714 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:41:53,360] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-27 06:41:54,711] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-27 06:41:54,763] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-27 06:41:54,765] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-27 06:41:54,765] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-27 06:41:54,766] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-27 06:41:54,767] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-27 06:41:54,768] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-27 06:41:54,768] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-27 06:41:54,769] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-27 06:41:54,770] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-27 06:41:54,771] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-27 06:41:54,771] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-27 06:41:54,772] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-27 06:41:54,773] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-27 06:41:54,773] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-27 06:41:54,774] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-27 06:41:54,774] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-27 06:41:54,775] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-27 06:41:54,775] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-27 06:41:54,776] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-27 06:41:54,776] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-27 06:41:54,777] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-27 06:41:54,777] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-27 06:41:54,778] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-27 06:41:54,778] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-27 06:41:54,782] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-27 06:41:54,783] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-27 06:41:54,783] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-27 06:41:54,784] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-27 06:41:54,784] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-27 06:41:54,785] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-27 06:41:54,786] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-27 06:41:54,787] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-27 06:41:54,787] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-27 06:41:54,787] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-27 06:41:54,788] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-27 06:41:54,788] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-27 06:41:54,789] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-27 06:41:54,789] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-27 06:41:54,790] EPOCHs No. 3 - PROGRESS: at 100.00% examples, 181070 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 06:41:54,790] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-27 06:41:54,791] EPOCH - 3 : training on 1993339819 raw words (2077412282 effective words) took 11473.0s, 181070 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 06:45:48,274] EPOCHs No. 4 - PROGRESS: at 1.00% examples, 380471 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:47:32,417] EPOCHs No. 4 - PROGRESS: at 2.00% examples, 377558 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:50:01,688] EPOCHs No. 4 - PROGRESS: at 3.00% examples, 380313 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 06:52:17,502] EPOCHs No. 4 - PROGRESS: at 4.00% examples, 374737 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:54:23,034] EPOCHs No. 4 - PROGRESS: at 5.00% examples, 368588 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:56:35,586] EPOCHs No. 4 - PROGRESS: at 6.00% examples, 357396 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 06:58:28,613] EPOCHs No. 4 - PROGRESS: at 7.00% examples, 351987 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:00:20,804] EPOCHs No. 4 - PROGRESS: at 8.00% examples, 346155 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:02:10,817] EPOCHs No. 4 - PROGRESS: at 9.00% examples, 340907 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:03:56,857] EPOCHs No. 4 - PROGRESS: at 10.00% examples, 336149 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:05:44,980] EPOCHs No. 4 - PROGRESS: at 11.00% examples, 331265 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:07:28,056] EPOCHs No. 4 - PROGRESS: at 12.00% examples, 327126 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:09:08,596] EPOCHs No. 4 - PROGRESS: at 13.00% examples, 323130 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 07:10:45,735] EPOCHs No. 4 - PROGRESS: at 14.00% examples, 319370 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:12:25,815] EPOCHs No. 4 - PROGRESS: at 15.00% examples, 315425 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:14:10,773] EPOCHs No. 4 - PROGRESS: at 16.00% examples, 310990 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:15:47,972] EPOCHs No. 4 - PROGRESS: at 17.00% examples, 307700 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:17:21,957] EPOCHs No. 4 - PROGRESS: at 18.00% examples, 304955 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:18:58,350] EPOCHs No. 4 - PROGRESS: at 19.00% examples, 302043 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:20:35,646] EPOCHs No. 4 - PROGRESS: at 20.00% examples, 298955 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:22:10,995] EPOCHs No. 4 - PROGRESS: at 21.00% examples, 295956 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:23:46,635] EPOCHs No. 4 - PROGRESS: at 22.00% examples, 292944 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:25:15,119] EPOCHs No. 4 - PROGRESS: at 23.00% examples, 290598 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:26:50,763] EPOCHs No. 4 - PROGRESS: at 24.00% examples, 287774 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:28:20,749] EPOCHs No. 4 - PROGRESS: at 25.00% examples, 285471 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:29:57,305] EPOCHs No. 4 - PROGRESS: at 26.00% examples, 282777 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:31:37,075] EPOCHs No. 4 - PROGRESS: at 27.00% examples, 280077 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:33:09,879] EPOCHs No. 4 - PROGRESS: at 28.00% examples, 277855 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:34:47,501] EPOCHs No. 4 - PROGRESS: at 29.00% examples, 275395 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 07:36:24,265] EPOCHs No. 4 - PROGRESS: at 30.00% examples, 273429 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:37:57,979] EPOCHs No. 4 - PROGRESS: at 31.00% examples, 271358 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:39:32,998] EPOCHs No. 4 - PROGRESS: at 32.00% examples, 269200 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:41:02,530] EPOCHs No. 4 - PROGRESS: at 33.00% examples, 267170 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:42:39,230] EPOCHs No. 4 - PROGRESS: at 34.00% examples, 264998 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:44:14,452] EPOCHs No. 4 - PROGRESS: at 35.00% examples, 263000 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:45:54,986] EPOCHs No. 4 - PROGRESS: at 36.00% examples, 260656 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:47:37,032] EPOCHs No. 4 - PROGRESS: at 37.00% examples, 258327 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 07:49:38,526] EPOCHs No. 4 - PROGRESS: at 38.00% examples, 255043 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:51:15,602] EPOCHs No. 4 - PROGRESS: at 39.00% examples, 253337 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:52:51,882] EPOCHs No. 4 - PROGRESS: at 40.00% examples, 251690 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:54:30,930] EPOCHs No. 4 - PROGRESS: at 41.00% examples, 249944 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:56:25,921] EPOCHs No. 4 - PROGRESS: at 42.00% examples, 247796 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:58:14,003] EPOCHs No. 4 - PROGRESS: at 43.00% examples, 245752 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 07:59:56,981] EPOCHs No. 4 - PROGRESS: at 44.00% examples, 244225 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:01:36,927] EPOCHs No. 4 - PROGRESS: at 45.00% examples, 242755 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:03:15,663] EPOCHs No. 4 - PROGRESS: at 46.00% examples, 241295 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:05:01,380] EPOCHs No. 4 - PROGRESS: at 47.00% examples, 239721 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:06:50,464] EPOCHs No. 4 - PROGRESS: at 48.00% examples, 238047 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:08:31,198] EPOCHs No. 4 - PROGRESS: at 49.00% examples, 236838 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:10:14,896] EPOCHs No. 4 - PROGRESS: at 50.00% examples, 235570 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:11:56,083] EPOCHs No. 4 - PROGRESS: at 51.00% examples, 234360 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:13:32,098] EPOCHs No. 4 - PROGRESS: at 52.00% examples, 233345 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:15:19,013] EPOCHs No. 4 - PROGRESS: at 53.00% examples, 231956 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:17:00,409] EPOCHs No. 4 - PROGRESS: at 54.00% examples, 230792 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:18:52,007] EPOCHs No. 4 - PROGRESS: at 55.00% examples, 229294 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:20:44,762] EPOCHs No. 4 - PROGRESS: at 56.00% examples, 227663 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:22:42,628] EPOCHs No. 4 - PROGRESS: at 57.00% examples, 226367 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:27:43,116] EPOCHs No. 4 - PROGRESS: at 59.00% examples, 221232 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:30:14,279] EPOCHs No. 4 - PROGRESS: at 60.00% examples, 218665 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:32:45,270] EPOCHs No. 4 - PROGRESS: at 61.00% examples, 216311 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:35:12,923] EPOCHs No. 4 - PROGRESS: at 62.00% examples, 214152 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:37:36,740] EPOCHs No. 4 - PROGRESS: at 63.00% examples, 212227 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:39:55,426] EPOCHs No. 4 - PROGRESS: at 64.00% examples, 210398 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:42:16,230] EPOCHs No. 4 - PROGRESS: at 65.00% examples, 208618 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:44:49,604] EPOCHs No. 4 - PROGRESS: at 66.00% examples, 206748 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:47:30,568] EPOCHs No. 4 - PROGRESS: at 67.00% examples, 204648 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:50:01,976] EPOCHs No. 4 - PROGRESS: at 68.00% examples, 203071 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:52:17,067] EPOCHs No. 4 - PROGRESS: at 69.00% examples, 201730 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:54:33,707] EPOCHs No. 4 - PROGRESS: at 70.00% examples, 200450 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:56:39,981] EPOCHs No. 4 - PROGRESS: at 71.00% examples, 199390 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 08:58:55,004] EPOCHs No. 4 - PROGRESS: at 72.00% examples, 198183 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:01:12,803] EPOCHs No. 4 - PROGRESS: at 73.00% examples, 196856 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-27 09:03:23,340] EPOCHs No. 4 - PROGRESS: at 74.00% examples, 195777 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:05:33,450] EPOCHs No. 4 - PROGRESS: at 75.00% examples, 194752 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 09:07:50,112] EPOCHs No. 4 - PROGRESS: at 76.00% examples, 193647 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:09:56,899] EPOCHs No. 4 - PROGRESS: at 77.00% examples, 192572 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:12:06,297] EPOCHs No. 4 - PROGRESS: at 78.00% examples, 191624 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:14:30,416] EPOCHs No. 4 - PROGRESS: at 79.00% examples, 190434 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:16:49,428] EPOCHs No. 4 - PROGRESS: at 80.00% examples, 189271 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:18:55,016] EPOCHs No. 4 - PROGRESS: at 81.00% examples, 188399 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:21:04,294] EPOCHs No. 4 - PROGRESS: at 82.00% examples, 187470 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:23:17,214] EPOCHs No. 4 - PROGRESS: at 83.00% examples, 186569 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:25:25,721] EPOCHs No. 4 - PROGRESS: at 84.00% examples, 185727 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:27:37,230] EPOCHs No. 4 - PROGRESS: at 85.00% examples, 184862 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:30:03,461] EPOCHs No. 4 - PROGRESS: at 86.00% examples, 183784 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:32:18,887] EPOCHs No. 4 - PROGRESS: at 87.00% examples, 182917 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:34:36,100] EPOCHs No. 4 - PROGRESS: at 88.00% examples, 182007 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:37:05,209] EPOCHs No. 4 - PROGRESS: at 89.00% examples, 180994 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:39:44,222] EPOCHs No. 4 - PROGRESS: at 90.00% examples, 179911 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:41:59,334] EPOCHs No. 4 - PROGRESS: at 91.00% examples, 179144 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:44:14,451] EPOCHs No. 4 - PROGRESS: at 92.00% examples, 178407 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:46:37,255] EPOCHs No. 4 - PROGRESS: at 93.00% examples, 177544 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:49:17,458] EPOCHs No. 4 - PROGRESS: at 94.00% examples, 176582 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:51:37,116] EPOCHs No. 4 - PROGRESS: at 95.00% examples, 175836 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:53:51,623] EPOCHs No. 4 - PROGRESS: at 96.00% examples, 175129 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:56:13,263] EPOCHs No. 4 - PROGRESS: at 97.00% examples, 174408 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 09:58:37,493] EPOCHs No. 4 - PROGRESS: at 98.00% examples, 173603 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:00:54,962] EPOCHs No. 4 - PROGRESS: at 99.00% examples, 172823 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:03:00,678] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-27 10:03:02,518] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-27 10:03:02,565] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-27 10:03:02,566] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-27 10:03:02,566] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-27 10:03:02,567] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-27 10:03:02,567] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-27 10:03:02,568] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-27 10:03:02,569] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-27 10:03:02,572] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-27 10:03:02,573] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-27 10:03:02,574] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-27 10:03:02,575] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-27 10:03:02,576] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-27 10:03:02,576] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-27 10:03:02,577] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-27 10:03:02,577] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-27 10:03:02,578] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-27 10:03:02,579] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-27 10:03:02,579] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-27 10:03:02,580] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-27 10:03:02,580] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-27 10:03:02,581] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-27 10:03:02,581] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-27 10:03:02,582] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-27 10:03:02,583] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-27 10:03:02,583] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-27 10:03:02,584] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-27 10:03:02,584] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-27 10:03:02,585] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-27 10:03:02,586] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-27 10:03:02,587] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-27 10:03:02,587] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-27 10:03:02,588] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-27 10:03:02,589] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-27 10:03:02,589] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-27 10:03:02,590] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-27 10:03:02,590] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-27 10:03:02,591] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-27 10:03:02,591] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-27 10:03:02,598] EPOCHs No. 4 - PROGRESS: at 100.00% examples, 172145 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 10:03:02,599] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-27 10:03:02,599] EPOCH - 4 : training on 1993339819 raw words (2077412282 effective words) took 12067.8s, 172145 effective words/s\n",
      "[2022-09-27 10:07:22,485] EPOCHs No. 5 - PROGRESS: at 1.00% examples, 341911 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:09:25,244] EPOCHs No. 5 - PROGRESS: at 2.00% examples, 333147 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:12:18,101] EPOCHs No. 5 - PROGRESS: at 3.00% examples, 333387 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:14:55,076] EPOCHs No. 5 - PROGRESS: at 4.00% examples, 327556 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:17:18,938] EPOCHs No. 5 - PROGRESS: at 5.00% examples, 322087 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:19:50,072] EPOCHs No. 5 - PROGRESS: at 6.00% examples, 312479 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:22:00,772] EPOCHs No. 5 - PROGRESS: at 7.00% examples, 307347 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:24:05,102] EPOCHs No. 5 - PROGRESS: at 8.00% examples, 303266 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:26:09,354] EPOCHs No. 5 - PROGRESS: at 9.00% examples, 298952 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:28:11,052] EPOCHs No. 5 - PROGRESS: at 10.00% examples, 294628 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:30:14,163] EPOCHs No. 5 - PROGRESS: at 11.00% examples, 290379 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:32:12,004] EPOCHs No. 5 - PROGRESS: at 12.00% examples, 286721 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:34:06,383] EPOCHs No. 5 - PROGRESS: at 13.00% examples, 283268 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:35:59,371] EPOCHs No. 5 - PROGRESS: at 14.00% examples, 279663 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:37:51,334] EPOCHs No. 5 - PROGRESS: at 15.00% examples, 276517 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:39:49,816] EPOCHs No. 5 - PROGRESS: at 16.00% examples, 272783 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 10:41:41,298] EPOCHs No. 5 - PROGRESS: at 17.00% examples, 269819 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:43:27,648] EPOCHs No. 5 - PROGRESS: at 18.00% examples, 267504 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:45:19,397] EPOCHs No. 5 - PROGRESS: at 19.00% examples, 264755 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:47:08,439] EPOCHs No. 5 - PROGRESS: at 20.00% examples, 262241 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:48:54,708] EPOCHs No. 5 - PROGRESS: at 21.00% examples, 259840 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:50:42,783] EPOCHs No. 5 - PROGRESS: at 22.00% examples, 257273 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:52:27,447] EPOCHs No. 5 - PROGRESS: at 23.00% examples, 254876 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:54:14,207] EPOCHs No. 5 - PROGRESS: at 24.00% examples, 252587 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:55:55,734] EPOCHs No. 5 - PROGRESS: at 25.00% examples, 250644 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:57:48,546] EPOCHs No. 5 - PROGRESS: at 26.00% examples, 248065 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 10:59:39,468] EPOCHs No. 5 - PROGRESS: at 27.00% examples, 245905 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:01:25,170] EPOCHs No. 5 - PROGRESS: at 28.00% examples, 243949 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:03:13,792] EPOCHs No. 5 - PROGRESS: at 29.00% examples, 241961 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:05:00,878] EPOCHs No. 5 - PROGRESS: at 30.00% examples, 240430 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:06:48,907] EPOCHs No. 5 - PROGRESS: at 31.00% examples, 238519 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:08:38,806] EPOCHs No. 5 - PROGRESS: at 32.00% examples, 236514 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:10:25,199] EPOCHs No. 5 - PROGRESS: at 33.00% examples, 234469 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:12:18,187] EPOCHs No. 5 - PROGRESS: at 34.00% examples, 232406 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:14:06,600] EPOCHs No. 5 - PROGRESS: at 35.00% examples, 230663 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:16:00,412] EPOCHs No. 5 - PROGRESS: at 36.00% examples, 228650 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:17:59,577] EPOCHs No. 5 - PROGRESS: at 37.00% examples, 226464 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:20:19,643] EPOCHs No. 5 - PROGRESS: at 38.00% examples, 223514 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:22:13,675] EPOCHs No. 5 - PROGRESS: at 39.00% examples, 221866 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:24:05,980] EPOCHs No. 5 - PROGRESS: at 40.00% examples, 220317 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:25:58,633] EPOCHs No. 5 - PROGRESS: at 41.00% examples, 218810 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:28:07,768] EPOCHs No. 5 - PROGRESS: at 42.00% examples, 217024 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-27 11:30:08,648] EPOCHs No. 5 - PROGRESS: at 43.00% examples, 215337 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:32:07,459] EPOCHs No. 5 - PROGRESS: at 44.00% examples, 213948 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:34:01,583] EPOCHs No. 5 - PROGRESS: at 45.00% examples, 212659 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:35:59,201] EPOCHs No. 5 - PROGRESS: at 46.00% examples, 211194 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:37:59,692] EPOCHs No. 5 - PROGRESS: at 47.00% examples, 209827 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:40:07,624] EPOCHs No. 5 - PROGRESS: at 48.00% examples, 208243 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:42:04,149] EPOCHs No. 5 - PROGRESS: at 49.00% examples, 207138 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:44:00,245] EPOCHs No. 5 - PROGRESS: at 50.00% examples, 206114 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:45:55,204] EPOCHs No. 5 - PROGRESS: at 51.00% examples, 205078 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:47:45,676] EPOCHs No. 5 - PROGRESS: at 52.00% examples, 204165 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:49:50,187] EPOCHs No. 5 - PROGRESS: at 53.00% examples, 202876 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:51:43,498] EPOCHs No. 5 - PROGRESS: at 54.00% examples, 201940 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:53:58,295] EPOCHs No. 5 - PROGRESS: at 55.00% examples, 200410 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:56:08,217] EPOCHs No. 5 - PROGRESS: at 56.00% examples, 198957 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 11:58:07,619] EPOCHs No. 5 - PROGRESS: at 57.00% examples, 198268 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:00:01,325] EPOCHs No. 5 - PROGRESS: at 58.00% examples, 197609 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:02:05,969] EPOCHs No. 5 - PROGRESS: at 59.00% examples, 196611 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:04:09,790] EPOCHs No. 5 - PROGRESS: at 60.00% examples, 195568 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:06:12,479] EPOCHs No. 5 - PROGRESS: at 61.00% examples, 194670 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:07:58,849] EPOCHs No. 5 - PROGRESS: at 62.00% examples, 194210 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:09:50,848] EPOCHs No. 5 - PROGRESS: at 63.00% examples, 193643 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:11:38,900] EPOCHs No. 5 - PROGRESS: at 64.00% examples, 193067 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:13:31,271] EPOCHs No. 5 - PROGRESS: at 65.00% examples, 192439 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:15:29,608] EPOCHs No. 5 - PROGRESS: at 66.00% examples, 191863 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:17:25,332] EPOCHs No. 5 - PROGRESS: at 67.00% examples, 191275 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:19:26,565] EPOCHs No. 5 - PROGRESS: at 68.00% examples, 190746 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:21:14,977] EPOCHs No. 5 - PROGRESS: at 69.00% examples, 190295 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:23:06,704] EPOCHs No. 5 - PROGRESS: at 70.00% examples, 189833 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:24:51,632] EPOCHs No. 5 - PROGRESS: at 71.00% examples, 189460 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:26:38,659] EPOCHs No. 5 - PROGRESS: at 72.00% examples, 189079 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:28:34,500] EPOCHs No. 5 - PROGRESS: at 73.00% examples, 188428 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:30:20,528] EPOCHs No. 5 - PROGRESS: at 74.00% examples, 188039 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:32:02,305] EPOCHs No. 5 - PROGRESS: at 75.00% examples, 187760 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:33:52,186] EPOCHs No. 5 - PROGRESS: at 76.00% examples, 187352 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:35:32,436] EPOCHs No. 5 - PROGRESS: at 77.00% examples, 186939 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:37:16,379] EPOCHs No. 5 - PROGRESS: at 78.00% examples, 186609 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:39:08,775] EPOCHs No. 5 - PROGRESS: at 79.00% examples, 186155 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:41:01,283] EPOCHs No. 5 - PROGRESS: at 80.00% examples, 185597 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:42:45,233] EPOCHs No. 5 - PROGRESS: at 81.00% examples, 185208 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:44:32,635] EPOCHs No. 5 - PROGRESS: at 82.00% examples, 184752 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:46:23,954] EPOCHs No. 5 - PROGRESS: at 83.00% examples, 184306 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:48:19,580] EPOCHs No. 5 - PROGRESS: at 84.00% examples, 183743 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:50:10,047] EPOCHs No. 5 - PROGRESS: at 85.00% examples, 183296 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:52:11,356] EPOCHs No. 5 - PROGRESS: at 86.00% examples, 182697 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:54:03,017] EPOCHs No. 5 - PROGRESS: at 87.00% examples, 182271 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:55:56,157] EPOCHs No. 5 - PROGRESS: at 88.00% examples, 181793 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 12:57:50,403] EPOCHs No. 5 - PROGRESS: at 89.00% examples, 181386 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:00:02,599] EPOCHs No. 5 - PROGRESS: at 90.00% examples, 180750 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 13:01:57,967] EPOCHs No. 5 - PROGRESS: at 91.00% examples, 180299 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:03:54,376] EPOCHs No. 5 - PROGRESS: at 92.00% examples, 179853 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:05:50,533] EPOCHs No. 5 - PROGRESS: at 93.00% examples, 179399 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:08:06,681] EPOCHs No. 5 - PROGRESS: at 94.00% examples, 178787 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:10:04,547] EPOCHs No. 5 - PROGRESS: at 95.00% examples, 178351 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:12:04,795] EPOCHs No. 5 - PROGRESS: at 96.00% examples, 177826 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:14:03,040] EPOCHs No. 5 - PROGRESS: at 97.00% examples, 177423 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:16:02,673] EPOCHs No. 5 - PROGRESS: at 98.00% examples, 176942 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:17:58,963] EPOCHs No. 5 - PROGRESS: at 99.00% examples, 176427 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-27 13:19:46,190] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-27 13:19:47,475] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-27 13:19:47,521] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-27 13:19:47,521] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-27 13:19:47,522] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-27 13:19:47,522] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-27 13:19:47,523] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-27 13:19:47,523] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-27 13:19:47,524] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-27 13:19:47,524] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-27 13:19:47,525] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-27 13:19:47,525] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-27 13:19:47,525] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-27 13:19:47,526] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-27 13:19:47,526] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-27 13:19:47,527] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-27 13:19:47,527] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-27 13:19:47,528] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-27 13:19:47,528] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-27 13:19:47,528] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-27 13:19:47,529] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-27 13:19:47,529] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-27 13:19:47,530] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-27 13:19:47,530] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-27 13:19:47,531] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-27 13:19:47,531] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-27 13:19:47,531] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-27 13:19:47,532] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-27 13:19:47,532] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-27 13:19:47,533] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-27 13:19:47,533] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-27 13:19:47,534] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-27 13:19:47,534] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-27 13:19:47,534] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-27 13:19:47,535] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-27 13:19:47,535] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-27 13:19:47,536] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-27 13:19:47,536] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-27 13:19:47,537] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-27 13:19:47,537] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-27 13:19:47,552] EPOCHs No. 5 - PROGRESS: at 100.00% examples, 175980 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-27 13:19:47,553] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-27 13:19:47,553] EPOCH - 5 : training on 1993339819 raw words (2077412282 effective words) took 11804.8s, 175980 effective words/s\n",
      "[2022-09-27 13:19:47,554] training on a 9966699095 raw words (10387061410 effective words) took 57092.2s, 181935 effective words/s\n",
      "[2022-09-27 13:19:47,555] Training done.\n"
     ]
    }
   ],
   "source": [
    "logging.info('Training model %s', 'wordnet')\n",
    "model = word2vec.Word2Vec(sentences, window=10, sg=1, hs=0, negative=5, size=300, workers=40, iter=5)\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:19:47.567151Z",
     "start_time": "2022-09-27T12:19:47.561283Z"
    }
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_filtered_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc1_epoch5_300_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2R_mc1_epoch5_300_filtered.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2w2v_mc1_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_con1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2S_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2B_mc100_epoch5_300_sub3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2TB_mc100_epoch5_300_LR.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2POS_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2DEP_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LRM3_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LOC_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_loc.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_pos.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_ent_w10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_dep2_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_ent_static_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_unsem.txt'\n",
    "#emb_file = '/home/manni/embs/en_wiki_spx_mc100_epoch5_300_em.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns_w1.txt'\n",
    "#emb_file = '/home/manni/embs/en_wiki_wnet_epoch5_300_w10.txt'\n",
    "emb_file = '/home/manni/embs/en_wiki_wnet_epoch5_300_w10_exclusive_noun.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:19:47.611756Z",
     "start_time": "2022-09-27T12:19:47.568626Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab #dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:19:47.638459Z",
     "start_time": "2022-09-27T12:19:47.613597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2673717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:19:47.655143Z",
     "start_time": "2022-09-27T12:19:47.641660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2673717"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.pop('[', None)\n",
    "vocab.pop(']', None)\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T12:19:49.949463Z",
     "start_time": "2022-09-27T12:19:47.656777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2602916"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_vocab = list(vocab.keys())\n",
    "vocab = set()\n",
    "for word in _vocab:\n",
    "    if '§' in word:\n",
    "        continue\n",
    "    vocab.add(word)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T14:46:01.175944Z",
     "start_time": "2022-09-27T14:28:38.594791Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 15:28:38,598] Save trained word vectors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2602916/2602916 [17:20<00:00, 2501.16it/s]\n",
      "[2022-09-27 15:46:01,173] Done\n"
     ]
    }
   ],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab, position=0):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
