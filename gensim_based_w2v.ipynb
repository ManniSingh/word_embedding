{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:17:34.426346Z",
     "start_time": "2022-09-19T03:17:33.041134Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "#import wiki_old as w # old wiki\n",
    "import wiki as w \n",
    " \n",
    "#from gensim.models import word2vec # for orignal w2v\n",
    "from localgensim.gensim2.models import word2vec \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models.fasttext import FastText\n",
    "#from gensim.models.word2vec import Word2Vec # not in use\n",
    "#from localgensim.gensim2.models.word2vec import Word2Vec # not in use\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream.xml.bz2'\n",
    "#WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream1.xml-p1p41242.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T05:53:32.483163Z",
     "start_time": "2022-03-23T05:53:30.879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manni/ner-s2s/word_embedding/wiki.py\n",
      "/home/manni/ner-s2s/word_embedding/localgensim/gensim2/models/word2vec.py\n"
     ]
    }
   ],
   "source": [
    "print(w.__file__)\n",
    "print(word2vec.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T05:53:32.484929Z",
     "start_time": "2022-03-23T05:53:31.194Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:17:37.198453Z",
     "start_time": "2022-09-19T03:17:37.194710Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO)\n",
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:25:05.774307Z",
     "start_time": "2020-10-25T13:25:05.608721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Training model %s', 'word2vec')\n",
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=10, size=300, sample=0, \n",
    "                 workers=1, iter=1, min_count=1)\n",
    "\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:48:45.834994Z",
     "start_time": "2020-10-25T13:47:14.780675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conll corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"conll_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:52:12.732111Z",
     "start_time": "2020-10-25T13:48:45.838043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=1, iter=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_spx2g.txt'\n",
    "emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip if sentence made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:32.868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:06:58,180] Parsing wiki corpus Altered...\n",
      "[2022-09-14 18:07:07,165] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "[2022-09-14 18:08:15,143] adding document #10000 to Dictionary(493671 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:09:16,028] adding document #20000 to Dictionary(699785 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:10:07,568] adding document #30000 to Dictionary(854217 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:10:52,879] adding document #40000 to Dictionary(985354 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:13:47,022] adding document #100000 to Dictionary(1329619 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:14:25,475] adding document #110000 to Dictionary(1420571 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:15:02,407] adding document #120000 to Dictionary(1510844 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:15:39,149] adding document #130000 to Dictionary(1582757 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:16:18,251] adding document #140000 to Dictionary(1672042 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:16:51,304] adding document #150000 to Dictionary(1757766 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:17:27,857] adding document #160000 to Dictionary(1839623 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:18:01,937] adding document #170000 to Dictionary(1907760 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:18:33,403] adding document #180000 to Dictionary(1968243 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:19:06,002] discarding 26793 tokens: [('minershagen', 1), ('mogidh', 1), ('moosen', 1), ('naeej', 1), ('naij', 1), ('nasfaah', 1), ('nashmi', 1), ('neurada', 1), ('nissah', 1), ('nuwaiser', 1)]...\n",
      "[2022-09-14 18:19:06,004] keeping 2000000 tokens which were in no less than 0 and no more than 190000 (=100.0%) documents\n",
      "[2022-09-14 18:19:09,335] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:19:09,363] adding document #190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:19:44,072] discarding 63075 tokens: [('prozeßorientierung', 1), ('rechtsschutzversicherungsgesellschaften', 1), ('sauerstoffflasche', 1), ('schaffell', 1), ('tiell', 1), ('tiär', 1), ('wachstube', 1), ('wachſtube', 1), ('xylofon', 1), ('zsch', 1)]...\n",
      "[2022-09-14 18:19:44,074] keeping 2000000 tokens which were in no less than 0 and no more than 200000 (=100.0%) documents\n",
      "[2022-09-14 18:19:48,231] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:19:48,291] adding document #200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:20:21,888] discarding 53688 tokens: [('rifco', 1), ('unpfa', 1), ('socomms', 1), ('mclorinan', 1), ('stegley', 1), ('stitfold', 1), ('warrain', 1), ('mahaoya', 1), ('banyool', 1), ('dimarelos', 1)]...\n",
      "[2022-09-14 18:20:21,889] keeping 2000000 tokens which were in no less than 0 and no more than 210000 (=100.0%) documents\n",
      "[2022-09-14 18:20:25,564] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:20:25,622] adding document #210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:20:59,142] discarding 57791 tokens: [('tlesey', 1), ('tomreed', 1), ('tomsuozzi', 1), ('townsendscudder', 1), ('tredwellscudder', 1), ('trippross', 1), ('unin', 1), ('valentineefner', 1), ('vanaernam', 1), ('vancortlandt', 1)]...\n",
      "[2022-09-14 18:20:59,144] keeping 2000000 tokens which were in no less than 0 and no more than 220000 (=100.0%) documents\n",
      "[2022-09-14 18:21:03,510] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:21:03,570] adding document #220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:21:34,147] discarding 58149 tokens: [('frasinei', 1), ('frassinetto', 1), ('frema', 1), ('fromâjo', 1), ('frâre', 1), ('fuelha', 1), ('fénisan', 1), ('fònt', 1), ('fòye', 1), ('fôlye', 1)]...\n",
      "[2022-09-14 18:21:34,148] keeping 2000000 tokens which were in no less than 0 and no more than 230000 (=100.0%) documents\n",
      "[2022-09-14 18:21:37,116] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:21:37,158] adding document #230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:22:10,506] discarding 51334 tokens: [('ropthe', 1), ('vitreoretinopathy', 1), ('zonei', 1), ('zoneii', 1), ('zoneiii', 1), ('contée', 1), ('mcelvie', 1), ('mensington', 1), ('sledmer', 1), ('mindjack', 1)]...\n",
      "[2022-09-14 18:22:10,510] keeping 2000000 tokens which were in no less than 0 and no more than 240000 (=100.0%) documents\n",
      "[2022-09-14 18:22:14,877] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:22:14,936] adding document #240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:22:48,343] discarding 56238 tokens: [('proskovice', 1), ('pustkovec', 1), ('přívoz', 1), ('radvanice', 1), ('ristanovski', 1), ('ropice', 1), ('rudolfshütte', 1), ('rychvald', 1), ('sareza', 1), ('svinov', 1)]...\n",
      "[2022-09-14 18:22:48,345] keeping 2000000 tokens which were in no less than 0 and no more than 250000 (=100.0%) documents\n",
      "[2022-09-14 18:22:53,030] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:22:53,091] adding document #250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:23:25,279] discarding 51645 tokens: [('franciscensis', 1), ('franciscodoras', 1), ('hypsolebias', 1), ('ibotirama', 1), ('jequitaí', 1), ('kaleshí', 1), ('opára', 1), ('substretches', 1), ('truká', 1), ('urucuia', 1)]...\n",
      "[2022-09-14 18:23:25,281] keeping 2000000 tokens which were in no less than 0 and no more than 260000 (=100.0%) documents\n",
      "[2022-09-14 18:23:29,126] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:23:29,187] adding document #260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:24:08,895] discarding 56594 tokens: [('gewachsen', 1), ('halcke', 1), ('heinscheidt', 1), ('kräutern', 1), ('sträuchen', 1), ('umständlich', 1), ('vegetabilien', 1), ('brittaniae', 1), ('papilionum', 1), ('arduchy', 1)]...\n",
      "[2022-09-14 18:24:08,897] keeping 2000000 tokens which were in no less than 0 and no more than 270000 (=100.0%) documents\n",
      "[2022-09-14 18:24:11,907] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:24:11,950] adding document #270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:24:41,771] discarding 54215 tokens: [('stagnon', 1), ('teobald', 1), ('travunian', 1), ('trpimirovic', 1), ('večenik', 1), ('vlatkovići', 1), ('xŭlmŭ', 1), ('zachloumoi', 1), ('zachlumi', 1), ('zachlumians', 1)]...\n",
      "[2022-09-14 18:24:41,773] keeping 2000000 tokens which were in no less than 0 and no more than 280000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:24:46,096] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:24:46,158] adding document #280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:25:16,583] discarding 60294 tokens: [('yastur', 1), ('megaweapon', 1), ('skedars', 1), ('chernen', 1), ('eggenweiler', 1), ('orcthira', 1), ('buouyancy', 1), ('overtiredness', 1), ('scubber', 1), ('liberalhistory', 1)]...\n",
      "[2022-09-14 18:25:16,584] keeping 2000000 tokens which were in no less than 0 and no more than 290000 (=100.0%) documents\n",
      "[2022-09-14 18:25:20,928] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:25:20,992] adding document #290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:25:50,358] discarding 56919 tokens: [('茂才', 1), ('diffamé', 1), ('entreateth', 1), ('rebatement', 1), ('sagerlund', 1), ('soveraignes', 1), ('flag_of_the_trucial_states', 1), ('standard_of_the_sultan_of_oman', 1), ('exension', 1), ('indnow', 1)]...\n",
      "[2022-09-14 18:25:50,360] keeping 2000000 tokens which were in no less than 0 and no more than 300000 (=100.0%) documents\n",
      "[2022-09-14 18:25:53,548] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:25:53,593] adding document #300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:26:24,699] discarding 58993 tokens: [('laplantine', 1), ('momusmcclymont', 1), ('pillycock', 1), ('popppappp', 1), ('shibuyakei', 1), ('sunbutler', 1), ('thunderclown', 1), ('ultraconformist', 1), ('unamerica', 1), ('boolangga', 1)]...\n",
      "[2022-09-14 18:26:24,701] keeping 2000000 tokens which were in no less than 0 and no more than 310000 (=100.0%) documents\n",
      "[2022-09-14 18:26:27,727] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:26:27,773] adding document #310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:26:56,790] discarding 50018 tokens: [('مروز', 1), ('نيمروز', 1), ('bibliografiya', 1), ('dishani', 1), ('gabars', 1), ('gisht', 1), ('kukhtina', 1), ('literatuyra', 1), ('nangalam', 1), ('nangarej', 1)]...\n",
      "[2022-09-14 18:26:56,791] keeping 2000000 tokens which were in no less than 0 and no more than 320000 (=100.0%) documents\n",
      "[2022-09-14 18:27:01,054] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:27:01,117] adding document #320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:27:28,546] discarding 46873 tokens: [('samandarin', 1), ('breamfox', 1), ('lucom', 1), ('shannonbream', 1), ('zgcac', 1), ('acquaintant', 1), ('aldilia', 1), ('philoparthens', 1), ('thealma', 1), ('eromena', 1)]...\n",
      "[2022-09-14 18:27:28,547] keeping 2000000 tokens which were in no less than 0 and no more than 330000 (=100.0%) documents\n",
      "[2022-09-14 18:27:32,874] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:27:32,945] adding document #330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:28:01,781] discarding 43829 tokens: [('palmencrona', 1), ('palmhielm', 1), ('palmkron', 1), ('palmsköld', 1), ('palmsvärd', 1), ('palén', 1), ('pantzarhielm', 1), ('pantzerhielm', 1), ('parmand', 1), ('pegauberg', 1)]...\n",
      "[2022-09-14 18:28:01,783] keeping 2000000 tokens which were in no less than 0 and no more than 340000 (=100.0%) documents\n",
      "[2022-09-14 18:28:04,886] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:28:04,934] adding document #340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:28:32,561] discarding 43078 tokens: [('aristogeíton', 1), ('convivalia', 1), ('eleutherioi', 1), ('harmódios', 1), ('leæna', 1), ('panathenaean', 1), ('proedria', 1), ('sitesis', 1), ('tyrannoktonoi', 1), ('tyrannophonoi', 1)]...\n",
      "[2022-09-14 18:28:32,562] keeping 2000000 tokens which were in no less than 0 and no more than 350000 (=100.0%) documents\n",
      "[2022-09-14 18:28:35,753] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:28:35,803] adding document #350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:29:04,970] discarding 43887 tokens: [('nekipelova', 1), ('pokéball', 1), ('poladko', 1), ('ポケモンgetだぜ', 1), ('newchang', 1), ('bamigboye', 1), ('buturlina', 1), ('cezon', 1), ('ichizawa', 1), ('inubito', 1)]...\n",
      "[2022-09-14 18:29:04,972] keeping 2000000 tokens which were in no less than 0 and no more than 360000 (=100.0%) documents\n",
      "[2022-09-14 18:29:09,375] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:29:09,439] adding document #360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:29:38,332] discarding 48282 tokens: [('inlinks', 1), ('ntoulas', 1), ('zerfos', 1), ('smite#1', 1), ('marchesino', 1), ('toffieha', 1), ('sæterdal', 1), ('aeronef', 1), ('chandes', 1), ('cracs', 1)]...\n",
      "[2022-09-14 18:29:38,334] keeping 2000000 tokens which were in no less than 0 and no more than 370000 (=100.0%) documents\n",
      "[2022-09-14 18:29:42,685] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:29:42,750] adding document #370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:30:13,456] discarding 42494 tokens: [('serelo', 1), ('sosoh', 1), ('sujaimana', 1), ('trambralinga', 1), ('bartlettepisode', 1), ('songepisode', 1), ('arrophoria', 1), ('baabullah', 1), ('baguna', 1), ('bobato', 1)]...\n",
      "[2022-09-14 18:30:13,458] keeping 2000000 tokens which were in no less than 0 and no more than 380000 (=100.0%) documents\n",
      "[2022-09-14 18:30:17,818] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:30:17,883] adding document #380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:30:45,418] discarding 44870 tokens: [('庵原郡', 1), ('有渡郡', 1), ('益津郡', 1), ('駿東郡', 1), ('bansongyuan', 1), ('bànsōngyuánlù', 1), ('dapuqiao', 1), ('dashijie', 1), ('dongjiadu', 1), ('dǎpǔqiáo', 1)]...\n",
      "[2022-09-14 18:30:45,419] keeping 2000000 tokens which were in no less than 0 and no more than 390000 (=100.0%) documents\n",
      "[2022-09-14 18:30:50,137] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:30:50,204] adding document #390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:31:18,467] discarding 50462 tokens: [('skubarschewo', 1), ('soßnitz', 1), ('starygaj', 1), ('stawiska', 1), ('strzyzewo', 1), ('strzyżewo', 1), ('sucharzewo', 1), ('suszewo', 1), ('swiente', 1), ('swierkowiec', 1)]...\n",
      "[2022-09-14 18:31:18,469] keeping 2000000 tokens which were in no less than 0 and no more than 400000 (=100.0%) documents\n",
      "[2022-09-14 18:31:22,183] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:31:22,234] adding document #400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:31:49,453] discarding 49798 tokens: [('lissagora', 1), ('liszkow', 1), ('louisenstein', 1), ('magielka', 1), ('malgow', 1), ('margarethendorf', 1), ('marianow', 1), ('mycielin', 1), ('mühlenabbau', 1), ('nepomucenowo', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:31:49,455] keeping 2000000 tokens which were in no less than 0 and no more than 410000 (=100.0%) documents\n",
      "[2022-09-14 18:31:53,799] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:31:53,867] adding document #410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:32:21,550] discarding 44186 tokens: [('horsgeat', 1), ('hàslam', 1), ('inshaig', 1), ('inverchaolain', 1), ('invercreran', 1), ('kilcalmonell', 1), ('kilmanshenachan', 1), ('kilmorich', 1), ('kinlocheil', 1), ('knipoch', 1)]...\n",
      "[2022-09-14 18:32:21,552] keeping 2000000 tokens which were in no less than 0 and no more than 420000 (=100.0%) documents\n",
      "[2022-09-14 18:32:25,028] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:32:25,094] adding document #420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:32:54,231] discarding 48722 tokens: [('soundvoltex', 1), ('surotore', 1), ('tatakisugi', 1), ('tekunobībī', 1), ('tengurigaeshi', 1), ('tentochito', 1), ('teraburst', 1), ('tomena', 1), ('tomenasanna', 1), ('tsurikko', 1)]...\n",
      "[2022-09-14 18:32:54,233] keeping 2000000 tokens which were in no less than 0 and no more than 430000 (=100.0%) documents\n",
      "[2022-09-14 18:32:58,581] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:32:58,649] adding document #430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:33:26,460] discarding 45834 tokens: [('aliortus', 1), ('aliotus', 1), ('aliotvs', 1), ('elmed', 1), ('elmetiacos', 1), ('elmetian', 1), ('elmetsæte', 1), ('lleenog', 1), ('damvilliers', 1), ('barkantine', 1)]...\n",
      "[2022-09-14 18:33:26,462] keeping 2000000 tokens which were in no less than 0 and no more than 440000 (=100.0%) documents\n",
      "[2022-09-14 18:33:29,532] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:33:29,579] adding document #440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:33:57,267] discarding 48998 tokens: [('szatoki', 1), ('szatoky', 1), ('szeleslevelue', 1), ('szemendriai', 1), ('szemendrianer', 1), ('szeplős', 1), ('szevernuej', 1), ('szilvani', 1), ('szod', 1), ('szoeloe', 1)]...\n",
      "[2022-09-14 18:33:57,269] keeping 2000000 tokens which were in no less than 0 and no more than 450000 (=100.0%) documents\n",
      "[2022-09-14 18:34:00,347] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:34:00,395] adding document #450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:34:26,641] discarding 38779 tokens: [('parchitana', 1), ('parmac', 1), ('parralada', 1), ('parreleta', 1), ('parsaryaska', 1), ('parč', 1), ('pasareasca', 1), ('pasarjaska', 1), ('pascaou', 1), ('pascolu', 1)]...\n",
      "[2022-09-14 18:34:26,642] keeping 2000000 tokens which were in no less than 0 and no more than 460000 (=100.0%) documents\n",
      "[2022-09-14 18:34:29,909] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:34:29,961] adding document #460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:34:56,478] discarding 47286 tokens: [('cappuciu', 1), ('caprettone', 1), ('caprone', 1), ('carajolo', 1), ('carbenet', 1), ('carbesse', 1), ('carbesso', 1), ('carbonet', 1), ('carbouet', 1), ('carcaghjoliu', 1)]...\n",
      "[2022-09-14 18:34:56,480] keeping 2000000 tokens which were in no less than 0 and no more than 470000 (=100.0%) documents\n",
      "[2022-09-14 18:35:00,943] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:35:01,012] adding document #470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:35:27,597] discarding 42509 tokens: [('kwvp', 1), ('hinzengberg', 1), ('aletheiologie', 1), ('aletheiologische', 1), ('aletheiologischen', 1), ('aletheologian', 1), ('alethiologian', 1), ('alethologian', 1), ('alethology', 1), ('begiashvili', 1)]...\n",
      "[2022-09-14 18:35:27,599] keeping 2000000 tokens which were in no less than 0 and no more than 480000 (=100.0%) documents\n",
      "[2022-09-14 18:35:31,974] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:35:32,042] adding document #480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:35:59,847] discarding 45867 tokens: [('schenfeld', 1), ('thoughtscape', 1), ('tomwhore', 1), ('voxchat', 1), ('voxmeats', 1), ('domewood', 1), ('fickleshole', 1), ('bractella', 1), ('eymore', 1), ('hwicca', 1)]...\n",
      "[2022-09-14 18:35:59,849] keeping 2000000 tokens which were in no less than 0 and no more than 490000 (=100.0%) documents\n",
      "[2022-09-14 18:36:04,316] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:36:04,386] adding document #490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:36:33,916] discarding 45042 tokens: [('felkerzam', 1), ('tiayuan', 1), ('afghanistan¹', 1), ('fcdd', 1), ('periodalong', 1), ('gajasingha', 1), ('krŏng', 1), ('canacassala', 1), ('foguetão', 1), ('ibendoa', 1)]...\n",
      "[2022-09-14 18:36:33,918] keeping 2000000 tokens which were in no less than 0 and no more than 500000 (=100.0%) documents\n",
      "[2022-09-14 18:36:38,344] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:36:38,415] adding document #500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:37:04,819] discarding 44903 tokens: [('affghanica', 1), ('altoportillensis', 1), ('anabaptizata', 1), ('aprutia', 1), ('arbusensis', 1), ('arizagae', 1), ('artwinensis', 1), ('ausetana', 1), ('bakeria', 1), ('benehoavensis', 1)]...\n",
      "[2022-09-14 18:37:04,821] keeping 2000000 tokens which were in no less than 0 and no more than 510000 (=100.0%) documents\n",
      "[2022-09-14 18:37:09,234] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:37:09,304] adding document #510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:37:37,210] discarding 42061 tokens: [('ghiaccioli', 1), ('piermattei', 1), ('pèrle', 1), ('vondish', 1), ('göns', 1), ('dukathole', 1), ('dukatole', 1), ('jouberton', 1), ('lokasie', 1), ('lokasies', 1)]...\n",
      "[2022-09-14 18:37:37,211] keeping 2000000 tokens which were in no less than 0 and no more than 520000 (=100.0%) documents\n",
      "[2022-09-14 18:37:41,651] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:37:41,720] adding document #520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:38:07,812] discarding 48295 tokens: [('rognvaldsson', 1), ('roskelyn', 1), ('schipkaensis', 1), ('zabeliana', 1), ('coklapop', 1), ('hallagan', 1), ('izdolshchina', 1), ('masoveria', 1), ('mediero', 1), ('muzara', 1)]...\n",
      "[2022-09-14 18:38:07,813] keeping 2000000 tokens which were in no less than 0 and no more than 530000 (=100.0%) documents\n",
      "[2022-09-14 18:38:12,562] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:38:12,634] adding document #530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:38:39,049] discarding 42178 tokens: [('замкавая', 1), ('кавальская', 1), ('карэліцкая', 1), ('мугір', 1), ('мячэтавая', 1), ('міцкевіч', 1), ('наваградак', 1), ('наваградзкая', 1), ('наваградзкі', 1), ('паліклініка', 1)]...\n",
      "[2022-09-14 18:38:39,050] keeping 2000000 tokens which were in no less than 0 and no more than 540000 (=100.0%) documents\n",
      "[2022-09-14 18:38:42,185] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:38:42,235] adding document #540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:39:07,493] discarding 43614 tokens: [('hendratmo', 1), ('jakákoliv', 1), ('kirkaryan', 1), ('meghedin', 1), ('melodiyu', 1), ('ntifi', 1), ('tiškevič', 1), ('ugadaika', 1), ('ugaday', 1), ('ugadayka', 1)]...\n",
      "[2022-09-14 18:39:07,494] keeping 2000000 tokens which were in no less than 0 and no more than 550000 (=100.0%) documents\n",
      "[2022-09-14 18:39:12,284] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:39:12,356] adding document #550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:39:37,323] discarding 39726 tokens: [('hersmendorff', 1), ('juanar', 1), ('lastonar', 1), ('marbellamar', 1), ('marbellan', 1), ('marbellenses', 1), ('marbelleros', 1), ('marbellíes', 1), ('marbellís', 1), ('mastieno', 1)]...\n",
      "[2022-09-14 18:39:37,324] keeping 2000000 tokens which were in no less than 0 and no more than 560000 (=100.0%) documents\n",
      "[2022-09-14 18:39:40,434] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:39:40,484] adding document #560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:40:06,134] discarding 37362 tokens: [('ˈɑnədʒər', 1), ('kesineni', 1), ('kausadikar', 1), ('tribunican', 1), ('udyachi', 1), ('bëtowsczi', 1), ('cassubiae', 1), ('cewice', 1), ('choczewo', 1), ('chòczewò', 1)]...\n",
      "[2022-09-14 18:40:06,136] keeping 2000000 tokens which were in no less than 0 and no more than 570000 (=100.0%) documents\n",
      "[2022-09-14 18:40:10,893] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:40:10,964] adding document #570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:40:38,325] discarding 44524 tokens: [('bindingpl', 1), ('bomss', 1), ('bopt', 1), ('byzmss', 1), ('castpl', 1), ('coboms', 1), ('copboms', 1), ('cosams', 1), ('cosamss', 1), ('diatessaronmss', 1)]...\n",
      "[2022-09-14 18:40:38,326] keeping 2000000 tokens which were in no less than 0 and no more than 580000 (=100.0%) documents\n",
      "[2022-09-14 18:40:42,946] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:40:43,020] adding document #580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:41:08,541] discarding 42928 tokens: [('schluckingen', 1), ('schmerlacke', 1), ('serlinghausen', 1), ('stanchick', 1), ('stroebeck', 1), ('tetelrath', 1), ('vierhausen', 1), ('vinnbruck', 1), ('vollinghausen', 1), ('wiehagen', 1)]...\n",
      "[2022-09-14 18:41:08,543] keeping 2000000 tokens which were in no less than 0 and no more than 590000 (=100.0%) documents\n",
      "[2022-09-14 18:41:13,335] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:41:13,408] adding document #590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:41:39,843] discarding 42200 tokens: [('cxhpu', 1), ('dgie', 1), ('dgoe', 1), ('dxhku', 1), ('gkohu', 1), ('glhku', 1), ('ksflp', 1), ('kswkt', 1), ('kxohu', 1), ('leohu', 1)]...\n",
      "[2022-09-14 18:41:39,844] keeping 2000000 tokens which were in no less than 0 and no more than 600000 (=100.0%) documents\n",
      "[2022-09-14 18:41:43,535] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:41:43,609] adding document #600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:42:08,102] discarding 40360 tokens: [('imgproc', 1), ('khorogh', 1), ('köhistoni', 1), ('nivodak', 1), ('tirdâd', 1), ('xoroq', 1), ('khatlān', 1), ('surayyo', 1), ('zarbdor', 1), ('kurganteppa', 1)]...\n",
      "[2022-09-14 18:42:08,106] keeping 2000000 tokens which were in no less than 0 and no more than 610000 (=100.0%) documents\n",
      "[2022-09-14 18:42:12,638] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:42:12,710] adding document #610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:42:38,443] discarding 42308 tokens: [('smededon', 1), ('smeedon', 1), ('smethden', 1), ('smethdon', 1), ('smethedon', 1), ('smethesdune', 1), ('smolte', 1), ('stainulf', 1), ('stochestede', 1), ('voelas', 1)]...\n",
      "[2022-09-14 18:42:38,445] keeping 2000000 tokens which were in no less than 0 and no more than 620000 (=100.0%) documents\n",
      "[2022-09-14 18:42:42,930] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:42:43,003] adding document #620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:43:09,856] discarding 41215 tokens: [('passme', 1), ('waliou', 1), ('popynjay', 1), ('hajila', 1), ('musliya', 1), ('nutayla', 1), ('tukana', 1), ('الحارث', 1), ('لبابة', 1), ('abgadam', 1)]...\n",
      "[2022-09-14 18:43:09,858] keeping 2000000 tokens which were in no less than 0 and no more than 630000 (=100.0%) documents\n",
      "[2022-09-14 18:43:12,977] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:43:13,027] adding document #630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:43:39,204] discarding 38974 tokens: [('mupeto', 1), ('kuparissos', 1), ('linguaefolia', 1), ('authoritism', 1), ('eriksons', 1), ('kivnick', 1), ('lotussee', 1), ('吳敏', 1), ('吴怀中', 1), ('吴敏', 1)]...\n",
      "[2022-09-14 18:43:39,206] keeping 2000000 tokens which were in no less than 0 and no more than 640000 (=100.0%) documents\n",
      "[2022-09-14 18:43:43,767] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:43:43,840] adding document #640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:44:07,420] discarding 50190 tokens: [('avmaktslave', 1), ('fortapelse', 1), ('sorgens', 1), ('svunnet', 1), ('fluers', 1), ('plissés', 1), ('seasted', 1), ('sursonyachtletoile', 1), ('counselers', 1), ('diseños', 1)]...\n",
      "[2022-09-14 18:44:07,422] keeping 2000000 tokens which were in no less than 0 and no more than 650000 (=100.0%) documents\n",
      "[2022-09-14 18:44:12,032] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:44:12,107] adding document #650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:44:36,595] discarding 38702 tokens: [('maymorning', 1), ('nawbags', 1), ('awardumd', 1), ('chumola', 1), ('paulwbmarsden', 1), ('gilhespy', 1), ('kleanthos', 1), ('thenstead', 1), ('aqah', 1), ('asfeeni', 1)]...\n",
      "[2022-09-14 18:44:36,597] keeping 2000000 tokens which were in no less than 0 and no more than 660000 (=100.0%) documents\n",
      "[2022-09-14 18:44:39,827] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:44:39,881] adding document #660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:45:04,843] discarding 37686 tokens: [('balmant', 1), ('equidirectional', 1), ('iwanuch', 1), ('kangerdluarssaq', 1), ('litchfieldite', 1), ('rembrandt_man_in_armour', 1), ('rosencreütz', 1), ('check_something', 1), ('oddingseles', 1), ('privman', 1)]...\n",
      "[2022-09-14 18:45:04,845] keeping 2000000 tokens which were in no less than 0 and no more than 670000 (=100.0%) documents\n",
      "[2022-09-14 18:45:09,453] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:45:09,527] adding document #670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:45:33,675] discarding 37567 tokens: [('resoul', 1), ('אלקוש', 1), ('hikutavake', 1), ('namukulu', 1), ('ethylphenethylamine', 1), ('etylfenetylamin', 1), ('webtryp', 1), ('tallyband', 1), ('allüberall', 1), ('aufblüh', 1)]...\n",
      "[2022-09-14 18:45:33,677] keeping 2000000 tokens which were in no less than 0 and no more than 680000 (=100.0%) documents\n",
      "[2022-09-14 18:45:38,675] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:45:38,749] adding document #680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:46:04,051] discarding 57334 tokens: [('ardenghesca', 1), ('nobilta', 1), ('fasco', 1), ('turone', 1), ('dėpartement', 1), ('impedit', 1), ('wasserhövel', 1), ('asmis', 1), ('canonic#3', 1), ('informanda', 1)]...\n",
      "[2022-09-14 18:46:04,053] keeping 2000000 tokens which were in no less than 0 and no more than 690000 (=100.0%) documents\n",
      "[2022-09-14 18:46:08,589] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:46:08,670] adding document #690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:46:35,421] discarding 39021 tokens: [('ancestorof', 1), ('baldspotted', 1), ('bauscat', 1), ('beloresavec', 1), ('bijeloopaljeni', 1), ('castorrex', 1), ('deilenaar', 1), ('dværgschecke', 1), ('eartype', 1), ('furtype', 1)]...\n",
      "[2022-09-14 18:46:35,422] keeping 2000000 tokens which were in no less than 0 and no more than 700000 (=100.0%) documents\n",
      "[2022-09-14 18:46:38,681] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:46:38,732] adding document #700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:47:04,677] discarding 39899 tokens: [('zuständigkeit', 1), ('dearl', 1), ('evorn', 1), ('hbdb', 1), ('infalt', 1), ('kolbucar', 1), ('pograis', 1), ('sederstrand', 1), ('swartzback', 1), ('pleromy', 1)]...\n",
      "[2022-09-14 18:47:04,679] keeping 2000000 tokens which were in no less than 0 and no more than 710000 (=100.0%) documents\n",
      "[2022-09-14 18:47:09,259] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:47:09,334] adding document #710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:47:36,345] discarding 38790 tokens: [('tschabach', 1), ('δίοικος', 1), ('коприве', 1), ('неће', 1), ('allemannsrett', 1), ('betretungsrecht', 1), ('hemfridszon', 1), ('innmark', 1), ('komunikací', 1), ('pheasantries', 1)]...\n",
      "[2022-09-14 18:47:36,347] keeping 2000000 tokens which were in no less than 0 and no more than 720000 (=100.0%) documents\n",
      "[2022-09-14 18:47:40,969] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:47:41,044] adding document #720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:48:07,593] discarding 39595 tokens: [('askj', 1), ('nrelate', 1), ('pollroll', 1), ('sendori', 1), ('symptomfind', 1), ('dolgynwal', 1), ('kettlemere', 1), ('mowthey', 1), ('newtonmere', 1), ('pymhill', 1)]...\n",
      "[2022-09-14 18:48:07,595] keeping 2000000 tokens which were in no less than 0 and no more than 730000 (=100.0%) documents\n",
      "[2022-09-14 18:48:12,573] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:48:12,649] adding document #730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:48:36,710] discarding 36952 tokens: [('porotta', 1), ('rotikā', 1), ('rɒtiː', 1), ('sackina', 1), ('thalichapati', 1), ('วยไข', 1), ('โรต', 1), ('kaleindaung', 1), ('chapshoro', 1), ('chawaal', 1)]...\n",
      "[2022-09-14 18:48:36,711] keeping 2000000 tokens which were in no less than 0 and no more than 740000 (=100.0%) documents\n",
      "[2022-09-14 18:48:39,927] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:48:40,002] adding document #740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:49:04,884] discarding 40883 tokens: [('deinopoidea', 1), ('domiciliorum', 1), ('dubiepeira', 1), ('enacrosoma', 1), ('encyosaccus', 1), ('escribellates', 1), ('eustacesia', 1), ('faradja', 1), ('galaporella', 1), ('gasteracanthinae', 1)]...\n",
      "[2022-09-14 18:49:04,886] keeping 2000000 tokens which were in no less than 0 and no more than 750000 (=100.0%) documents\n",
      "[2022-09-14 18:49:09,787] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:49:09,868] adding document #750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:49:34,730] discarding 36027 tokens: [('freqüents', 1), ('gédeón', 1), ('històries', 1), ('infiltrada', 1), ('pamano', 1), ('stercoris', 1), ('veterinaris', 1), ('zàping', 1), ('kanalklyvningen', 1), ('sverigekanalen', 1)]...\n",
      "[2022-09-14 18:49:34,732] keeping 2000000 tokens which were in no less than 0 and no more than 760000 (=100.0%) documents\n",
      "[2022-09-14 18:49:38,013] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:49:38,064] adding document #760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:50:01,669] discarding 37193 tokens: [('wwii_mem_fountain_atlantic', 1), ('wwiimemorialnc', 1), ('chicarelli', 1), ('chidas', 1), ('keubler', 1), ('bidam', 1), ('bokhilgu', 1), ('chilsuk', 1), ('doricheon', 1), ('heumbahn', 1)]...\n",
      "[2022-09-14 18:50:01,671] keeping 2000000 tokens which were in no less than 0 and no more than 770000 (=100.0%) documents\n",
      "[2022-09-14 18:50:06,140] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:50:06,216] adding document #770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:50:31,039] discarding 42213 tokens: [('guilotes', 1), ('hadites', 1), ('hengconarius', 1), ('himalcoelotes', 1), ('hoffmannilena', 1), ('huangyuania', 1), ('hypocoelotes', 1), ('inermocoelotes', 1), ('iwogumoa', 1), ('kidugua', 1)]...\n",
      "[2022-09-14 18:50:31,040] keeping 2000000 tokens which were in no less than 0 and no more than 780000 (=100.0%) documents\n",
      "[2022-09-14 18:50:35,655] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:50:35,737] adding document #780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:51:01,563] discarding 39147 tokens: [('sstis', 1), ('synzolid', 1), ('vibativ', 1), ('voxazoldin', 1), ('zevtera', 1), ('zizolid', 1), ('zodlin', 1), ('zolinid', 1), ('zyvoxam', 1), ('zyvoxid', 1)]...\n",
      "[2022-09-14 18:51:01,564] keeping 2000000 tokens which were in no less than 0 and no more than 790000 (=100.0%) documents\n",
      "[2022-09-14 18:51:04,983] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:51:05,060] adding document #790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:51:28,907] discarding 36810 tokens: [('enstasy', 1), ('gewissheiten', 1), ('herestay', 1), ('hiersoux', 1), ('leaflight', 1), ('metambesen', 1), ('mondaron', 1), ('otherwind', 1), ('roughboks', 1), ('salitter', 1)]...\n",
      "[2022-09-14 18:51:28,908] keeping 2000000 tokens which were in no less than 0 and no more than 800000 (=100.0%) documents\n",
      "[2022-09-14 18:51:32,138] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:51:32,190] adding document #800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:51:57,216] discarding 41012 tokens: [('tetanoptera', 1), ('tetanura', 1), ('teutoniomyia', 1), ('thecomyia', 1), ('trypetolimnia', 1), ('trypetoptera', 1), ('verbekaria', 1), ('ammig', 1), ('doitinmuskoka', 1), ('avniel', 1)]...\n",
      "[2022-09-14 18:51:57,218] keeping 2000000 tokens which were in no less than 0 and no more than 810000 (=100.0%) documents\n",
      "[2022-09-14 18:52:01,903] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:52:01,982] adding document #810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:52:26,208] discarding 38291 tokens: [('kituxi', 1), ('negoleiros', 1), ('ngomas', 1), ('quilapanga', 1), ('adinon', 1), ('afafa', 1), ('ardiess', 1), ('baxx', 1), ('blaaz', 1), ('danhoun', 1)]...\n",
      "[2022-09-14 18:52:26,210] keeping 2000000 tokens which were in no less than 0 and no more than 820000 (=100.0%) documents\n",
      "[2022-09-14 18:52:29,479] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:52:29,532] adding document #820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:52:54,056] discarding 38766 tokens: [('vécseyvölgy', 1), ('zalánki', 1), ('zipsen', 1), ('bertbelongs', 1), ('bertblyleven', 1), ('curveballista', 1), ('interwebs', 1), ('rikaalbert', 1), ('siragu', 1), ('maconka', 1)]...\n",
      "[2022-09-14 18:52:54,058] keeping 2000000 tokens which were in no less than 0 and no more than 830000 (=100.0%) documents\n",
      "[2022-09-14 18:52:58,817] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:52:58,896] adding document #830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:53:23,447] discarding 40921 tokens: [('jablkami', 1), ('jabłka', 1), ('jabłkami', 1), ('jajkiem', 1), ('kalafiorowa', 1), ('kapuściana', 1), ('karkówka', 1), ('karpatka', 1), ('kartoflane', 1), ('kartoflanka', 1)]...\n",
      "[2022-09-14 18:53:23,449] keeping 2000000 tokens which were in no less than 0 and no more than 840000 (=100.0%) documents\n",
      "[2022-09-14 18:53:27,890] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:53:27,967] adding document #840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:53:50,163] discarding 37868 tokens: [('sabellariae', 1), ('sljusarev', 1), ('stoecharthrum', 1), ('vermiculicola', 1), ('medham', 1), ('medmeham', 1), ('pulsedialing', 1), ('tonedialing', 1), ('buchannans', 1), ('danee', 1)]...\n",
      "[2022-09-14 18:53:50,165] keeping 2000000 tokens which were in no less than 0 and no more than 850000 (=100.0%) documents\n",
      "[2022-09-14 18:53:53,457] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:53:53,511] adding document #850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:54:17,132] discarding 34929 tokens: [('brmv', 1), ('bysmv', 1), ('cbdav', 1), ('cocv', 1), ('corsv', 1), ('daffsv', 1), ('danasv', 1), ('dimmsv', 1), ('dmelsv', 1), ('dobssv', 1)]...\n",
      "[2022-09-14 18:54:17,134] keeping 2000000 tokens which were in no less than 0 and no more than 860000 (=100.0%) documents\n",
      "[2022-09-14 18:54:20,420] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:54:20,473] adding document #860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:54:45,526] discarding 42336 tokens: [('ballyartella', 1), ('cloughprior', 1), ('dromynnyre', 1), ('fowerteene', 1), ('killaoe', 1), ('kilodiernan', 1), ('monsea', 1), ('bartołomiej', 1), ('fatôme', 1), ('felicjanow', 1)]...\n",
      "[2022-09-14 18:54:45,527] keeping 2000000 tokens which were in no less than 0 and no more than 870000 (=100.0%) documents\n",
      "[2022-09-14 18:54:48,813] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:54:48,870] adding document #870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:55:13,396] discarding 34755 tokens: [('nasiredin', 1), ('orioloromano', 1), ('polifunctional', 1), ('taeduk', 1), ('youngchun', 1), ('courcet', 1), ('sentain', 1), ('freeclaim', 1), ('sammyville', 1), ('andeastern', 1)]...\n",
      "[2022-09-14 18:55:13,398] keeping 2000000 tokens which were in no less than 0 and no more than 880000 (=100.0%) documents\n",
      "[2022-09-14 18:55:18,106] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:55:18,182] adding document #880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:55:43,889] discarding 36262 tokens: [('woloski', 1), ('irrequieto', 1), ('cachikis', 1), ('petmed', 1), ('zooscape', 1), ('kinnnock', 1), ('plutopress', 1), ('yarls', 1), ('andyandhilary', 1), ('aerfen', 1)]...\n",
      "[2022-09-14 18:55:43,891] keeping 2000000 tokens which were in no less than 0 and no more than 890000 (=100.0%) documents\n",
      "[2022-09-14 18:55:47,190] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:55:47,243] adding document #890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:56:10,830] discarding 38694 tokens: [('qtcups', 1), ('splix', 1), ('texttops', 1), ('wassarman', 1), ('tonnette', 1), ('batmanesque', 1), ('briggadiers', 1), ('lcjb', 1), ('sdvc', 1), ('sdvcs', 1)]...\n",
      "[2022-09-14 18:56:10,832] keeping 2000000 tokens which were in no less than 0 and no more than 900000 (=100.0%) documents\n",
      "[2022-09-14 18:56:15,547] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:56:15,626] adding document #900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:56:38,064] discarding 34162 tokens: [('ōane', 1), ('ahiliyabai', 1), ('augarnath', 1), ('begampul', 1), ('begumpul', 1), ('bhainsali', 1), ('distillary', 1), ('gokalpur', 1), ('janikhurd', 1), ('janwani', 1)]...\n",
      "[2022-09-14 18:56:38,066] keeping 2000000 tokens which were in no less than 0 and no more than 910000 (=100.0%) documents\n",
      "[2022-09-14 18:56:41,399] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:56:41,453] adding document #910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:57:06,870] discarding 34374 tokens: [('coraa', 1), ('cosmebio', 1), ('ecovin', 1), ('kilimohai', 1), ('ulase', 1), ('úkzúz', 1), ('eratoxica', 1), ('krakowiaks', 1), ('saozinha', 1), ('yavasharian', 1)]...\n",
      "[2022-09-14 18:57:06,872] keeping 2000000 tokens which were in no less than 0 and no more than 920000 (=100.0%) documents\n",
      "[2022-09-14 18:57:12,022] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 18:57:12,100] adding document #920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:57:37,437] discarding 41043 tokens: [('drohnen', 1), ('ehegarten', 1), ('europska', 1), ('feldherrnhügel', 1), ('humoristen', 1), ('kinnbacken', 1), ('ljublin', 1), ('pensionistengambit', 1), ('rauchtabak', 1), ('rossetummler', 1)]...\n",
      "[2022-09-14 18:57:37,439] keeping 2000000 tokens which were in no less than 0 and no more than 930000 (=100.0%) documents\n",
      "[2022-09-14 18:57:40,824] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:57:40,878] adding document #930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:58:03,381] discarding 37689 tokens: [('broşteni', 1), ('constin', 1), ('drogoman', 1), ('lână', 1), ('mateiaș', 1), ('vistiern', 1), ('nithish', 1), ('kahdijah', 1), ('keanda', 1), ('penwah', 1)]...\n",
      "[2022-09-14 18:58:03,382] keeping 2000000 tokens which were in no less than 0 and no more than 940000 (=100.0%) documents\n",
      "[2022-09-14 18:58:08,124] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:58:08,208] adding document #940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:58:32,871] discarding 52826 tokens: [('schmiedt', 1), ('gandharv', 1), ('honeymakers', 1), ('spiritdrum', 1), ('jhoomo', 1), ('meharbani', 1), ('courthorpe', 1), ('evidenciju', 1), ('villepoix', 1), ('narodnosni', 1)]...\n",
      "[2022-09-14 18:58:32,873] keeping 2000000 tokens which were in no less than 0 and no more than 950000 (=100.0%) documents\n",
      "[2022-09-14 18:58:36,822] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:58:36,879] adding document #950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:59:00,510] discarding 41337 tokens: [('khödölgöönii', 1), ('khödölmöriin', 1), ('khögjliin', 1), ('khötölböriin', 1), ('khünii', 1), ('mppман', 1), ('naidalaa', 1), ('negdliin', 1), ('olny', 1), ('olonkhiin', 1)]...\n",
      "[2022-09-14 18:59:00,512] keeping 2000000 tokens which were in no less than 0 and no more than 960000 (=100.0%) documents\n",
      "[2022-09-14 18:59:03,834] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:59:03,888] adding document #960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:59:27,865] discarding 43558 tokens: [('sinoeun', 1), ('baongla', 1), ('cpdmrdpc', 1), ('cpnrpcrn', 1), ('fnscfsnc', 1), ('foligar', 1), ('nudpundp', 1), ('rufasca', 1), ('sdffsd', 1), ('suhaiqi', 1)]...\n",
      "[2022-09-14 18:59:27,867] keeping 2000000 tokens which were in no less than 0 and no more than 970000 (=100.0%) documents\n",
      "[2022-09-14 18:59:31,497] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:59:31,552] adding document #970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:59:54,280] discarding 36850 tokens: [('mulligian', 1), ('radulavitch', 1), ('sadmantha', 1), ('slopworth', 1), ('kinderactive', 1), ('morphabet', 1), ('gnadenlos', 1), ('gnadenloser', 1), ('linkspopulismus', 1), ('rechtspopulisten', 1)]...\n",
      "[2022-09-14 18:59:54,282] keeping 2000000 tokens which were in no less than 0 and no more than 980000 (=100.0%) documents\n",
      "[2022-09-14 18:59:59,082] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 18:59:59,166] adding document #980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:00:22,020] discarding 42877 tokens: [('konoagil', 1), ('malendok', 1), ('nimamar', 1), ('tohian', 1), ('agroromantica', 1), ('alocard', 1), ('bezanka', 1), ('boieri', 1), ('buzurniuc', 1), ('cafemax', 1)]...\n",
      "[2022-09-14 19:00:22,022] keeping 2000000 tokens which were in no less than 0 and no more than 990000 (=100.0%) documents\n",
      "[2022-09-14 19:00:26,323] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:00:26,403] adding document #990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:00:47,836] discarding 47705 tokens: [('woombie', 1), ('дискография', 1), ('bassprey', 1), ('funkslap', 1), ('guzzla', 1), ('mythz', 1), ('skreecher', 1), ('soundshower', 1), ('suddendef', 1), ('clownstep', 1)]...\n",
      "[2022-09-14 19:00:47,838] keeping 2000000 tokens which were in no less than 0 and no more than 1000000 (=100.0%) documents\n",
      "[2022-09-14 19:00:52,966] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:00:53,049] adding document #1000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:01:17,700] discarding 44243 tokens: [('bapalmen', 1), ('bapalmois', 1), ('bapalmoises', 1), ('batpalmen', 1), ('déchin', 1), ('guidet', 1), ('helluin', 1), ('mathildeartois', 1), ('robertartois', 1), ('stenne', 1)]...\n",
      "[2022-09-14 19:01:17,701] keeping 2000000 tokens which were in no less than 0 and no more than 1010000 (=100.0%) documents\n",
      "[2022-09-14 19:01:21,492] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:01:21,578] adding document #1010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:01:46,502] discarding 38264 tokens: [('mаршрутное', 1), ('naseleniye', 1), ('omissiya', 1), ('oruss', 1), ('rezvychaynaya', 1), ('rubiti', 1), ('scoptsy', 1), ('sektsiya', 1), ('siloviye', 1), ('skoptzi', 1)]...\n",
      "[2022-09-14 19:01:46,503] keeping 2000000 tokens which were in no less than 0 and no more than 1020000 (=100.0%) documents\n",
      "[2022-09-14 19:01:51,258] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:01:51,339] adding document #1020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:02:13,696] discarding 33381 tokens: [('norshirakan', 1), ('olnut', 1), ('oodians', 1), ('orduniq', 1), ('qadchberuni', 1), ('qanaqer', 1), ('qolian', 1), ('raphsonian', 1), ('remposian', 1), ('selkuniq', 1)]...\n",
      "[2022-09-14 19:02:13,697] keeping 2000000 tokens which were in no less than 0 and no more than 1030000 (=100.0%) documents\n",
      "[2022-09-14 19:02:18,803] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:02:18,884] adding document #1030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:02:41,576] discarding 34993 tokens: [('mobilemate', 1), ('ogdan', 1), ('runz', 1), ('annecdotes', 1), ('linderbrock', 1), ('pourgourides', 1), ('rackhman', 1), ('raeden', 1), ('aghdashlou', 1), ('davayi', 1)]...\n",
      "[2022-09-14 19:02:41,578] keeping 2000000 tokens which were in no less than 0 and no more than 1040000 (=100.0%) documents\n",
      "[2022-09-14 19:02:44,915] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:02:44,970] adding document #1040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:03:07,147] discarding 33077 tokens: [('мирзиёев', 1), ('миромон', 1), ('миромонович', 1), ('шавкат', 1), ('ўғли', 1), ('peribathous', 1), ('cassinite', 1), ('genteelest', 1), ('detudjmanization', 1), ('pavošević', 1)]...\n",
      "[2022-09-14 19:03:07,149] keeping 2000000 tokens which were in no less than 0 and no more than 1050000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:03:12,146] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:03:12,226] adding document #1050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:03:36,080] discarding 34917 tokens: [('skullfracahtmark', 1), ('anshans', 1), ('anzalag', 1), ('anzalazh', 1), ('anzaliport', 1), ('anzalis', 1), ('anzaluy', 1), ('anzalī', 1), ('bolvare', 1), ('caspiansea', 1)]...\n",
      "[2022-09-14 19:03:36,082] keeping 2000000 tokens which were in no less than 0 and no more than 1060000 (=100.0%) documents\n",
      "[2022-09-14 19:03:40,874] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:03:40,953] adding document #1060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:04:02,614] discarding 32473 tokens: [('李福和', 1), ('李福樹', 1), ('李越挺', 1), ('杜家駒', 1), ('杜惠愷', 1), ('林健鋒', 1), ('林兆鑫', 1), ('林和起', 1), ('林孟達', 1), ('林淔源', 1)]...\n",
      "[2022-09-14 19:04:02,616] keeping 2000000 tokens which were in no less than 0 and no more than 1070000 (=100.0%) documents\n",
      "[2022-09-14 19:04:07,418] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:04:07,500] adding document #1070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:04:30,193] discarding 36862 tokens: [('人民联合阵线', 1), ('人民自由民主党parti', 1), ('人民行动党parti', 1), ('人民阵线barisan', 1), ('公民党parti', 1), ('加东居民统一工会persatuan', 1), ('劳工党parti', 1), ('劳工阵线barisan', 1), ('团结新加坡民主党demokrat', 1), ('国人为先warga', 1)]...\n",
      "[2022-09-14 19:04:30,196] keeping 2000000 tokens which were in no less than 0 and no more than 1080000 (=100.0%) documents\n",
      "[2022-09-14 19:04:35,069] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:04:35,124] adding document #1080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:04:57,758] discarding 37181 tokens: [('arabiense', 1), ('arupense', 1), ('aubagnense', 1), ('aubagnese', 1), ('austroafricanum', 1), ('bacteremicum', 1), ('barrassiae', 1), ('boenickei', 1), ('bolletii', 1), ('brisbanense', 1)]...\n",
      "[2022-09-14 19:04:57,760] keeping 2000000 tokens which were in no less than 0 and no more than 1090000 (=100.0%) documents\n",
      "[2022-09-14 19:05:02,554] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:05:02,640] adding document #1090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:05:23,409] discarding 34505 tokens: [('edixhoven', 1), ('monodromie', 1), ('revêtements', 1), ('texed', 1), ('étales', 1), ('cctvphoenix', 1), ('meilitao', 1), ('dörpinghaus', 1), ('horrmeyer', 1), ('langhein', 1)]...\n",
      "[2022-09-14 19:05:23,411] keeping 2000000 tokens which were in no less than 0 and no more than 1100000 (=100.0%) documents\n",
      "[2022-09-14 19:05:28,119] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:05:28,201] adding document #1100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:05:51,779] discarding 34472 tokens: [('cesettianus', 1), ('diadumenus', 1), ('gestarent', 1), ('implicavit', 1), ('interficio', 1), ('iudicanda', 1), ('maecii', 1), ('moesius', 1), ('mythistoricis', 1), ('pollentiam', 1)]...\n",
      "[2022-09-14 19:05:51,780] keeping 2000000 tokens which were in no less than 0 and no more than 1110000 (=100.0%) documents\n",
      "[2022-09-14 19:05:55,146] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:05:55,202] adding document #1110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:06:17,452] discarding 33008 tokens: [('arakady', 1), ('arrita', 1), ('aureny', 1), ('ayeid', 1), ('baeidinejad', 1), ('benguerrah', 1), ('beratović', 1), ('bocchit', 1), ('burmistre', 1), ('cakacakabalavu', 1)]...\n",
      "[2022-09-14 19:06:17,454] keeping 2000000 tokens which were in no less than 0 and no more than 1120000 (=100.0%) documents\n",
      "[2022-09-14 19:06:20,812] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:06:20,867] adding document #1120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:06:45,157] discarding 34611 tokens: [('jactitation#2', 1), ('beatíssima', 1), ('ibvms', 1), ('iebvm', 1), ('sporoblasts', 1), ('cestap', 1), ('conspirationnisme', 1), ('canettii', 1), ('granulotomatous', 1), ('isotuberculosinol', 1)]...\n",
      "[2022-09-14 19:06:45,158] keeping 2000000 tokens which were in no less than 0 and no more than 1130000 (=100.0%) documents\n",
      "[2022-09-14 19:06:48,491] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:06:48,547] adding document #1130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:07:12,141] discarding 36121 tokens: [('außenhandelsbank', 1), ('bezposhchadny', 1), ('boiky', 1), ('fidonixi', 1), ('fidonísi', 1), ('kritzkii', 1), ('kubanskyi', 1), ('leonymus', 1), ('olbiopolitan', 1), ('serpilor', 1)]...\n",
      "[2022-09-14 19:07:12,144] keeping 2000000 tokens which were in no less than 0 and no more than 1140000 (=100.0%) documents\n",
      "[2022-09-14 19:07:16,846] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:07:16,926] adding document #1140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:07:38,928] discarding 34862 tokens: [('khadesh', 1), ('kmyţ', 1), ('krkmš', 1), ('krkš', 1), ('kškš', 1), ('mushanet', 1), ('mwšꜣnt', 1), ('nrrn', 1), ('nuḥḥašši', 1), ('nwgs', 1)]...\n",
      "[2022-09-14 19:07:38,930] keeping 2000000 tokens which were in no less than 0 and no more than 1150000 (=100.0%) documents\n",
      "[2022-09-14 19:07:43,651] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:07:43,733] adding document #1150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:08:06,818] discarding 39928 tokens: [('óióai', 1), ('gypen', 1), ('marsenich', 1), ('oppla', 1), ('ashizuka', 1), ('azhizuka', 1), ('dojiro', 1), ('dojita', 1), ('jijinuki', 1), ('progolfer', 1)]...\n",
      "[2022-09-14 19:08:06,820] keeping 2000000 tokens which were in no less than 0 and no more than 1160000 (=100.0%) documents\n",
      "[2022-09-14 19:08:11,614] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:08:11,696] adding document #1160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:08:35,220] discarding 32832 tokens: [('studzieniczne', 1), ('browarowy', 1), ('grodztwo', 1), ('kamieniogóra', 1), ('landishute', 1), ('męczenników', 1), ('rażańcowej', 1), ('rehabilitacji', 1), ('tkactwa', 1), ('automatec', 1)]...\n",
      "[2022-09-14 19:08:35,222] keeping 2000000 tokens which were in no less than 0 and no more than 1170000 (=100.0%) documents\n",
      "[2022-09-14 19:08:40,083] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:08:40,165] adding document #1170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:09:02,772] discarding 37725 tokens: [('둔덕산', 1), ('마니산', 1), ('마대산', 1), ('마이산', 1), ('마적산', 1), ('막장봉', 1), ('만덕산', 1), ('만복대', 1), ('만수봉', 1), ('만탑산', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:09:02,774] keeping 2000000 tokens which were in no less than 0 and no more than 1180000 (=100.0%) documents\n",
      "[2022-09-14 19:09:06,123] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:09:06,183] adding document #1180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:09:29,280] discarding 41462 tokens: [('옹암리', 1), ('용상리', 1), ('용혈리', 1), ('운문리', 1), ('월호리', 1), ('유전리', 1), ('읍내리', 1), ('이산면', 1), ('일원리', 1), ('임곡리', 1)]...\n",
      "[2022-09-14 19:09:29,283] keeping 2000000 tokens which were in no less than 0 and no more than 1190000 (=100.0%) documents\n",
      "[2022-09-14 19:09:34,092] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:09:34,176] adding document #1190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:09:58,972] discarding 32839 tokens: [('sprectrally', 1), ('doughderas', 1), ('doughleanders', 1), ('waterslip', 1), ('aliantei', 1), ('ayufd', 1), ('cppy', 1), ('democratilor', 1), ('eflry', 1), ('euroyouthua', 1)]...\n",
      "[2022-09-14 19:09:58,974] keeping 2000000 tokens which were in no less than 0 and no more than 1200000 (=100.0%) documents\n",
      "[2022-09-14 19:10:03,755] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:10:03,837] adding document #1200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:10:26,394] discarding 34357 tokens: [('고하대로', 1), ('광명역나들목', 1), ('광천나들목', 1), ('군산나들목', 1), ('군산휴게소', 1), ('금천나들목', 1), ('남북', 1), ('당진나들목', 1), ('당진분기점', 1), ('대천나들목', 1)]...\n",
      "[2022-09-14 19:10:26,395] keeping 2000000 tokens which were in no less than 0 and no more than 1210000 (=100.0%) documents\n",
      "[2022-09-14 19:10:29,780] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:10:29,837] adding document #1210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:10:53,276] discarding 34958 tokens: [('velenići', 1), ('vitine', 1), ('vranjevići', 1), ('vukušići', 1), ('zakmur', 1), ('zavait', 1), ('ćurevo', 1), ('čelikovo', 1), ('đeđevo', 1), ('škobalji', 1)]...\n",
      "[2022-09-14 19:10:53,277] keeping 2000000 tokens which were in no less than 0 and no more than 1220000 (=100.0%) documents\n",
      "[2022-09-14 19:10:58,092] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:10:58,175] adding document #1220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:11:21,499] discarding 31202 tokens: [('gameza', 1), ('rickyamadour', 1), ('liistro', 1), ('ecomm', 1), ('onesynthetic', 1), ('riulf', 1), ('soapery', 1), ('clariano', 1), ('karamokoba', 1), ('metallarius', 1)]...\n",
      "[2022-09-14 19:11:21,501] keeping 2000000 tokens which were in no less than 0 and no more than 1230000 (=100.0%) documents\n",
      "[2022-09-14 19:11:26,334] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:11:26,416] adding document #1230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:11:49,285] discarding 32049 tokens: [('ckhs', 1), ('amaugou', 1), ('bellators', 1), ('browarski', 1), ('cedenblad', 1), ('constandache', 1), ('haciani', 1), ('mastioli', 1), ('mortelette', 1), ('tihamer', 1)]...\n",
      "[2022-09-14 19:11:49,287] keeping 2000000 tokens which were in no less than 0 and no more than 1240000 (=100.0%) documents\n",
      "[2022-09-14 19:11:52,741] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:11:52,803] adding document #1240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:12:15,996] discarding 33823 tokens: [('giełczyńska', 1), ('hakehilot', 1), ('woziwodzka', 1), ('zatylna', 1), ('nauseb', 1), ('elongatedand', 1), ('phansianelle', 1), ('phasianellidae', 1), ('bcbusiness', 1), ('chandzod', 1)]...\n",
      "[2022-09-14 19:12:15,998] keeping 2000000 tokens which were in no less than 0 and no more than 1250000 (=100.0%) documents\n",
      "[2022-09-14 19:12:20,905] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:12:20,993] adding document #1250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:12:46,008] discarding 34965 tokens: [('brawerman', 1), ('fivepoint', 1), ('alfredalumni', 1), ('brodieco', 1), ('calendar_show', 1), ('ceramicstoday', 1), ('chitton', 1), ('fultonstreetgallery', 1), ('home_main', 1), ('lattitudegallery', 1)]...\n",
      "[2022-09-14 19:12:46,010] keeping 2000000 tokens which were in no less than 0 and no more than 1260000 (=100.0%) documents\n",
      "[2022-09-14 19:12:50,722] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:12:50,805] adding document #1260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:13:14,730] discarding 38284 tokens: [('berrondo', 1), ('biliardoweb', 1), ('birilli', 1), ('birillo', 1), ('boccette', 1), ('cavazzana', 1), ('cifalà', 1), ('contentitem', 1), ('fibis', 1), ('gualemi', 1)]...\n",
      "[2022-09-14 19:13:14,732] keeping 2000000 tokens which were in no less than 0 and no more than 1270000 (=100.0%) documents\n",
      "[2022-09-14 19:13:19,547] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:13:19,631] adding document #1270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:13:44,021] discarding 34267 tokens: [('diferencijalne', 1), ('fizici', 1), ('geometrije', 1), ('kvantnoj', 1), ('nerelativističkoj', 1), ('opšte', 1), ('simetriji', 1), ('дамњановић', 1), ('activolcans', 1), ('modvolc', 1)]...\n",
      "[2022-09-14 19:13:44,023] keeping 2000000 tokens which were in no less than 0 and no more than 1280000 (=100.0%) documents\n",
      "[2022-09-14 19:13:48,757] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:13:48,841] adding document #1280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:14:12,817] discarding 41282 tokens: [('cegos', 1), ('namorada', 1), ('popô', 1), ('prefiro', 1), ('ségio', 1), ('vestidos', 1), ('violência', 1), ('eye_problem', 1), ('hinu', 1), ('milogardner', 1)]...\n",
      "[2022-09-14 19:14:12,819] keeping 2000000 tokens which were in no less than 0 and no more than 1290000 (=100.0%) documents\n",
      "[2022-09-14 19:14:17,570] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:14:17,654] adding document #1290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:14:39,451] discarding 30146 tokens: [('addinston', 1), ('ugston', 1), ('struttergear', 1), ('zpittvar', 1), ('brylski', 1), ('burroweed', 1), ('grossenheider', 1), ('pencillatus', 1), ('prothermolysin', 1), ('stearothermophillus', 1)]...\n",
      "[2022-09-14 19:14:39,453] keeping 2000000 tokens which were in no less than 0 and no more than 1300000 (=100.0%) documents\n",
      "[2022-09-14 19:14:44,194] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:14:44,276] adding document #1300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:15:06,938] discarding 33671 tokens: [('abdulagadir', 1), ('alembekova', 1), ('bahmad', 1), ('bakheet', 1), ('bisluke', 1), ('bledman', 1), ('bryshon', 1), ('chemning', 1), ('cleiton', 1), ('deiac', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:15:06,940] keeping 2000000 tokens which were in no less than 0 and no more than 1310000 (=100.0%) documents\n",
      "[2022-09-14 19:15:10,838] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:15:10,923] adding document #1310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:15:34,285] discarding 35194 tokens: [('vpln', 1), ('guluband', 1), ('mithoo', 1), ('phulan', 1), ('xhush', 1), ('achiola', 1), ('beikos', 1), ('museumin', 1), ('myede', 1), ('notesa', 1)]...\n",
      "[2022-09-14 19:15:34,287] keeping 2000000 tokens which were in no less than 0 and no more than 1320000 (=100.0%) documents\n",
      "[2022-09-14 19:15:37,675] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:15:37,732] adding document #1320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:15:59,821] discarding 32412 tokens: [('sulatycky', 1), ('eilzabeth', 1), ('hinchingbroke', 1), ('jeslyn', 1), ('abscheid', 1), ('taisui', 1), ('楊任', 1), ('甲子太歲之神', 1), ('arliano', 1), ('burlamacca', 1)]...\n",
      "[2022-09-14 19:15:59,823] keeping 2000000 tokens which were in no less than 0 and no more than 1330000 (=100.0%) documents\n",
      "[2022-09-14 19:16:03,253] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:16:03,311] adding document #1330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:16:27,235] discarding 36590 tokens: [('elphnstone', 1), ('gauto', 1), ('ibérique', 1), ('iscovich', 1), ('lebonian', 1), ('lestingi', 1), ('lomianto', 1), ('pontrémoli', 1), ('rolandelli', 1), ('smudt', 1)]...\n",
      "[2022-09-14 19:16:27,237] keeping 2000000 tokens which were in no less than 0 and no more than 1340000 (=100.0%) documents\n",
      "[2022-09-14 19:16:30,646] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:16:30,705] adding document #1340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:16:54,478] discarding 35347 tokens: [('cortesina', 1), ('amaruka', 1), ('amarukaśataka', 1), ('amarusataka', 1), ('amarusatakam', 1), ('amarushataka', 1), ('amaruśataka', 1), ('devadhar', 1), ('ravichandra', 1), ('demest', 1)]...\n",
      "[2022-09-14 19:16:54,479] keeping 2000000 tokens which were in no less than 0 and no more than 1350000 (=100.0%) documents\n",
      "[2022-09-14 19:16:57,886] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:16:57,944] adding document #1350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:17:21,290] discarding 35810 tokens: [('czeydner', 1), ('ermil', 1), ('feketehalmy', 1), ('fioravanzo', 1), ('gorondy', 1), ('jány', 1), ('kisbarnak', 1), ('nemzetvezető', 1), ('rattanakun', 1), ('reichmarshall', 1)]...\n",
      "[2022-09-14 19:17:21,292] keeping 2000000 tokens which were in no less than 0 and no more than 1360000 (=100.0%) documents\n",
      "[2022-09-14 19:17:26,165] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:17:26,250] adding document #1360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:17:49,491] discarding 39728 tokens: [('apmamman', 1), ('bielich', 1), ('crossmahon', 1), ('mluver', 1), ('resilire', 1), ('unddr', 1), ('wcdr', 1), ('decimazione', 1), ('kallstroem', 1), ('nuidheacht', 1)]...\n",
      "[2022-09-14 19:17:49,493] keeping 2000000 tokens which were in no less than 0 and no more than 1370000 (=100.0%) documents\n",
      "[2022-09-14 19:17:52,936] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:17:52,994] adding document #1370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:18:15,565] discarding 33750 tokens: [('cantalonia', 1), ('milldred', 1), ('ourdigitalworld', 1), ('geitsætri', 1), ('leirdalen', 1), ('storgjuvtinden', 1), ('storjuvbrean', 1), ('storjuvet', 1), ('storjuvtinden', 1), ('transcddj', 1)]...\n",
      "[2022-09-14 19:18:15,567] keeping 2000000 tokens which were in no less than 0 and no more than 1380000 (=100.0%) documents\n",
      "[2022-09-14 19:18:20,357] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:18:20,442] adding document #1380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:18:43,268] discarding 35589 tokens: [('senthamil', 1), ('thillainathan', 1), ('zabrina', 1), ('amerex', 1), ('frostline', 1), ('hotohara', 1), ('kesshitai', 1), ('kyaeen', 1), ('rodoplphe', 1), ('skav', 1)]...\n",
      "[2022-09-14 19:18:43,270] keeping 2000000 tokens which were in no less than 0 and no more than 1390000 (=100.0%) documents\n",
      "[2022-09-14 19:18:46,669] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:18:46,727] adding document #1390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:19:10,888] discarding 33916 tokens: [('downarowicz', 1), ('kiernik', 1), ('kierownik', 1), ('smólski', 1), ('bródáin', 1), ('dancity', 1), ('deachunter', 1), ('electrosleep', 1), ('eyott', 1), ('faggot#2', 1)]...\n",
      "[2022-09-14 19:19:10,890] keeping 2000000 tokens which were in no less than 0 and no more than 1400000 (=100.0%) documents\n",
      "[2022-09-14 19:19:14,459] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:19:14,523] adding document #1400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:19:38,809] discarding 35472 tokens: [('egīls', 1), ('kinyor', 1), ('kocuvan', 1), ('kucej', 1), ('tēbelis', 1), ('zadoynov', 1), ('lignière', 1), ('asaeqw', 1), ('axskb', 1), ('hennerton', 1)]...\n",
      "[2022-09-14 19:19:38,811] keeping 2000000 tokens which were in no less than 0 and no more than 1410000 (=100.0%) documents\n",
      "[2022-09-14 19:19:42,337] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:19:42,401] adding document #1410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:20:07,314] discarding 31031 tokens: [('hirshel', 1), ('elmsn', 1), ('blasenschnecken', 1), ('gemisto', 1), ('lithoglyphus', 1), ('physcella', 1), ('phytobentos', 1), ('uktag', 1), ('naturalhistoryroom', 1), ('woburnpubliclibrary', 1)]...\n",
      "[2022-09-14 19:20:07,316] keeping 2000000 tokens which were in no less than 0 and no more than 1420000 (=100.0%) documents\n",
      "[2022-09-14 19:20:12,080] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:20:12,171] adding document #1420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:20:35,830] discarding 34687 tokens: [('hocktideh', 1), ('boutviseth', 1), ('kirisute', 1), ('mackfall', 1), ('usacti', 1), ('usharddigi', 1), ('bandmastership', 1), ('lachryis', 1), ('willowbye', 1), ('jesset', 1)]...\n",
      "[2022-09-14 19:20:35,834] keeping 2000000 tokens which were in no less than 0 and no more than 1430000 (=100.0%) documents\n",
      "[2022-09-14 19:20:41,094] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:20:41,178] adding document #1430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:21:05,120] discarding 33967 tokens: [('arnautz', 1), ('ataüt', 1), ('corbatàs', 1), ('cucunhan', 1), ('maucòr', 1), ('mètge', 1), ('nuòch', 1), ('pietat', 1), ('secrèt', 1), ('servici', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:21:05,121] keeping 2000000 tokens which were in no less than 0 and no more than 1440000 (=100.0%) documents\n",
      "[2022-09-14 19:21:09,502] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:21:09,587] adding document #1440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:21:31,802] discarding 31669 tokens: [('uhtrex', 1), ('gcvt', 1), ('subtilise', 1), ('exog', 1), ('maturases', 1), ('josyln', 1), ('borsodban', 1), ('cserehát', 1), ('dörgicse', 1), ('emlékek', 1)]...\n",
      "[2022-09-14 19:21:31,803] keeping 2000000 tokens which were in no less than 0 and no more than 1450000 (=100.0%) documents\n",
      "[2022-09-14 19:21:36,968] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:21:37,052] adding document #1450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:21:56,798] discarding 29311 tokens: [('bfnc', 1), ('cebron', 1), ('gourgéennes', 1), ('gourgéens', 1), ('heptadeca', 1), ('mycocentrosporina', 1), ('stafon', 1), ('acrocylindrium', 1), ('caerulens', 1), ('cerulein', 1)]...\n",
      "[2022-09-14 19:21:56,800] keeping 2000000 tokens which were in no less than 0 and no more than 1460000 (=100.0%) documents\n",
      "[2022-09-14 19:22:01,642] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:22:01,725] adding document #1460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:22:25,366] discarding 36264 tokens: [('braincogs', 1), ('tobbs', 1), ('cietra', 1), ('makkole', 1), ('wehmann', 1), ('mansoorian', 1), ('nonnested', 1), ('ajungilak', 1), ('conzzeta', 1), ('dintikon', 1)]...\n",
      "[2022-09-14 19:22:25,368] keeping 2000000 tokens which were in no less than 0 and no more than 1470000 (=100.0%) documents\n",
      "[2022-09-14 19:22:30,134] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:22:30,219] adding document #1470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:22:54,283] discarding 31442 tokens: [('etablisements', 1), ('polynesie', 1), ('établisements', 1), ('liberpluscarden', 1), ('macgreagor', 1), ('ophirville', 1), ('bronchorst', 1), ('catalijntje', 1), ('muzikaal', 1), ('agersborg', 1)]...\n",
      "[2022-09-14 19:22:54,285] keeping 2000000 tokens which were in no less than 0 and no more than 1480000 (=100.0%) documents\n",
      "[2022-09-14 19:22:59,098] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:22:59,183] adding document #1480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:23:22,190] discarding 31231 tokens: [('boysag', 1), ('cheyava', 1), ('nankwoeap', 1), ('shinumo', 1), ('tuckup', 1), ('carcone', 1), ('giannoccaro', 1), ('machitski', 1), ('sarafree', 1), ('stepec', 1)]...\n",
      "[2022-09-14 19:23:22,192] keeping 2000000 tokens which were in no less than 0 and no more than 1490000 (=100.0%) documents\n",
      "[2022-09-14 19:23:25,908] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:23:25,992] adding document #1490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:23:50,539] discarding 36021 tokens: [('hemispericle', 1), ('setoff', 1), ('slipsheets', 1), ('birkenwaldfinnland', 1), ('grädel', 1), ('hertzmann', 1), ('kolaitis', 1), ('libkin', 1), ('hurdla', 1), ('ispirati', 1)]...\n",
      "[2022-09-14 19:23:50,541] keeping 2000000 tokens which were in no less than 0 and no more than 1500000 (=100.0%) documents\n",
      "[2022-09-14 19:23:55,324] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:23:55,410] adding document #1500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:24:18,265] discarding 32631 tokens: [('abandonedwithin', 1), ('anappanasati', 1), ('assāsa', 1), ('bojjhanga', 1), ('cittapassaddhi', 1), ('energeticness', 1), ('jhanic', 1), ('kayāpassaddhi', 1), ('khemavaggo', 1), ('kāyo', 1)]...\n",
      "[2022-09-14 19:24:18,266] keeping 2000000 tokens which were in no less than 0 and no more than 1510000 (=100.0%) documents\n",
      "[2022-09-14 19:24:21,632] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:24:21,689] adding document #1510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:24:42,725] discarding 29622 tokens: [('precoloniale', 1), ('educatated', 1), ('andavari', 1), ('bidashal', 1), ('cubasil', 1), ('damiem', 1), ('futatsuki', 1), ('gandláfr', 1), ('geboundolf', 1), ('gimili', 1)]...\n",
      "[2022-09-14 19:24:42,727] keeping 2000000 tokens which were in no less than 0 and no more than 1520000 (=100.0%) documents\n",
      "[2022-09-14 19:24:46,101] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:24:46,159] adding document #1520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:25:07,359] discarding 23547 tokens: [('celab', 1), ('laboratiores', 1), ('multicampus', 1), ('ruffilli', 1), ('sitlec', 1), ('sslmit', 1), ('eospalax', 1), ('fontanierii', 1), ('ineternet', 1), ('reflectus', 1)]...\n",
      "[2022-09-14 19:25:07,362] keeping 2000000 tokens which were in no less than 0 and no more than 1530000 (=100.0%) documents\n",
      "[2022-09-14 19:25:12,221] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:25:12,304] adding document #1530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:25:35,532] discarding 31460 tokens: [('giordanni', 1), ('returen', 1), ('sabbides', 1), ('wnje', 1), ('gorffenol', 1), ('verulanium', 1), ('bittlesman', 1), ('moviealso', 1), ('roleminiseries', 1), ('snowcoming', 1)]...\n",
      "[2022-09-14 19:25:35,533] keeping 2000000 tokens which were in no less than 0 and no more than 1540000 (=100.0%) documents\n",
      "[2022-09-14 19:25:40,415] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:25:40,501] adding document #1540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:26:04,157] discarding 36224 tokens: [('radolphus', 1), ('whittleswick', 1), ('wolvernote', 1), ('driloleirus', 1), ('macelfreshi', 1), ('pacificbio', 1), ('siciliensis', 1), ('compuling', 1), ('mistbelt', 1), ('salsafied', 1)]...\n",
      "[2022-09-14 19:26:04,159] keeping 2000000 tokens which were in no less than 0 and no more than 1550000 (=100.0%) documents\n",
      "[2022-09-14 19:26:08,993] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:26:09,080] adding document #1550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:26:32,556] discarding 37125 tokens: [('miconioides', 1), ('κώδειά', 1), ('κώδων', 1), ('markusfostre', 1), ('donnianum', 1), ('armitanus', 1), ('ceylanicus', 1), ('crundw', 1), ('hutchinsiae', 1), ('alaschanicum', 1)]...\n",
      "[2022-09-14 19:26:32,558] keeping 2000000 tokens which were in no less than 0 and no more than 1560000 (=100.0%) documents\n",
      "[2022-09-14 19:26:37,352] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:26:37,439] adding document #1560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:27:01,554] discarding 38339 tokens: [('auspacher', 1), ('dembelein', 1), ('ellrodts', 1), ('ellroth', 1), ('engelmorsweiler', 1), ('hillesheims', 1), ('ingwyler', 1), ('kegelbahnstraße', 1), ('kellergeister', 1), ('kerweplatz', 1)]...\n",
      "[2022-09-14 19:27:01,556] keeping 2000000 tokens which were in no less than 0 and no more than 1570000 (=100.0%) documents\n",
      "[2022-09-14 19:27:06,376] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:27:06,462] adding document #1570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:27:31,650] discarding 31072 tokens: [('seelard', 1), ('derlwyn', 1), ('berça', 1), ('muscelul', 1), ('capieau', 1), ('ettyket', 1), ('pooey', 1), ('ecueil', 1), ('escoueuille', 1), ('escueles', 1)]...\n",
      "[2022-09-14 19:27:31,652] keeping 2000000 tokens which were in no less than 0 and no more than 1580000 (=100.0%) documents\n",
      "[2022-09-14 19:27:36,566] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:27:36,650] adding document #1580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:28:00,717] discarding 35144 tokens: [('hannedouche', 1), ('boszacky', 1), ('metalochimic', 1), ('turcuș', 1), ('cortiguera', 1), ('gredilla', 1), ('nidáguila', 1), ('nocedo', 1), ('quintanaloma', 1), ('simencourt', 1)]...\n",
      "[2022-09-14 19:28:00,719] keeping 2000000 tokens which were in no less than 0 and no more than 1590000 (=100.0%) documents\n",
      "[2022-09-14 19:28:05,554] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:28:05,639] adding document #1590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:28:28,036] discarding 28894 tokens: [('bapx', 1), ('eureptiles', 1), ('morganucodont', 1), ('morganucodontidae', 1), ('theromorphs', 1), ('ursich', 1), ('passionettes', 1), ('bivariant', 1), ('kasilyo', 1), ('kesilyo', 1)]...\n",
      "[2022-09-14 19:28:28,038] keeping 2000000 tokens which were in no less than 0 and no more than 1600000 (=100.0%) documents\n",
      "[2022-09-14 19:28:31,405] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:28:31,463] adding document #1600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:28:56,495] discarding 32948 tokens: [('bondway', 1), ('viadux', 1), ('lbnr', 1), ('wpyc', 1), ('deasonville', 1), ('rolingson', 1), ('colwitz', 1), ('aljomaa', 1), ('annapoornamma', 1), ('chukkalu', 1)]...\n",
      "[2022-09-14 19:28:56,496] keeping 2000000 tokens which were in no less than 0 and no more than 1610000 (=100.0%) documents\n",
      "[2022-09-14 19:29:01,301] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:29:01,387] adding document #1610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:29:25,391] discarding 31339 tokens: [('incased', 1), ('rapest', 1), ('abhyudayam', 1), ('mamayya', 1), ('mokkapati', 1), ('mrokkubadi', 1), ('parvateesam', 1), ('pativratyam', 1), ('samavesamu', 1), ('lennacháin', 1)]...\n",
      "[2022-09-14 19:29:25,393] keeping 2000000 tokens which were in no less than 0 and no more than 1620000 (=100.0%) documents\n",
      "[2022-09-14 19:29:29,621] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:29:29,707] adding document #1620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:29:52,299] discarding 39484 tokens: [('chader', 1), ('darashikoh', 1), ('gaayan', 1), ('littletoy', 1), ('saiwalaya', 1), ('sangeetayan', 1), ('tamashai', 1), ('tukra', 1), ('marignoli', 1), ('sandonà', 1)]...\n",
      "[2022-09-14 19:29:52,300] keeping 2000000 tokens which were in no less than 0 and no more than 1630000 (=100.0%) documents\n",
      "[2022-09-14 19:29:57,433] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:29:57,520] adding document #1630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:30:19,825] discarding 42117 tokens: [('sportsshoes', 1), ('bardellini', 1), ('xystum', 1), ('villareños', 1), ('elbonirus', 1), ('grandmotet', 1), ('becomine', 1), ('kentelioù', 1), ('mcormand', 1), ('bonfons', 1)]...\n",
      "[2022-09-14 19:30:19,826] keeping 2000000 tokens which were in no less than 0 and no more than 1640000 (=100.0%) documents\n",
      "[2022-09-14 19:30:23,217] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:30:23,277] adding document #1640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:30:45,604] discarding 29439 tokens: [('cuusoo', 1), ('しんかい', 1), ('atthow', 1), ('stevingstone', 1), ('cuaŋ', 1), ('cɯaŋ', 1), ('kwean', 1), ('qenaan', 1), ('wippercht', 1), ('dimnus', 1)]...\n",
      "[2022-09-14 19:30:45,606] keeping 2000000 tokens which were in no less than 0 and no more than 1650000 (=100.0%) documents\n",
      "[2022-09-14 19:30:48,972] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:30:49,030] adding document #1650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:31:13,266] discarding 33943 tokens: [('zayta', 1), ('banketjes', 1), ('pauwel', 1), ('tijssens', 1), ('ikénga', 1), ('mbùríchi', 1), ('obalike', 1), ('ìkénga', 1), ('decero', 1), ('liveonthenet', 1)]...\n",
      "[2022-09-14 19:31:13,268] keeping 2000000 tokens which were in no less than 0 and no more than 1660000 (=100.0%) documents\n",
      "[2022-09-14 19:31:18,356] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:31:18,443] adding document #1660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:31:44,980] discarding 32811 tokens: [('cuitlateco', 1), ('moguler', 1), ('casademont', 1), ('csakanyi', 1), ('ereña', 1), ('esmorís', 1), ('ezugwu', 1), ('hettsheimeir', 1), ('lacero', 1), ('rudež', 1)]...\n",
      "[2022-09-14 19:31:44,982] keeping 2000000 tokens which were in no less than 0 and no more than 1670000 (=100.0%) documents\n",
      "[2022-09-14 19:31:49,602] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:31:49,689] adding document #1670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:32:12,253] discarding 35373 tokens: [('nuttaphon', 1), ('apaxco', 1), ('axapusco', 1), ('cocotitlán', 1), ('jiquipilco', 1), ('otzolotepec', 1), ('polotitlán', 1), ('texcaltitlán', 1), ('tezoyuca', 1), ('un_coyote_al_asecho_', 1)]...\n",
      "[2022-09-14 19:32:12,255] keeping 2000000 tokens which were in no less than 0 and no more than 1680000 (=100.0%) documents\n",
      "[2022-09-14 19:32:17,457] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:32:17,544] adding document #1680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:32:39,778] discarding 29775 tokens: [('didorosi', 1), ('holychuk', 1), ('saalai', 1), ('smutek', 1), ('mathematicaforprediction', 1), ('qgam', 1), ('travelthessaloniki', 1), ('μακεδονια', 1), ('cérvoles', 1), ('áraba', 1)]...\n",
      "[2022-09-14 19:32:39,780] keeping 2000000 tokens which were in no less than 0 and no more than 1690000 (=100.0%) documents\n",
      "[2022-09-14 19:32:44,597] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:32:44,683] adding document #1690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:33:08,020] discarding 38924 tokens: [('よか稚児ざくら', 1), ('カレーライス', 1), ('ギャング対ギャング', 1), ('クレージーの花嫁と七人の仲間', 1), ('サラリーマンどんと節', 1), ('サラリーマン一刀流', 1), ('ソーラン渡り鳥', 1), ('ニッポン無責任野郎', 1), ('ハイティーンやくざ', 1), ('ブルータウン', 1)]...\n",
      "[2022-09-14 19:33:08,021] keeping 2000000 tokens which were in no less than 0 and no more than 1700000 (=100.0%) documents\n",
      "[2022-09-14 19:33:11,458] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:33:11,518] adding document #1700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:33:37,886] discarding 45753 tokens: [('emska', 1), ('noakowski', 1), ('poprzeczka', 1), ('zamszany', 1), ('zarudzie', 1), ('złojec', 1), ('złojecka', 1), ('czarnostocka', 1), ('dzielce', 1), ('radecznicaonline', 1)]...\n",
      "[2022-09-14 19:33:37,888] keeping 2000000 tokens which were in no less than 0 and no more than 1710000 (=100.0%) documents\n",
      "[2022-09-14 19:33:43,087] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:33:43,179] adding document #1710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:34:06,642] discarding 39902 tokens: [('supervert', 1), ('xxgirls', 1), ('xxmodels', 1), ('zigelboum', 1), ('alternipinnada', 1), ('asnells', 1), ('bioinertness', 1), ('euphorbiaceous', 1), ('tjipetir', 1), ('kauleinamoku', 1)]...\n",
      "[2022-09-14 19:34:06,644] keeping 2000000 tokens which were in no less than 0 and no more than 1720000 (=100.0%) documents\n",
      "[2022-09-14 19:34:10,055] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:34:10,114] adding document #1720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:34:33,799] discarding 32721 tokens: [('otherwaies', 1), ('arrallano', 1), ('denhalter', 1), ('tejaztlan', 1), ('arheghan', 1), ('celebratekindness', 1), ('duthinh', 1), ('fistgate', 1), ('glistn', 1), ('glstn', 1)]...\n",
      "[2022-09-14 19:34:33,801] keeping 2000000 tokens which were in no less than 0 and no more than 1730000 (=100.0%) documents\n",
      "[2022-09-14 19:34:38,614] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:34:38,701] adding document #1730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:35:04,038] discarding 32616 tokens: [('扶餘阿美', 1), ('扶餘阿花', 1), ('扶餘阿莘', 1), ('扶餘靑稽', 1), ('承乾', 1), ('政開', 1), ('故國原王', 1), ('故國壤王', 1), ('故國川王', 1), ('敬孝', 1)]...\n",
      "[2022-09-14 19:35:04,041] keeping 2000000 tokens which were in no less than 0 and no more than 1740000 (=100.0%) documents\n",
      "[2022-09-14 19:35:08,818] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:35:08,905] adding document #1740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:35:33,977] discarding 32156 tokens: [('開心寺', 1), ('陽和寺', 1), ('雙磎寺', 1), ('雲住寺', 1), ('靈鷲寺', 1), ('鳳停寺', 1), ('龍興寺', 1), ('龍華寺', 1), ('강서사', 1), ('개원사', 1)]...\n",
      "[2022-09-14 19:35:33,979] keeping 2000000 tokens which were in no less than 0 and no more than 1750000 (=100.0%) documents\n",
      "[2022-09-14 19:35:38,789] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:35:38,877] adding document #1750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:36:04,406] discarding 29530 tokens: [('eungye', 1), ('geumam', 1), ('jigot', 1), ('naesammi', 1), ('nueup', 1), ('oesammi', 1), ('sucheong', 1), ('伐音洞', 1), ('佳水洞', 1), ('佳長洞', 1)]...\n",
      "[2022-09-14 19:36:04,408] keeping 2000000 tokens which were in no less than 0 and no more than 1760000 (=100.0%) documents\n",
      "[2022-09-14 19:36:07,796] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:36:07,854] adding document #1760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:36:33,289] discarding 31611 tokens: [('ferraniacolor_r', 1), ('slide_archive_hg', 1), ('slide_frame_', 1), ('freinadametz', 1), ('giovanmattia', 1), ('hartwoch', 1), ('missionstätigkeit', 1), ('plutz', 1), ('sacredheartparishkamuningjf', 1), ('taichia', 1)]...\n",
      "[2022-09-14 19:36:33,291] keeping 2000000 tokens which were in no less than 0 and no more than 1770000 (=100.0%) documents\n",
      "[2022-09-14 19:36:36,696] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:36:36,754] adding document #1770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:37:03,419] discarding 35418 tokens: [('gynocardin', 1), ('hahniopathanthus', 1), ('holrungiella', 1), ('isoshaftoside', 1), ('krishnakamala', 1), ('lilikoʻi', 1), ('lucenin', 1), ('lutenin', 1), ('octandranthus', 1), ('pardifolia', 1)]...\n",
      "[2022-09-14 19:37:03,421] keeping 2000000 tokens which were in no less than 0 and no more than 1780000 (=100.0%) documents\n",
      "[2022-09-14 19:37:08,222] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:37:08,310] adding document #1780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:37:37,842] discarding 29054 tokens: [('raviceps', 1), ('rhutidira', 1), ('roquetus', 1), ('stictigaster', 1), ('diplolæmus', 1), ('liosaurus', 1), ('paronae', 1), ('casuhatien', 1), ('casuhatiensis', 1), ('lamborot', 1)]...\n",
      "[2022-09-14 19:37:37,844] keeping 2000000 tokens which were in no less than 0 and no more than 1790000 (=100.0%) documents\n",
      "[2022-09-14 19:37:41,269] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:37:41,329] adding document #1790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:38:14,638] discarding 26888 tokens: [('etifier', 1), ('rouys', 1), ('nyūryoku', 1), ('墲', 1), ('beiderstädtische', 1), ('landherrenschaft', 1), ('oberbillwerder', 1), ('pferdeschwanz', 1), ('sachsentor', 1), ('krystalán', 1)]...\n",
      "[2022-09-14 19:38:14,640] keeping 2000000 tokens which were in no less than 0 and no more than 1800000 (=100.0%) documents\n",
      "[2022-09-14 19:38:19,790] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:38:19,878] adding document #1800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:38:43,209] discarding 29856 tokens: [('olotaxws', 1), ('paidiki', 1), ('pantelonia', 1), ('papaoikonomou', 1), ('periptwsi', 1), ('polixroniou', 1), ('prwi', 1), ('psithuroi', 1), ('rekupero', 1), ('rougeri', 1)]...\n",
      "[2022-09-14 19:38:43,211] keeping 2000000 tokens which were in no less than 0 and no more than 1810000 (=100.0%) documents\n",
      "[2022-09-14 19:38:48,001] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:38:48,088] adding document #1810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:39:10,927] discarding 30605 tokens: [('metand', 1), ('talentjim', 1), ('talkand', 1), ('trickedwhich', 1), ('soteca', 1), ('abéche', 1), ('awsd', 1), ('bekhtar', 1), ('fadhigaradle', 1), ('foodwatershelter', 1)]...\n",
      "[2022-09-14 19:39:10,929] keeping 2000000 tokens which were in no less than 0 and no more than 1820000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:39:15,721] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:39:15,808] adding document #1820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:39:38,868] discarding 33754 tokens: [('卫视体育台', 1), ('原住民族電視', 1), ('厦门电视台', 1), ('台灣公共廣播電視集團', 1), ('台灣宏觀電視', 1), ('台灣藝術', 1), ('台灣電視', 1), ('合肥电视台', 1), ('吉林市电视台', 1), ('吉林教育电视台', 1)]...\n",
      "[2022-09-14 19:39:38,870] keeping 2000000 tokens which were in no less than 0 and no more than 1830000 (=100.0%) documents\n",
      "[2022-09-14 19:39:43,699] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:39:43,787] adding document #1830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:40:06,324] discarding 29356 tokens: [('额诺氏', 1), ('aksyu', 1), ('dibong', 1), ('duwarmara', 1), ('hpum', 1), ('htem', 1), ('hugawng', 1), ('innao', 1), ('jǐngpōyǔ', 1), ('kumchai', 1)]...\n",
      "[2022-09-14 19:40:06,326] keeping 2000000 tokens which were in no less than 0 and no more than 1840000 (=100.0%) documents\n",
      "[2022-09-14 19:40:11,123] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:40:11,210] adding document #1840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:40:33,615] discarding 28228 tokens: [('mukuẽ', 1), ('mukúi', 1), ('mundurucú', 1), ('mundápa', 1), ('munitaruka', 1), ('musapeire', 1), ('musaput', 1), ('mutírem', 1), ('muén', 1), ('mábé', 1)]...\n",
      "[2022-09-14 19:40:33,616] keeping 2000000 tokens which were in no less than 0 and no more than 1850000 (=100.0%) documents\n",
      "[2022-09-14 19:40:36,903] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:40:36,962] adding document #1850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:40:58,613] discarding 29819 tokens: [('ζύγωμα', 1), ('wintre', 1), ('direcrorate', 1), ('fararr', 1), ('muilne', 1), ('thomasoneil', 1), ('ateneobluecloud', 1), ('ateneojf', 1), ('gbseald', 1), ('jgsom', 1)]...\n",
      "[2022-09-14 19:40:58,615] keeping 2000000 tokens which were in no less than 0 and no more than 1860000 (=100.0%) documents\n",
      "[2022-09-14 19:41:02,341] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:41:02,429] adding document #1860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:41:26,349] discarding 31125 tokens: [('suotiv', 1), ('suucan', 1), ('suyaan', 1), ('tabaon', 1), ('tabolongan', 1), ('tabusao', 1), ('tagalinong', 1), ('tagampol', 1), ('taganongan', 1), ('tagaporo', 1)]...\n",
      "[2022-09-14 19:41:26,350] keeping 2000000 tokens which were in no less than 0 and no more than 1870000 (=100.0%) documents\n",
      "[2022-09-14 19:41:29,690] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:41:29,751] adding document #1870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:41:55,219] discarding 30363 tokens: [('howinson', 1), ('biosource', 1), ('fkps', 1), ('fsph', 1), ('semenggok', 1), ('upmkb', 1), ('mcscoup', 1), ('taravatanians', 1), ('ohanaja', 1), ('cragwich', 1)]...\n",
      "[2022-09-14 19:41:55,221] keeping 2000000 tokens which were in no less than 0 and no more than 1880000 (=100.0%) documents\n",
      "[2022-09-14 19:41:59,911] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:42:00,000] adding document #1880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:42:23,044] discarding 31756 tokens: [('carrswold', 1), ('chalaak', 1), ('chihuhua', 1), ('kupleri', 1), ('purushutam', 1), ('aquapets', 1), ('fizzie', 1), ('floptopus', 1), ('kitzi', 1), ('likabee', 1)]...\n",
      "[2022-09-14 19:42:23,046] keeping 2000000 tokens which were in no less than 0 and no more than 1890000 (=100.0%) documents\n",
      "[2022-09-14 19:42:27,784] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:42:27,873] adding document #1890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:42:52,397] discarding 31994 tokens: [('wboq', 1), ('perlot', 1), ('suzio', 1), ('beirat', 1), ('churmark', 1), ('cornerus', 1), ('stratner', 1), ('chrusciel', 1), ('generalowie', 1), ('kryska', 1)]...\n",
      "[2022-09-14 19:42:52,398] keeping 2000000 tokens which were in no less than 0 and no more than 1900000 (=100.0%) documents\n",
      "[2022-09-14 19:42:56,494] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:42:56,582] adding document #1900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:43:19,772] discarding 29598 tokens: [('erosi', 1), ('kitsmarishvili', 1), ('teneycke', 1), ('adbot', 1), ('mapblast', 1), ('smartad', 1), ('mabben', 1), ('bilgisi', 1), ('demegog', 1), ('doğmalıydı', 1)]...\n",
      "[2022-09-14 19:43:19,774] keeping 2000000 tokens which were in no less than 0 and no more than 1910000 (=100.0%) documents\n",
      "[2022-09-14 19:43:23,117] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:43:23,178] adding document #1910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:43:45,527] discarding 31617 tokens: [('alkheng', 1), ('arakanis', 1), ('bawmlai', 1), ('bawmzo', 1), ('chhawnmanga', 1), ('chinzah', 1), ('chuncung', 1), ('hauhulh', 1), ('hlawnchhing', 1), ('kanpalet', 1)]...\n",
      "[2022-09-14 19:43:45,529] keeping 2000000 tokens which were in no less than 0 and no more than 1920000 (=100.0%) documents\n",
      "[2022-09-14 19:43:50,160] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:43:50,248] adding document #1920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:44:13,815] discarding 49612 tokens: [('alexandoupoli', 1), ('aκtor', 1), ('kratygos', 1), ('αυτοκινητόδρομοι', 1), ('εxpression', 1), ('ιnterest', 1), ('abergs', 1), ('kouzounas', 1), ('kyshia', 1), ('schreibel', 1)]...\n",
      "[2022-09-14 19:44:13,817] keeping 2000000 tokens which were in no less than 0 and no more than 1930000 (=100.0%) documents\n",
      "[2022-09-14 19:44:17,150] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:44:17,211] adding document #1930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:44:39,879] discarding 26546 tokens: [('hamiklat', 1), ('paphnuce', 1), ('ogeed', 1), ('readingdesk', 1), ('hurlbuts', 1), ('wollison', 1), ('assuntina', 1), ('buonanima', 1), ('farfallon', 1), ('farmacista', 1)]...\n",
      "[2022-09-14 19:44:39,881] keeping 2000000 tokens which were in no less than 0 and no more than 1940000 (=100.0%) documents\n",
      "[2022-09-14 19:44:44,586] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:44:44,673] adding document #1940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:45:12,148] discarding 33727 tokens: [('戸田忠昌', 1), ('戸田忠温', 1), ('戸田忠真', 1), ('戸田氏教', 1), ('有馬道純', 1), ('本多忠民', 1), ('本多忠籌', 1), ('本多忠良', 1), ('本多正信', 1), ('本多正永', 1)]...\n",
      "[2022-09-14 19:45:12,150] keeping 2000000 tokens which were in no less than 0 and no more than 1950000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:45:16,868] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:45:16,958] adding document #1950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:45:43,332] discarding 32490 tokens: [('kilwynet', 1), ('netherhill', 1), ('stangrant', 1), ('yewcrest', 1), ('belsuites', 1), ('gudaibya', 1), ('memmar', 1), ('sukoon', 1), ('tijaria', 1), ('okoua', 1)]...\n",
      "[2022-09-14 19:45:43,334] keeping 2000000 tokens which were in no less than 0 and no more than 1960000 (=100.0%) documents\n",
      "[2022-09-14 19:45:48,151] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:45:48,239] adding document #1960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:46:14,140] discarding 32394 tokens: [('arubamu', 1), ('hajimatta', 1), ('nareru', 1), ('tsukuritakute', 1), ('yuiから', 1), ('あれば', 1), ('いつだって', 1), ('このアルバムを', 1), ('その物語は', 1), ('となりで', 1)]...\n",
      "[2022-09-14 19:46:14,142] keeping 2000000 tokens which were in no less than 0 and no more than 1970000 (=100.0%) documents\n",
      "[2022-09-14 19:46:18,872] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:46:18,961] adding document #1970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:46:43,554] discarding 38583 tokens: [('saunakin', 1), ('smrititattva', 1), ('srautasurta', 1), ('staudayana', 1), ('sutrakaras', 1), ('svidh', 1), ('taittirita', 1), ('tamdya', 1), ('taudayana', 1), ('telavakara', 1)]...\n",
      "[2022-09-14 19:46:43,556] keeping 2000000 tokens which were in no less than 0 and no more than 1980000 (=100.0%) documents\n",
      "[2022-09-14 19:46:48,220] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:46:48,310] adding document #1980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:47:14,943] discarding 35721 tokens: [('windowie', 1), ('wooloobidallah', 1), ('kurtid', 1), ('jucos', 1), ('旧制高校', 1), ('전문대학', 1), ('eurasist', 1), ('yevraziya', 1), ('caeacal', 1), ('rackelhahn', 1)]...\n",
      "[2022-09-14 19:47:14,945] keeping 2000000 tokens which were in no less than 0 and no more than 1990000 (=100.0%) documents\n",
      "[2022-09-14 19:47:19,612] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:47:19,703] adding document #1990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:47:49,170] discarding 34524 tokens: [('passelet', 1), ('pæssa', 1), ('aecommunity', 1), ('deansmen', 1), ('georgeshammond', 1), ('merminaders', 1), ('roterized', 1), ('takenote', 1), ('nemelaer', 1), ('unificational', 1)]...\n",
      "[2022-09-14 19:47:49,171] keeping 2000000 tokens which were in no less than 0 and no more than 2000000 (=100.0%) documents\n",
      "[2022-09-14 19:47:53,836] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:47:53,925] adding document #2000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:48:18,992] discarding 33138 tokens: [('ephraimshay', 1), ('glrr', 1), ('longviewlibrary', 1), ('rhsnny', 1), ('shayaccessories', 1), ('shaydrive', 1), ('shaylocomotiveengine', 1), ('butoridesspodiogasterkeulemans', 1), ('chloriceps', 1), ('greenbacked', 1)]...\n",
      "[2022-09-14 19:48:18,995] keeping 2000000 tokens which were in no less than 0 and no more than 2010000 (=100.0%) documents\n",
      "[2022-09-14 19:48:23,755] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:48:23,846] adding document #2010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:48:46,601] discarding 28688 tokens: [('abwegen', 1), ('dollheubel', 1), ('duffon', 1), ('filmdirektor', 1), ('finanzminister', 1), ('gefängnisdirektor', 1), ('hollberg', 1), ('konservator', 1), ('marczek', 1), ('scharwitz', 1)]...\n",
      "[2022-09-14 19:48:46,602] keeping 2000000 tokens which were in no less than 0 and no more than 2020000 (=100.0%) documents\n",
      "[2022-09-14 19:48:51,378] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:48:51,468] adding document #2020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:49:14,793] discarding 33568 tokens: [('llwyncelny', 1), ('newcomber', 1), ('nyharborparks', 1), ('scarino', 1), ('tylunas', 1), ('atracos', 1), ('byoukimon', 1), ('clarrysney', 1), ('cucuruchito', 1), ('gourmandines', 1)]...\n",
      "[2022-09-14 19:49:14,795] keeping 2000000 tokens which were in no less than 0 and no more than 2030000 (=100.0%) documents\n",
      "[2022-09-14 19:49:19,525] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:49:19,616] adding document #2030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:49:41,676] discarding 34920 tokens: [('秘伝少林寺拳法', 1), ('突技', 1), ('立無相構え', 1), ('結手構え', 1), ('締技', 1), ('縛法', 1), ('義和九陣', 1), ('義和門拳', 1), ('菩提達磨', 1), ('虚実', 1)]...\n",
      "[2022-09-14 19:49:41,678] keeping 2000000 tokens which were in no less than 0 and no more than 2040000 (=100.0%) documents\n",
      "[2022-09-14 19:49:45,760] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:49:45,849] adding document #2040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:50:09,290] discarding 33278 tokens: [('passiónis', 1), ('pendébat', 1), ('pertransívit', 1), ('plángere', 1), ('præclára', 1), ('recólere', 1), ('sentíre', 1), ('sociáre', 1), ('succénsus', 1), ('supplício', 1)]...\n",
      "[2022-09-14 19:50:09,292] keeping 2000000 tokens which were in no less than 0 and no more than 2050000 (=100.0%) documents\n",
      "[2022-09-14 19:50:13,969] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:50:14,059] adding document #2050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:50:39,403] discarding 36942 tokens: [('ierápetra', 1), ('kolektos', 1), ('athenodrus', 1), ('screamparks', 1), ('shinnishihara', 1), ('mirtoscrete', 1), ('modernizerovany', 1), ('promontory_summit_nps', 1), ('caemb', 1), ('camestone', 1)]...\n",
      "[2022-09-14 19:50:39,405] keeping 2000000 tokens which were in no less than 0 and no more than 2060000 (=100.0%) documents\n",
      "[2022-09-14 19:50:43,245] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:50:43,337] adding document #2060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:51:07,466] discarding 32348 tokens: [('gncv', 1), ('shalvata', 1), ('konstapel', 1), ('maskinist', 1), ('squadleaders', 1), ('genapol', 1), ('attingalaru', 1), ('chemunjimotta', 1), ('kollampuzha', 1), ('kollampuzhayaru', 1)]...\n",
      "[2022-09-14 19:51:07,468] keeping 2000000 tokens which were in no less than 0 and no more than 2070000 (=100.0%) documents\n",
      "[2022-09-14 19:51:12,168] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:51:12,259] adding document #2070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:51:37,799] discarding 32206 tokens: [('brodesky', 1), ('coffeeweeds', 1), ('bernhardson', 1), ('olness', 1), ('manfteuffel', 1), ('öberrhein', 1), ('borrebach', 1), ('breutzman', 1), ('dcloud', 1), ('iamwar', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:51:37,800] keeping 2000000 tokens which were in no less than 0 and no more than 2080000 (=100.0%) documents\n",
      "[2022-09-14 19:51:42,643] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:51:42,742] adding document #2080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:52:06,137] discarding 32849 tokens: [('borsului', 1), ('bumbacului', 1), ('centrall', 1), ('cordau', 1), ('cursei', 1), ('daliei', 1), ('lucrătoare', 1), ('magnoliei', 1), ('mardarescu', 1), ('nelucrătoare', 1)]...\n",
      "[2022-09-14 19:52:06,140] keeping 2000000 tokens which were in no less than 0 and no more than 2090000 (=100.0%) documents\n",
      "[2022-09-14 19:52:11,013] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:52:11,122] adding document #2090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:52:37,154] discarding 39078 tokens: [('zijnenederlandsche', 1), ('henneton', 1), ('administratorn', 1), ('briegk', 1), ('durchlauchten', 1), ('gräffin', 1), ('hercinie', 1), ('hochgebornen', 1), ('intermonats', 1), ('lobgetichte', 1)]...\n",
      "[2022-09-14 19:52:37,156] keeping 2000000 tokens which were in no less than 0 and no more than 2100000 (=100.0%) documents\n",
      "[2022-09-14 19:52:41,760] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:52:41,832] adding document #2100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:53:07,037] discarding 33466 tokens: [('harold_kroto_', 1), ('hashimzai', 1), ('seerc', 1), ('strategakis', 1), ('angioparalyticia', 1), ('civamide', 1), ('erythroprosopalgia', 1), ('amfert', 1), ('aradnik', 1), ('habulbusim', 1)]...\n",
      "[2022-09-14 19:53:07,039] keeping 2000000 tokens which were in no less than 0 and no more than 2110000 (=100.0%) documents\n",
      "[2022-09-14 19:53:11,756] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:53:11,846] adding document #2110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:53:35,030] discarding 35930 tokens: [('quirogalestes', 1), ('quizkids', 1), ('microbiotheriids', 1), ('dasyurmorphs', 1), ('gadiyuli', 1), ('timmulvaneyi', 1), ('caenolestoides', 1), ('gaimanlestes', 1), ('tripotamicus', 1), ('altruim', 1)]...\n",
      "[2022-09-14 19:53:35,032] keeping 2000000 tokens which were in no less than 0 and no more than 2120000 (=100.0%) documents\n",
      "[2022-09-14 19:53:38,372] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:53:38,434] adding document #2120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:54:00,330] discarding 26410 tokens: [('cinnans', 1), ('dictatores', 1), ('spinning#8', 1), ('suffecti', 1), ('aruscity', 1), ('condició', 1), ('finalista', 1), ('flaixtv', 1), ('gosadera', 1), ('gossadera', 1)]...\n",
      "[2022-09-14 19:54:00,331] keeping 2000000 tokens which were in no less than 0 and no more than 2130000 (=100.0%) documents\n",
      "[2022-09-14 19:54:03,655] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:54:03,716] adding document #2130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:54:25,803] discarding 33583 tokens: [('notamï', 1), ('novatorstva', 1), ('nïkh', 1), ('obrzędy', 1), ('odiyi', 1), ('ohlyad', 1), ('partesniy', 1), ('partesny', 1), ('persyds', 1), ('pesennaya', 1)]...\n",
      "[2022-09-14 19:54:25,804] keeping 2000000 tokens which were in no less than 0 and no more than 2140000 (=100.0%) documents\n",
      "[2022-09-14 19:54:30,341] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:54:30,433] adding document #2140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:54:55,809] discarding 52263 tokens: [('nyctophasma', 1), ('inocellata', 1), ('substruens', 1), ('manageed', 1), ('indenta', 1), ('flugmotorenzylindern', 1), ('wärmebeherrschung', 1), ('assimilina', 1), ('guangdon', 1), ('ochribasis', 1)]...\n",
      "[2022-09-14 19:54:55,811] keeping 2000000 tokens which were in no less than 0 and no more than 2150000 (=100.0%) documents\n",
      "[2022-09-14 19:54:59,112] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:54:59,175] adding document #2150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:55:24,463] discarding 34804 tokens: [('ircuits', 1), ('irsim', 1), ('rtwork', 1), ('insipids', 1), ('bohémský', 1), ('moravanů', 1), ('moravljane', 1), ('moraváci', 1), ('favouredly', 1), ('liturgize', 1)]...\n",
      "[2022-09-14 19:55:24,465] keeping 2000000 tokens which were in no less than 0 and no more than 2160000 (=100.0%) documents\n",
      "[2022-09-14 19:55:29,596] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:55:29,689] adding document #2160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:55:53,388] discarding 34052 tokens: [('frostatingseglet', 1), ('naumdølafylki', 1), ('norðmørafylki', 1), ('ulögum', 1), ('øyða', 1), ('earthhopenetwork', 1), ('國文', 1), ('國音字典', 1), ('羅馬字母拼音研究委員會', 1), ('jeništová', 1)]...\n",
      "[2022-09-14 19:55:53,391] keeping 2000000 tokens which were in no less than 0 and no more than 2170000 (=100.0%) documents\n",
      "[2022-09-14 19:55:58,110] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:55:58,200] adding document #2170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:56:22,204] discarding 36602 tokens: [('stracyzynski', 1), ('apothecariorum', 1), ('bohemoslovenica', 1), ('concordie', 1), ('dispensariums', 1), ('farmakope', 1), ('gyógyszerkönyv', 1), ('liekopis', 1), ('lékopis', 1), ('pharmacopieas', 1)]...\n",
      "[2022-09-14 19:56:22,206] keeping 2000000 tokens which were in no less than 0 and no more than 2180000 (=100.0%) documents\n",
      "[2022-09-14 19:56:25,541] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:56:25,604] adding document #2180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:56:48,378] discarding 28789 tokens: [('lentigratum', 1), ('numuloidal', 1), ('oncoceratidae', 1), ('lightningpanthers', 1), ('motusbank', 1), ('ticinense', 1), ('tolta', 1), ('birlem', 1), ('brisbee', 1), ('espar', 1)]...\n",
      "[2022-09-14 19:56:48,380] keeping 2000000 tokens which were in no less than 0 and no more than 2190000 (=100.0%) documents\n",
      "[2022-09-14 19:56:52,100] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:56:52,191] adding document #2190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:57:14,570] discarding 30054 tokens: [('tenuimaculatus', 1), ('vitticauda', 1), ('vulpeculus', 1), ('catharsius', 1), ('coprini', 1), ('metacatharsius', 1), ('necrophagy', 1), ('pedotrophic', 1), ('oxytellus', 1), ('gamezi', 1)]...\n",
      "[2022-09-14 19:57:14,572] keeping 2000000 tokens which were in no less than 0 and no more than 2200000 (=100.0%) documents\n",
      "[2022-09-14 19:57:17,912] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:57:17,975] adding document #2200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 19:57:43,005] discarding 29536 tokens: [('allog', 1), ('shawgo', 1), ('bakon', 1), ('loginovo', 1), ('uptegrove', 1), ('anisothecium', 1), ('aongstroemia', 1), ('aongstroemiopsis', 1), ('braunfelsia', 1), ('brotherobryum', 1)]...\n",
      "[2022-09-14 19:57:43,007] keeping 2000000 tokens which were in no less than 0 and no more than 2210000 (=100.0%) documents\n",
      "[2022-09-14 19:57:46,369] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:57:46,431] adding document #2210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:58:13,224] discarding 34170 tokens: [('attekum', 1), ('babjë', 1), ('beqarit', 1), ('bëgarit', 1), ('candaviae', 1), ('chatzopulos', 1), ('diffèrent', 1), ('dirrachium', 1), ('dyrracgium', 1), ('egnatía', 1)]...\n",
      "[2022-09-14 19:58:13,225] keeping 2000000 tokens which were in no less than 0 and no more than 2220000 (=100.0%) documents\n",
      "[2022-09-14 19:58:17,898] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:58:17,961] adding document #2220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:58:50,369] discarding 34339 tokens: [('中國傳世經典名劇', 1), ('任劍輝', 1), ('倩女奇冤', 1), ('傅娟', 1), ('元曲選', 1), ('廉訪使', 1), ('張父', 1), ('張驢兒', 1), ('桃杌', 1), ('民間傳奇', 1)]...\n",
      "[2022-09-14 19:58:50,371] keeping 2000000 tokens which were in no less than 0 and no more than 2230000 (=100.0%) documents\n",
      "[2022-09-14 19:58:55,482] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:58:55,574] adding document #2230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:59:22,298] discarding 33466 tokens: [('omnisql', 1), ('readpast', 1), ('sybsystemdb', 1), ('sybsystemprocs', 1), ('romaneczko', 1), ('frakulator', 1), ('kinear', 1), ('knockentinnel', 1), ('gordaq', 1), ('paxarotti', 1)]...\n",
      "[2022-09-14 19:59:22,299] keeping 2000000 tokens which were in no less than 0 and no more than 2240000 (=100.0%) documents\n",
      "[2022-09-14 19:59:26,257] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:59:26,350] adding document #2240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:59:49,004] discarding 49057 tokens: [('грядущее', 1), ('озарил', 1), ('отчизну', 1), ('поведём', 1), ('подлых', 1), ('поднял', 1), ('растили', 1), ('решаем', 1), ('сияло', 1), ('сметём', 1)]...\n",
      "[2022-09-14 19:59:49,006] keeping 2000000 tokens which were in no less than 0 and no more than 2250000 (=100.0%) documents\n",
      "[2022-09-14 19:59:53,744] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 19:59:53,839] adding document #2250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:00:19,146] discarding 37127 tokens: [('arrenamento', 1), ('besoldung', 1), ('borofcich', 1), ('cheatechoes', 1), ('contiles', 1), ('damaskios', 1), ('dategli', 1), ('elefanta', 1), ('erost', 1), ('ersomixes', 1)]...\n",
      "[2022-09-14 20:00:19,148] keeping 2000000 tokens which were in no less than 0 and no more than 2260000 (=100.0%) documents\n",
      "[2022-09-14 20:00:22,501] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:00:22,565] adding document #2260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:00:46,764] discarding 37850 tokens: [('sanbōshū', 1), ('sennyūji', 1), ('shinanshoji', 1), ('zenmui', 1), ('zuishinin', 1), ('朝護孫子寺', 1), ('griseras', 1), ('badenhausens', 1), ('deuhs', 1), ('akhdut', 1)]...\n",
      "[2022-09-14 20:00:46,766] keeping 2000000 tokens which were in no less than 0 and no more than 2270000 (=100.0%) documents\n",
      "[2022-09-14 20:00:50,693] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:00:50,756] adding document #2270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:01:14,652] discarding 35739 tokens: [('boγda', 1), ('dayičing', 1), ('daγurisγaγsan', 1), ('dàqīng', 1), ('erdemui', 1), ('gyeongseok', 1), ('sambatnaru', 1), ('大淸皇帝功德碑', 1), ('大清皇帝功德碑毁損', 1), ('conlegae', 1)]...\n",
      "[2022-09-14 20:01:14,654] keeping 2000000 tokens which were in no less than 0 and no more than 2280000 (=100.0%) documents\n",
      "[2022-09-14 20:01:19,755] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:01:19,848] adding document #2280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:01:43,715] discarding 33177 tokens: [('sugera', 1), ('sujunctive', 1), ('suspendió', 1), ('sănătos', 1), ('teithiwn', 1), ('tektebi', 1), ('teriam', 1), ('teríeis', 1), ('thathar', 1), ('tiverdes', 1)]...\n",
      "[2022-09-14 20:01:43,717] keeping 2000000 tokens which were in no less than 0 and no more than 2290000 (=100.0%) documents\n",
      "[2022-09-14 20:01:47,061] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:01:47,124] adding document #2290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:02:12,740] discarding 35473 tokens: [('heguanier', 1), ('adamcalc', 1), ('adamlink', 1), ('adamnet', 1), ('microfox', 1), ('orphanware', 1), ('smartwriter', 1), ('midradii', 1), ('alétophile', 1), ('barmécides', 1)]...\n",
      "[2022-09-14 20:02:12,741] keeping 2000000 tokens which were in no less than 0 and no more than 2300000 (=100.0%) documents\n",
      "[2022-09-14 20:02:17,196] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:02:17,290] adding document #2300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:02:41,234] discarding 34413 tokens: [('mailartists', 1), ('telenetlink', 1), ('amebozoa', 1), ('barophiles', 1), ('melanopyrus', 1), ('amountsand', 1), ('elevationsan', 1), ('fossilsincluding', 1), ('nimbusa', 1), ('onahu', 1)]...\n",
      "[2022-09-14 20:02:41,235] keeping 2000000 tokens which were in no less than 0 and no more than 2310000 (=100.0%) documents\n",
      "[2022-09-14 20:02:44,623] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:02:44,687] adding document #2310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:03:07,794] discarding 40975 tokens: [('camaloch', 1), ('archiboldia', 1), ('bowerbuilding', 1), ('ptilonorhynchid', 1), ('fawncy', 1), ('heftis', 1), ('icqx', 1), ('iculig', 1), ('icuq', 1), ('kwoifq', 1)]...\n",
      "[2022-09-14 20:03:07,796] keeping 2000000 tokens which were in no less than 0 and no more than 2320000 (=100.0%) documents\n",
      "[2022-09-14 20:03:12,996] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:03:13,098] adding document #2320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:03:36,382] discarding 34289 tokens: [('alkhee', 1), ('caupaṛ', 1), ('cowri', 1), ('hsuwheb', 1), ('mundhree', 1), ('mundhreee', 1), ('peghedu', 1), ('sogthi', 1), ('sundhree', 1), ('thoree', 1)]...\n",
      "[2022-09-14 20:03:36,384] keeping 2000000 tokens which were in no less than 0 and no more than 2330000 (=100.0%) documents\n",
      "[2022-09-14 20:03:41,275] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:03:41,374] adding document #2330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:04:04,747] discarding 34975 tokens: [('brounian', 1), ('indiebound', 1), ('trendspotter', 1), ('eyca', 1), ('geodiscounts', 1), ('jugendkarte', 1), ('bukharinites', 1), ('churikov', 1), ('dometian', 1), ('eniseisk', 1)]...\n",
      "[2022-09-14 20:04:04,749] keeping 2000000 tokens which were in no less than 0 and no more than 2340000 (=100.0%) documents\n",
      "[2022-09-14 20:04:08,077] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:04:08,140] adding document #2340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:04:33,371] discarding 35207 tokens: [('lseu', 1), ('jegbefume', 1), ('kanape', 1), ('azafrancillo', 1), ('ditaxin', 1), ('heterantha', 1), ('heterathin', 1), ('carrione', 1), ('cautum', 1), ('cemorem', 1)]...\n",
      "[2022-09-14 20:04:33,373] keeping 2000000 tokens which were in no less than 0 and no more than 2350000 (=100.0%) documents\n",
      "[2022-09-14 20:04:38,129] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:04:38,222] adding document #2350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:05:03,939] discarding 29249 tokens: [('barworth', 1), ('hiltachk', 1), ('marsalee', 1), ('ncvli', 1), ('btum', 1), ('cooccurrences', 1), ('niversité', 1), ('nologie', 1), ('ontréal', 1), ('ballyhooigans', 1)]...\n",
      "[2022-09-14 20:05:03,941] keeping 2000000 tokens which were in no less than 0 and no more than 2360000 (=100.0%) documents\n",
      "[2022-09-14 20:05:07,289] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:05:07,351] adding document #2360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:05:29,346] discarding 29904 tokens: [('bluntseed', 1), ('shockblast', 1), ('bangyi', 1), ('gonandarit', 1), ('hlayga', 1), ('htauk', 1), ('kanyin', 1), ('lanbu', 1), ('laungshay', 1), ('legaing', 1)]...\n",
      "[2022-09-14 20:05:29,348] keeping 2000000 tokens which were in no less than 0 and no more than 2370000 (=100.0%) documents\n",
      "[2022-09-14 20:05:32,686] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:05:32,749] adding document #2370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:05:59,011] discarding 31650 tokens: [('caputitla', 1), ('embriz', 1), ('madaí', 1), ('rochín', 1), ('zayrik', 1), ('anorthopygidae', 1), ('coenholectypidae', 1), ('discoididae', 1), ('holectypidae', 1), ('holectypoida', 1)]...\n",
      "[2022-09-14 20:05:59,013] keeping 2000000 tokens which were in no less than 0 and no more than 2380000 (=100.0%) documents\n",
      "[2022-09-14 20:06:03,880] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:06:03,974] adding document #2380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:06:29,288] discarding 34315 tokens: [('herbarist', 1), ('høgre', 1), ('chathuruthy', 1), ('geevargese', 1), ('hidayatulla', 1), ('ivanius', 1), ('kunnassery', 1), ('kurilos', 1), ('palakkunnath', 1), ('pallathetta', 1)]...\n",
      "[2022-09-14 20:06:29,290] keeping 2000000 tokens which were in no less than 0 and no more than 2390000 (=100.0%) documents\n",
      "[2022-09-14 20:06:33,714] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:06:33,808] adding document #2390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:06:56,136] discarding 34899 tokens: [('arduinome', 1), ('fabber', 1), ('lulzbot', 1), ('makerplane', 1), ('multimachine', 1), ('netduino', 1), ('netfpga', 1), ('nitrokey', 1), ('nodemcu', 1), ('openbci', 1)]...\n",
      "[2022-09-14 20:06:56,138] keeping 2000000 tokens which were in no less than 0 and no more than 2400000 (=100.0%) documents\n",
      "[2022-09-14 20:07:01,262] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:07:01,357] adding document #2400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:07:25,746] discarding 34487 tokens: [('küpü', 1), ('halap', 1), ('cosentry', 1), ('masewicz', 1), ('scriptpro', 1), ('sponsorpro', 1), ('ekserdjian', 1), ('perienne', 1), ('queerarts', 1), ('stephaine', 1)]...\n",
      "[2022-09-14 20:07:25,755] keeping 2000000 tokens which were in no less than 0 and no more than 2410000 (=100.0%) documents\n",
      "[2022-09-14 20:07:30,534] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:07:30,626] adding document #2410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:07:53,997] discarding 33072 tokens: [('yutagami', 1), ('memhet', 1), ('yashiroda', 1), ('furutsu', 1), ('satsukino', 1), ('ogikawa', 1), ('亀田駅西口駅前', 1), ('biehe', 1), ('dazui', 1), ('hanwen', 1)]...\n",
      "[2022-09-14 20:07:53,999] keeping 2000000 tokens which were in no less than 0 and no more than 2420000 (=100.0%) documents\n",
      "[2022-09-14 20:07:58,799] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:07:58,893] adding document #2420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:08:22,876] discarding 37218 tokens: [('käkelä', 1), ('getdistance', 1), ('maxiteration', 1), ('casarett', 1), ('degenholtz', 1), ('dianeemeier', 1), ('fischberg', 1), ('leenay', 1), ('zuroski', 1), ('abrahamsdatter', 1)]...\n",
      "[2022-09-14 20:08:22,877] keeping 2000000 tokens which were in no less than 0 and no more than 2430000 (=100.0%) documents\n",
      "[2022-09-14 20:08:26,272] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:08:26,336] adding document #2430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:08:49,729] discarding 36469 tokens: [('afrothismia', 1), ('atollensis', 1), ('budongensis', 1), ('calomala', 1), ('ectochaete', 1), ('grisebachianus', 1), ('lucigaudens', 1), ('parvicentralis', 1), ('proteranthus', 1), ('rukararana', 1)]...\n",
      "[2022-09-14 20:08:49,730] keeping 2000000 tokens which were in no less than 0 and no more than 2440000 (=100.0%) documents\n",
      "[2022-09-14 20:08:53,624] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:08:53,726] adding document #2440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:09:19,145] discarding 28357 tokens: [('hinsal', 1), ('katpuo', 1), ('kulaps', 1), ('papawa', 1), ('mathemagical', 1), ('remik', 1), ('mmoths', 1), ('christiannorth', 1), ('dakotanew', 1), ('kearneywinona', 1)]...\n",
      "[2022-09-14 20:09:19,147] keeping 2000000 tokens which were in no less than 0 and no more than 2450000 (=100.0%) documents\n",
      "[2022-09-14 20:09:23,155] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:09:23,254] adding document #2450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:09:45,186] discarding 28825 tokens: [('angulately', 1), ('phyxanor', 1), ('subequally', 1), ('siphonala', 1), ('pyrrhogramma', 1), ('subaraneosa', 1), ('turrispira', 1), ('vicella', 1), ('channelle', 1), ('subequidistant', 1)]...\n",
      "[2022-09-14 20:09:45,188] keeping 2000000 tokens which were in no less than 0 and no more than 2460000 (=100.0%) documents\n",
      "[2022-09-14 20:09:50,014] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:09:50,108] adding document #2460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:10:14,788] discarding 32122 tokens: [('cityhallnews', 1), ('full_story', 1), ('queensledger', 1), ('underdog_', 1), ('boroștean', 1), ('bourceanu', 1), ('burdujan', 1), ('calcan', 1), ('chipirliu', 1), ('cojocărel', 1)]...\n",
      "[2022-09-14 20:10:14,790] keeping 2000000 tokens which were in no less than 0 and no more than 2470000 (=100.0%) documents\n",
      "[2022-09-14 20:10:19,743] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:10:19,855] adding document #2470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:10:42,568] discarding 30156 tokens: [('arzika', 1), ('babgana', 1), ('jichuan', 1), ('nabungudu', 1), ('nagarta', 1), ('ojeba', 1), ('conscientization', 1), ('makagon', 1), ('passerson', 1), ('rurual', 1)]...\n",
      "[2022-09-14 20:10:42,570] keeping 2000000 tokens which were in no less than 0 and no more than 2480000 (=100.0%) documents\n",
      "[2022-09-14 20:10:47,324] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:10:47,418] adding document #2480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:11:12,844] discarding 76512 tokens: [('𧷏', 1), ('𧷘', 1), ('𧷚', 1), ('𧷬', 1), ('𧷵', 1), ('𧷸', 1), ('𧷹', 1), ('𧸀', 1), ('𧸁', 1), ('𧸂', 1)]...\n",
      "[2022-09-14 20:11:12,846] keeping 2000000 tokens which were in no less than 0 and no more than 2490000 (=100.0%) documents\n",
      "[2022-09-14 20:11:17,544] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:11:17,643] adding document #2490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:11:44,911] discarding 42374 tokens: [('empaire', 1), ('endocrino', 1), ('fonomecanografia', 1), ('hemodinamia', 1), ('hibernado', 1), ('honorifica', 1), ('isquemic', 1), ('isquemica', 1), ('isquémica', 1), ('mencion', 1)]...\n",
      "[2022-09-14 20:11:44,913] keeping 2000000 tokens which were in no less than 0 and no more than 2500000 (=100.0%) documents\n",
      "[2022-09-14 20:11:48,299] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:11:48,365] adding document #2500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:12:14,539] discarding 36727 tokens: [('기아대교앞', 1), ('낙수', 1), ('남양저수지', 1), ('내화', 1), ('논산시보건소논산종합사회복지관', 1), ('농공단지삼거리', 1), ('농성역', 1), ('다시공용버스터미널', 1), ('다시교', 1), ('단전', 1)]...\n",
      "[2022-09-14 20:12:14,541] keeping 2000000 tokens which were in no less than 0 and no more than 2510000 (=100.0%) documents\n",
      "[2022-09-14 20:12:18,764] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:12:18,858] adding document #2510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:12:42,854] discarding 34314 tokens: [('sudár', 1), ('beenak', 1), ('chaostola', 1), ('yabbimunna', 1), ('begrep', 1), ('entall', 1), ('høyde', 1), ('kontrapunktisk', 1), ('løsøre', 1), ('naturligvis', 1)]...\n",
      "[2022-09-14 20:12:42,855] keeping 2000000 tokens which were in no less than 0 and no more than 2520000 (=100.0%) documents\n",
      "[2022-09-14 20:12:46,250] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:12:46,315] adding document #2520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:13:09,603] discarding 34903 tokens: [('carliner', 1), ('wanken', 1), ('coolmain', 1), ('chornohus', 1), ('lutzak', 1), ('michalcheon', 1), ('mihalcheon', 1), ('mihălcean', 1), ('dehado', 1), ('magmahal', 1)]...\n",
      "[2022-09-14 20:13:09,604] keeping 2000000 tokens which were in no less than 0 and no more than 2530000 (=100.0%) documents\n",
      "[2022-09-14 20:13:12,989] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:13:13,053] adding document #2530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:13:35,196] discarding 30630 tokens: [('skewville', 1), ('uncomissioned', 1), ('bonatstraße', 1), ('helmboldstraße', 1), ('nordelbien', 1), ('bimoto', 1), ('caravanner', 1), ('einride', 1), ('essesse', 1), ('finishline', 1)]...\n",
      "[2022-09-14 20:13:35,199] keeping 2000000 tokens which were in no less than 0 and no more than 2540000 (=100.0%) documents\n",
      "[2022-09-14 20:13:39,972] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:13:40,065] adding document #2540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:14:02,476] discarding 31041 tokens: [('tamlaghoskutt', 1), ('tammaskey', 1), ('tamneosker', 1), ('tamneyaskey', 1), ('tateneosker', 1), ('tauniagher', 1), ('tawnaosker', 1), ('tawneosker', 1), ('tomnasker', 1), ('townasker', 1)]...\n",
      "[2022-09-14 20:14:02,478] keeping 2000000 tokens which were in no less than 0 and no more than 2550000 (=100.0%) documents\n",
      "[2022-09-14 20:14:07,246] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:14:07,339] adding document #2550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:14:36,596] discarding 35711 tokens: [('vohnja', 1), ('bsra', 1), ('mundeseley', 1), ('cinetto', 1), ('antoniucci', 1), ('bluesmusician', 1), ('sidneywells', 1), ('voltigern', 1), ('aranyfüst', 1), ('aranyifjú', 1)]...\n",
      "[2022-09-14 20:14:36,598] keeping 2000000 tokens which were in no less than 0 and no more than 2560000 (=100.0%) documents\n",
      "[2022-09-14 20:14:41,234] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:14:41,329] adding document #2560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:15:09,181] discarding 37685 tokens: [('tridentis', 1), ('trifurcipilus', 1), ('youcefi', 1), ('zaluckii', 1), ('sibiriensis', 1), ('arctorotundus', 1), ('bicaudatus', 1), ('bickleyi', 1), ('bloemfonteinensis', 1), ('contentiosus', 1)]...\n",
      "[2022-09-14 20:15:09,183] keeping 2000000 tokens which were in no less than 0 and no more than 2570000 (=100.0%) documents\n",
      "[2022-09-14 20:15:13,655] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:15:13,749] adding document #2570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:15:39,507] discarding 55471 tokens: [('inchincummer', 1), ('kealduff', 1), ('kilbrean', 1), ('kilbreanbeg', 1), ('kilfeighny', 1), ('kilfelim', 1), ('kilgarriv', 1), ('kilgarrylander', 1), ('kilgulbin', 1), ('kilkeaveragh', 1)]...\n",
      "[2022-09-14 20:15:39,508] keeping 2000000 tokens which were in no less than 0 and no more than 2580000 (=100.0%) documents\n",
      "[2022-09-14 20:15:42,967] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:15:43,034] adding document #2580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:16:07,536] discarding 34048 tokens: [('kjcg', 1), ('kmcj', 1), ('knpc', 1), ('knpj', 1), ('kvcm', 1), ('kxeh', 1), ('tranlsators', 1), ('philippinischen', 1), ('wasseragamen', 1), ('buonavita', 1)]...\n",
      "[2022-09-14 20:16:07,538] keeping 2000000 tokens which were in no less than 0 and no more than 2590000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:16:11,615] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:16:11,710] adding document #2590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:16:35,846] discarding 32116 tokens: [('stephanacius', 1), ('sunnicas', 1), ('āzārethes', 1), ('cramoysin', 1), ('cremisi', 1), ('crymysyn', 1), ('kṛmi', 1), ('čermnyj', 1), ('čruminu', 1), ('чермный', 1)]...\n",
      "[2022-09-14 20:16:35,847] keeping 2000000 tokens which were in no less than 0 and no more than 2600000 (=100.0%) documents\n",
      "[2022-09-14 20:16:40,607] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:16:40,703] adding document #2600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:17:04,506] discarding 28177 tokens: [('อนซ', 1), ('อๆ', 1), ('เกาเอาเอง', 1), ('bagls', 1), ('floorhouses', 1), ('lamker', 1), ('ammomum', 1), ('chryseatos', 1), ('nobleorange', 1), ('aghaindum', 1)]...\n",
      "[2022-09-14 20:17:04,508] keeping 2000000 tokens which were in no less than 0 and no more than 2610000 (=100.0%) documents\n",
      "[2022-09-14 20:17:08,308] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:17:08,378] adding document #2610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:17:31,009] discarding 27769 tokens: [('затвореник', 1), ('затвореникот', 1), ('затворникът', 1), ('кімната', 1), ('мудроста', 1), ('напівкровний', 1), ('нечистокръвния', 1), ('нууцат', 1), ('огненият', 1), ('одајата', 1)]...\n",
      "[2022-09-14 20:17:31,010] keeping 2000000 tokens which were in no less than 0 and no more than 2620000 (=100.0%) documents\n",
      "[2022-09-14 20:17:34,757] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:17:34,851] adding document #2620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:18:03,370] discarding 29424 tokens: [('mädchenfeinde', 1), ('parlamentär', 1), ('corynth', 1), ('nomokamon', 1), ('alternatelydone', 1), ('boosway', 1), ('mountainsdone', 1), ('nonprostitute', 1), ('prostitutors', 1), ('xxxvideos', 1)]...\n",
      "[2022-09-14 20:18:03,372] keeping 2000000 tokens which were in no less than 0 and no more than 2630000 (=100.0%) documents\n",
      "[2022-09-14 20:18:08,290] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:18:08,390] adding document #2630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:18:41,571] discarding 36352 tokens: [('imineh', 1), ('lnmh', 1), ('rchdch', 1), ('resorcinolh', 1), ('succpdh', 1), ('xyliphos', 1), ('musikês', 1), ('κοϊντιλιανός', 1), ('ambylodus', 1), ('ananjevi', 1)]...\n",
      "[2022-09-14 20:18:41,572] keeping 2000000 tokens which were in no less than 0 and no more than 2640000 (=100.0%) documents\n",
      "[2022-09-14 20:18:46,452] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:18:46,547] adding document #2640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:19:11,505] discarding 36529 tokens: [('eterias', 1), ('ethikis', 1), ('garidosalata', 1), ('goúna', 1), ('katsoni', 1), ('kavouropilafo', 1), ('klaudon', 1), ('koutzakiotis', 1), ('likourinos', 1), ('moselie', 1)]...\n",
      "[2022-09-14 20:19:11,507] keeping 2000000 tokens which were in no less than 0 and no more than 2650000 (=100.0%) documents\n",
      "[2022-09-14 20:19:16,309] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:19:16,407] adding document #2650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:19:43,070] discarding 34157 tokens: [('gnônai', 1), ('génesin', 1), ('géronta', 1), ('gêras', 1), ('gēraskō', 1), ('heautòn', 1), ('hellē', 1), ('hetérōthi', 1), ('hippokleídēi', 1), ('hippolocus', 1)]...\n",
      "[2022-09-14 20:19:43,072] keeping 2000000 tokens which were in no less than 0 and no more than 2660000 (=100.0%) documents\n",
      "[2022-09-14 20:19:47,842] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:19:47,937] adding document #2660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:20:11,725] discarding 34413 tokens: [('道南いさりび鉄道株式会社', 1), ('遠鉄', 1), ('都電', 1), ('野岩鉄道', 1), ('金沢シーサイドライン', 1), ('鉄道省', 1), ('銚子電気鉄道', 1), ('銚電', 1), ('錦川鉄道', 1), ('長崎電気軌道', 1)]...\n",
      "[2022-09-14 20:20:11,726] keeping 2000000 tokens which were in no less than 0 and no more than 2670000 (=100.0%) documents\n",
      "[2022-09-14 20:20:15,104] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:20:15,168] adding document #2670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:20:39,700] discarding 34962 tokens: [('כפת', 1), ('עצי', 1), ('תמרים', 1), ('superwings', 1), ('workbenchin', 1), ('dagubbati', 1), ('incubency', 1), ('kkhan', 1), ('davenhurst', 1), ('monchamps', 1)]...\n",
      "[2022-09-14 20:20:39,701] keeping 2000000 tokens which were in no less than 0 and no more than 2680000 (=100.0%) documents\n",
      "[2022-09-14 20:20:44,500] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:20:44,597] adding document #2680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:21:10,908] discarding 42112 tokens: [('eggelte', 1), ('engelbewaarder', 1), ('geijerstein', 1), ('mislukkingskunstenaar', 1), ('osewoudt', 1), ('ruisend', 1), ('talloos', 1), ('vraaggesprek', 1), ('zomerplaag', 1), ('otsog', 1)]...\n",
      "[2022-09-14 20:21:10,910] keeping 2000000 tokens which were in no less than 0 and no more than 2690000 (=100.0%) documents\n",
      "[2022-09-14 20:21:14,297] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:21:14,362] adding document #2690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:21:38,182] discarding 34981 tokens: [('peaceruby', 1), ('radiosvyaz', 1), ('troposphären', 1), ('anaprop', 1), ('anoprop', 1), ('garbish', 1), ('aghbatank', 1), ('ktchoyan', 1), ('miatsin', 1), ('monophysis', 1)]...\n",
      "[2022-09-14 20:21:38,183] keeping 2000000 tokens which were in no less than 0 and no more than 2700000 (=100.0%) documents\n",
      "[2022-09-14 20:21:41,588] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:21:41,654] adding document #2700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:22:05,188] discarding 43302 tokens: [('taskless', 1), ('trtf', 1), ('chepekar', 1), ('hunneysett', 1), ('journalix', 1), ('mirrorbingo', 1), ('planetslade', 1), ('upôjela', 1), ('meliška', 1), ('irishmanifestos', 1)]...\n",
      "[2022-09-14 20:22:05,190] keeping 2000000 tokens which were in no less than 0 and no more than 2710000 (=100.0%) documents\n",
      "[2022-09-14 20:22:08,656] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:22:08,723] adding document #2710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:22:35,152] discarding 48140 tokens: [('qugyuq', 1), ('quhmax', 1), ('quineq', 1), ('quiniq', 1), ('qujjuk', 1), ('qukingix', 1), ('qulex', 1), ('qulǝ', 1), ('qussuk', 1), ('qutsuk', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:22:35,154] keeping 2000000 tokens which were in no less than 0 and no more than 2720000 (=100.0%) documents\n",
      "[2022-09-14 20:22:38,564] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:22:38,631] adding document #2720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:23:05,631] discarding 38425 tokens: [('photocyclized', 1), ('triethoxycaprylylsilane', 1), ('藤嶋効果', 1), ('fontiverosa', 1), ('juanelos', 1), ('dependecy', 1), ('presolved', 1), ('prievous', 1), ('truistic', 1), ('acedaemonian', 1)]...\n",
      "[2022-09-14 20:23:05,633] keeping 2000000 tokens which were in no less than 0 and no more than 2730000 (=100.0%) documents\n",
      "[2022-09-14 20:23:09,053] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:23:09,119] adding document #2730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:23:34,439] discarding 34880 tokens: [('nanfri', 1), ('interclast', 1), ('ˈkiːvənɔː', 1), ('alhayki', 1), ('chajek', 1), ('hayeck', 1), ('daihong', 1), ('hmoam', 1), ('ชาวไทในจ', 1), ('uezzell', 1)]...\n",
      "[2022-09-14 20:23:34,440] keeping 2000000 tokens which were in no less than 0 and no more than 2740000 (=100.0%) documents\n",
      "[2022-09-14 20:23:37,860] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:23:37,926] adding document #2740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:24:03,220] discarding 37290 tokens: [('surtitlers', 1), ('barudiyya', 1), ('karawiyya', 1), ('mawlad', 1), ('dodecanuclear', 1), ('néels', 1), ('ηba', 1), ('zürchersee', 1), ('silviasaint', 1), ('tomcalova', 1)]...\n",
      "[2022-09-14 20:24:03,222] keeping 2000000 tokens which were in no less than 0 and no more than 2750000 (=100.0%) documents\n",
      "[2022-09-14 20:24:08,379] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:24:08,475] adding document #2750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:24:34,681] discarding 34362 tokens: [('gretchell', 1), ('kipax', 1), ('klapstuhl', 1), ('wurthering', 1), ('bullös', 1), ('mesotymp', 1), ('mesotympanalis', 1), ('schollig', 1), ('cheapermedia', 1), ('cheapestmedia', 1)]...\n",
      "[2022-09-14 20:24:34,683] keeping 2000000 tokens which were in no less than 0 and no more than 2760000 (=100.0%) documents\n",
      "[2022-09-14 20:24:39,493] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:24:39,590] adding document #2760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:25:04,691] discarding 35725 tokens: [('icositrigonal', 1), ('mathisfunforum', 1), ('myriagonal', 1), ('tetradecagonal', 1), ('esperdi', 1), ('galasti', 1), ('numerosi', 1), ('oratorj', 1), ('rispettosa', 1), ('scaléa', 1)]...\n",
      "[2022-09-14 20:25:04,693] keeping 2000000 tokens which were in no less than 0 and no more than 2770000 (=100.0%) documents\n",
      "[2022-09-14 20:25:09,449] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:25:09,544] adding document #2770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:25:35,394] discarding 35685 tokens: [('amisdepierreleroux', 1), ('fraternitario', 1), ('maîtron', 1), ('plutocratie', 1), ('sozialphilosophischer', 1), ('éclectisme', 1), ('networkrank', 1), ('carbonarist', 1), ('marachuchos', 1), ('orianen', 1)]...\n",
      "[2022-09-14 20:25:35,396] keeping 2000000 tokens which were in no less than 0 and no more than 2780000 (=100.0%) documents\n",
      "[2022-09-14 20:25:38,828] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:25:38,893] adding document #2780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:26:03,746] discarding 36506 tokens: [('quærendæ', 1), ('resipuit', 1), ('sampsone', 1), ('sceicne', 1), ('sobrietate', 1), ('therapeutæ', 1), ('zητήματα', 1), ('θεωρητικός', 1), ('ϓποθετικά', 1), ('ἀλληγορίαι', 1)]...\n",
      "[2022-09-14 20:26:03,748] keeping 2000000 tokens which were in no less than 0 and no more than 2790000 (=100.0%) documents\n",
      "[2022-09-14 20:26:07,165] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:26:07,245] adding document #2790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:26:32,154] discarding 43626 tokens: [('mariehønseår', 1), ('mexicaliearthquakeswarm', 1), ('certaynes', 1), ('covenantes', 1), ('felloweshippe', 1), ('folowen', 1), ('lyncoll', 1), ('suliard', 1), ('pallaphilos', 1), ('papistrye', 1)]...\n",
      "[2022-09-14 20:26:32,156] keeping 2000000 tokens which were in no less than 0 and no more than 2800000 (=100.0%) documents\n",
      "[2022-09-14 20:26:36,980] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:26:37,079] adding document #2800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:27:02,868] discarding 36161 tokens: [('napirasu', 1), ('sherihumians', 1), ('shimashkians', 1), ('shushiana', 1), ('sirukdukh', 1), ('sousiānḗ', 1), ('stån', 1), ('tirutur', 1), ('ulaï', 1), ('zabshalians', 1)]...\n",
      "[2022-09-14 20:27:02,869] keeping 2000000 tokens which were in no less than 0 and no more than 2810000 (=100.0%) documents\n",
      "[2022-09-14 20:27:06,285] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:27:06,351] adding document #2810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:27:31,016] discarding 32040 tokens: [('jasztrebie', 1), ('zofiowka', 1), ('syrkov', 1), ('zverin', 1), ('zverinets', 1), ('groenwegen', 1), ('sieveri', 1), ('dasonomia', 1), ('departemento', 1), ('ecologicos', 1)]...\n",
      "[2022-09-14 20:27:31,018] keeping 2000000 tokens which were in no less than 0 and no more than 2820000 (=100.0%) documents\n",
      "[2022-09-14 20:27:35,821] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:27:35,917] adding document #2820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:28:02,192] discarding 35418 tokens: [('lonstad', 1), ('municipalcouncils', 1), ('nasjonalsocialistiske', 1), ('partyhelsepartiet', 1), ('partynorgespartiet', 1), ('reformpartiet', 1), ('slettli', 1), ('valuesverdipartiet', 1), ('østrådal', 1), ('holboelii', 1)]...\n",
      "[2022-09-14 20:28:02,194] keeping 2000000 tokens which were in no less than 0 and no more than 2830000 (=100.0%) documents\n",
      "[2022-09-14 20:28:07,015] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:28:07,113] adding document #2830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:28:33,658] discarding 36414 tokens: [('highlighed', 1), ('inversives', 1), ('jbačra', 1), ('jirɨmawpʼ', 1), ('jiχɨ', 1), ('jºnә', 1), ('jºənc', 1), ('jºә', 1), ('jəbzә', 1), ('jəq', 1)]...\n",
      "[2022-09-14 20:28:33,660] keeping 2000000 tokens which were in no less than 0 and no more than 2840000 (=100.0%) documents\n",
      "[2022-09-14 20:28:38,660] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:28:38,758] adding document #2840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:29:03,660] discarding 35815 tokens: [('pheṇapiṇḍa', 1), ('sunyatisunya', 1), ('vaināśika', 1), ('śvi', 1), ('śūnyatavāda', 1), ('describedand', 1), ('lügenfabrik', 1), ('hlutre', 1), ('bingaraṭṭha', 1), ('chaiyasettha', 1)]...\n",
      "[2022-09-14 20:29:03,662] keeping 2000000 tokens which were in no less than 0 and no more than 2850000 (=100.0%) documents\n",
      "[2022-09-14 20:29:08,443] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:29:08,541] adding document #2850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:29:36,293] discarding 38317 tokens: [('unqā', 1), ('uthmāniyya', 1), ('yughan', 1), ('إجماع', 1), ('البان', 1), ('الموصلية', 1), ('تكفير', 1), ('تنزلات', 1), ('قضيب', 1), ('والجمال', 1)]...\n",
      "[2022-09-14 20:29:36,295] keeping 2000000 tokens which were in no less than 0 and no more than 2860000 (=100.0%) documents\n",
      "[2022-09-14 20:29:41,044] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:29:41,142] adding document #2860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:30:05,887] discarding 33559 tokens: [('olavinen', 1), ('peoplewatchers', 1), ('stratæ', 1), ('streetnote', 1), ('zastavy', 1), ('alitália', 1), ('buracica', 1), ('garapuá', 1), ('ifbaiano', 1), ('itobira', 1)]...\n",
      "[2022-09-14 20:30:05,889] keeping 2000000 tokens which were in no less than 0 and no more than 2870000 (=100.0%) documents\n",
      "[2022-09-14 20:30:10,677] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:30:10,773] adding document #2870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:30:35,815] discarding 36853 tokens: [('yapsé', 1), ('bawacen', 1), ('buddhasri', 1), ('chegompa', 1), ('drowolung', 1), ('goleg', 1), ('gyaltön', 1), ('gyeltsa', 1), ('kagyus', 1), ('khenkong', 1)]...\n",
      "[2022-09-14 20:30:35,817] keeping 2000000 tokens which were in no less than 0 and no more than 2880000 (=100.0%) documents\n",
      "[2022-09-14 20:30:39,200] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:30:39,266] adding document #2880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:31:05,670] discarding 33404 tokens: [('lessik', 1), ('sarashe', 1), ('löehen', 1), ('ministrables', 1), ('sosick', 1), ('amygist', 1), ('hippopoetess', 1), ('kudalu', 1), ('rougails', 1), ('designwood', 1)]...\n",
      "[2022-09-14 20:31:05,672] keeping 2000000 tokens which were in no less than 0 and no more than 2890000 (=100.0%) documents\n",
      "[2022-09-14 20:31:10,458] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:31:10,554] adding document #2890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:31:35,028] discarding 34037 tokens: [('iggyphiles', 1), ('africi', 1), ('akampton', 1), ('ampiezza', 1), ('apoichae', 1), ('aureolos', 1), ('consurgite', 1), ('corifea', 1), ('corifei', 1), ('crudelior', 1)]...\n",
      "[2022-09-14 20:31:35,029] keeping 2000000 tokens which were in no less than 0 and no more than 2900000 (=100.0%) documents\n",
      "[2022-09-14 20:31:39,891] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:31:40,006] adding document #2900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:32:03,003] discarding 31761 tokens: [('faceplace', 1), ('garnsviken', 1), ('kyrkroin', 1), ('sydväst', 1), ('chōichirō', 1), ('aberglasslyn', 1), ('allynbrook', 1), ('bagwyllydiart', 1), ('brynderwyn', 1), ('bryndwr', 1)]...\n",
      "[2022-09-14 20:32:03,004] keeping 2000000 tokens which were in no less than 0 and no more than 2910000 (=100.0%) documents\n",
      "[2022-09-14 20:32:08,057] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:32:08,185] adding document #2910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:32:35,370] discarding 34861 tokens: [('xambá', 1), ('differuser', 1), ('jr東日本本社ビル', 1), ('hopinn', 1), ('franchenfordia', 1), ('noraky', 1), ('reimmichl', 1), ('stutzinger', 1), ('leggman', 1), ('spoooooon', 1)]...\n",
      "[2022-09-14 20:32:35,372] keeping 2000000 tokens which were in no less than 0 and no more than 2920000 (=100.0%) documents\n",
      "[2022-09-14 20:32:40,243] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:32:40,340] adding document #2920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:33:07,837] discarding 34287 tokens: [('nctblueblaze', 1), ('nctsign', 1), ('lakelogan', 1), ('mentormarsh', 1), ('trailatbendview', 1), ('trailattarhollow', 1), ('ethylacrylate', 1), ('phenylbutanenitrile', 1), ('phenylgutarimide', 1), ('substitutionmittel', 1)]...\n",
      "[2022-09-14 20:33:07,839] keeping 2000000 tokens which were in no less than 0 and no more than 2930000 (=100.0%) documents\n",
      "[2022-09-14 20:33:11,287] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:33:11,354] adding document #2930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:33:35,785] discarding 33010 tokens: [('zhongyǒu', 1), ('zhèndìng', 1), ('zhèngdìng', 1), ('zhèngděng', 1), ('zhèngjiàn', 1), ('zhèngjué', 1), ('zhèngjīngjìn', 1), ('zhèngmìng', 1), ('zhèngsīwéi', 1), ('zhèngyè', 1)]...\n",
      "[2022-09-14 20:33:35,786] keeping 2000000 tokens which were in no less than 0 and no more than 2940000 (=100.0%) documents\n",
      "[2022-09-14 20:33:39,191] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:33:39,257] adding document #2940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:34:06,324] discarding 37040 tokens: [('naturschutzprojekt', 1), ('oberpeiching', 1), ('pegelstände', 1), ('wolfzahnauwehr', 1), ('yoryona', 1), ('chingem', 1), ('aarendsvlei', 1), ('frosler', 1), ('malgraff', 1), ('norodien', 1)]...\n",
      "[2022-09-14 20:34:06,326] keeping 2000000 tokens which were in no less than 0 and no more than 2950000 (=100.0%) documents\n",
      "[2022-09-14 20:34:11,209] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:34:11,307] adding document #2950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:34:36,654] discarding 34324 tokens: [('kiläyde', 1), ('közge', 1), ('közgö', 1), ('küssez', 1), ('larda', 1), ('larnı', 1), ('larnıñ', 1), ('larğa', 1), ('lıs', 1), ('makhmutova', 1)]...\n",
      "[2022-09-14 20:34:36,656] keeping 2000000 tokens which were in no less than 0 and no more than 2960000 (=100.0%) documents\n",
      "[2022-09-14 20:34:41,450] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:34:41,548] adding document #2960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:35:07,942] discarding 39298 tokens: [('ktisti', 1), ('miritoiu', 1), ('polyaneus', 1), ('seuthiiiheroon', 1), ('thraciantribes', 1), ('thrāikes', 1), ('θρηίκιος', 1), ('θρᾴκιος', 1), ('θρῄκη', 1), ('hoodooo', 1)]...\n",
      "[2022-09-14 20:35:07,944] keeping 2000000 tokens which were in no less than 0 and no more than 2970000 (=100.0%) documents\n",
      "[2022-09-14 20:35:12,830] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:35:12,929] adding document #2970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:35:39,129] discarding 35910 tokens: [('retënia', 1), ('schetruarë', 1), ('sepsé', 1), ('shenjtëroftë', 1), ('shpëtoi', 1), ('shpëtó', 1), ('shqipëtarçe', 1), ('shtinë', 1), ('shtiér', 1), ('shënjtnue', 1)]...\n",
      "[2022-09-14 20:35:39,131] keeping 2000000 tokens which were in no less than 0 and no more than 2980000 (=100.0%) documents\n",
      "[2022-09-14 20:35:43,924] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:35:44,022] adding document #2980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:36:10,021] discarding 29565 tokens: [('kshach', 1), ('k舐lek', 1), ('luanma', 1), ('muurdhaja', 1), ('qzyka', 1), ('rgã', 1), ('rvztr', 1), ('rvztűr', 1), ('smã', 1), ('smˆrgâs', 1)]...\n",
      "[2022-09-14 20:36:10,023] keeping 2000000 tokens which were in no less than 0 and no more than 2990000 (=100.0%) documents\n",
      "[2022-09-14 20:36:14,804] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:36:14,899] adding document #2990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:36:40,648] discarding 34549 tokens: [('rehospitalizedfollow', 1), ('risperlet', 1), ('schizomimetic', 1), ('desalkylquetiapine', 1), ('dibenzothiazepinone', 1), ('levelaverage', 1), ('glaurys', 1), ('adamed', 1), ('alonzap', 1), ('amulsin', 1)]...\n",
      "[2022-09-14 20:36:40,649] keeping 2000000 tokens which were in no less than 0 and no more than 3000000 (=100.0%) documents\n",
      "[2022-09-14 20:36:45,457] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:36:45,556] adding document #3000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:37:08,939] discarding 31507 tokens: [('малалур', 1), ('малийна', 1), ('малийнер', 1), ('малийра', 1), ('малийта', 1), ('малийти', 1), ('малийтина', 1), ('малийтинера', 1), ('малийтира', 1), ('мелира', 1)]...\n",
      "[2022-09-14 20:37:08,940] keeping 2000000 tokens which were in no less than 0 and no more than 3010000 (=100.0%) documents\n",
      "[2022-09-14 20:37:14,000] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:37:14,066] adding document #3010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:37:39,066] discarding 36231 tokens: [('adomėnas', 1), ('steponavičius', 1), ('abiriw', 1), ('aboakyere', 1), ('adaekese', 1), ('afutus', 1), ('ahantas', 1), ('ahoboa', 1), ('appoo', 1), ('awukugua', 1)]...\n",
      "[2022-09-14 20:37:39,068] keeping 2000000 tokens which were in no less than 0 and no more than 3020000 (=100.0%) documents\n",
      "[2022-09-14 20:37:43,882] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:37:43,981] adding document #3020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:38:10,120] discarding 29751 tokens: [('hambuchen', 1), ('hindermann', 1), ('padurariu', 1), ('varinska', 1), ('ballegoijen', 1), ('battlejuice', 1), ('caylana', 1), ('musicload', 1), ('popakademie', 1), ('popmusicdesign', 1)]...\n",
      "[2022-09-14 20:38:10,122] keeping 2000000 tokens which were in no less than 0 and no more than 3030000 (=100.0%) documents\n",
      "[2022-09-14 20:38:15,289] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:38:15,385] adding document #3030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:38:41,590] discarding 40891 tokens: [('viscidity', 1), ('ngatas', 1), ('wangata', 1), ('endarkened', 1), ('tavvy', 1), ('aplombov', 1), ('apposito', 1), ('buonanima', 1), ('ciulian', 1), ('cugine', 1)]...\n",
      "[2022-09-14 20:38:41,592] keeping 2000000 tokens which were in no less than 0 and no more than 3040000 (=100.0%) documents\n",
      "[2022-09-14 20:38:46,565] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:38:46,665] adding document #3040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:39:11,985] discarding 34549 tokens: [('dhull', 1), ('julana', 1), ('kiloi', 1), ('biascica', 1), ('patané', 1), ('ellerbusches', 1), ('honerkamp', 1), ('jakkaphan', 1), ('kaewprom', 1), ('kmotrík', 1)]...\n",
      "[2022-09-14 20:39:11,987] keeping 2000000 tokens which were in no less than 0 and no more than 3050000 (=100.0%) documents\n",
      "[2022-09-14 20:39:16,815] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:39:16,912] adding document #3050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:39:43,112] discarding 33696 tokens: [('afsheen', 1), ('chebacca', 1), ('fixyn', 1), ('gaviati', 1), ('klepticenter', 1), ('playfightvfxfilmed', 1), ('rowleygaffer', 1), ('skrubeeffects', 1), ('sprzedaży', 1), ('stirlingdevin', 1)]...\n",
      "[2022-09-14 20:39:43,114] keeping 2000000 tokens which were in no less than 0 and no more than 3060000 (=100.0%) documents\n",
      "[2022-09-14 20:39:46,529] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:39:46,595] adding document #3060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:40:13,612] discarding 32468 tokens: [('leutge', 1), ('deepabai', 1), ('derhadi', 1), ('devalgaon', 1), ('elur', 1), ('ghorpades', 1), ('hingni', 1), ('jadhavs', 1), ('jagpalrao', 1), ('jategau', 1)]...\n",
      "[2022-09-14 20:40:13,613] keeping 2000000 tokens which were in no less than 0 and no more than 3070000 (=100.0%) documents\n",
      "[2022-09-14 20:40:18,415] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:40:18,513] adding document #3070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:40:51,682] discarding 34356 tokens: [('ballymacaw', 1), ('rathmoylan', 1), ('berzunza', 1), ('escárrega', 1), ('kerlegand', 1), ('paquín', 1), ('carliner', 1), ('stitiler', 1), ('derrinsallow', 1), ('doirín', 1)]...\n",
      "[2022-09-14 20:40:51,683] keeping 2000000 tokens which were in no less than 0 and no more than 3080000 (=100.0%) documents\n",
      "[2022-09-14 20:40:55,952] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:40:56,050] adding document #3080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:41:20,708] discarding 34148 tokens: [('neurodegenrative', 1), ('breakshot', 1), ('ignizio', 1), ('eliotglassheim', 1), ('johnhoeven', 1), ('joaosinho', 1), ('joãosinho', 1), ('awekening', 1), ('oklisten', 1), ('porisoy', 1)]...\n",
      "[2022-09-14 20:41:20,710] keeping 2000000 tokens which were in no less than 0 and no more than 3090000 (=100.0%) documents\n",
      "[2022-09-14 20:41:24,148] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:41:24,215] adding document #3090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:41:51,498] discarding 34137 tokens: [('genades', 1), ('hikoku', 1), ('sokuji', 1), ('tuleans', 1), ('artasiamerica', 1), ('陳廷芬', 1), ('commisioner', 1), ('paramedico', 1), ('hairshop', 1), ('handprinting', 1)]...\n",
      "[2022-09-14 20:41:51,500] keeping 2000000 tokens which were in no less than 0 and no more than 3100000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:41:54,972] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:41:55,040] adding document #3100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:42:25,808] discarding 37140 tokens: [('docircle', 1), ('textlocal', 1), ('textmagic', 1), ('trumpia', 1), ('golnur', 1), ('aerocorales', 1), ('nekritz', 1), ('sumpton', 1), ('cerebrocerebellar', 1), ('aparajitapriccha', 1)]...\n",
      "[2022-09-14 20:42:25,809] keeping 2000000 tokens which were in no less than 0 and no more than 3110000 (=100.0%) documents\n",
      "[2022-09-14 20:42:29,296] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:42:29,363] adding document #3110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:42:57,191] discarding 36882 tokens: [('anomalousness', 1), ('kharelthok', 1), ('chabbis', 1), ('goprincetontigers', 1), ('ivyleaguesports', 1), ('teksüt', 1), ('asdefault', 1), ('presentus', 1), ('usebelowbox', 1), ('arabinoofuranose', 1)]...\n",
      "[2022-09-14 20:42:57,193] keeping 2000000 tokens which were in no less than 0 and no more than 3120000 (=100.0%) documents\n",
      "[2022-09-14 20:43:00,675] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:43:00,742] adding document #3120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:43:26,728] discarding 32257 tokens: [('conocos', 1), ('mineland', 1), ('minelands', 1), ('stockpiling#1', 1), ('calcife', 1), ('lilliel', 1), ('shikikagami', 1), ('tarchin', 1), ('yiik', 1), ('maggiolina', 1)]...\n",
      "[2022-09-14 20:43:26,729] keeping 2000000 tokens which were in no less than 0 and no more than 3130000 (=100.0%) documents\n",
      "[2022-09-14 20:43:30,186] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:43:30,252] adding document #3130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:43:58,929] discarding 36234 tokens: [('almersjo', 1), ('amersjo', 1), ('bengmark', 1), ('benjmark', 1), ('fibrocollagenous', 1), ('floydena', 1), ('göteberg', 1), ('korasabengtsen', 1), ('sahlgresnska', 1), ('shireky', 1)]...\n",
      "[2022-09-14 20:43:58,931] keeping 2000000 tokens which were in no less than 0 and no more than 3140000 (=100.0%) documents\n",
      "[2022-09-14 20:44:04,102] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:44:04,201] adding document #3140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:44:33,924] discarding 31720 tokens: [('amangkuratiii', 1), ('madurais', 1), ('celebertti', 1), ('eslaquit', 1), ('amatto', 1), ('gontse', 1), ('kamillah', 1), ('ramarimela', 1), ('trumain', 1), ('kvenipneveli', 1)]...\n",
      "[2022-09-14 20:44:33,927] keeping 2000000 tokens which were in no less than 0 and no more than 3150000 (=100.0%) documents\n",
      "[2022-09-14 20:44:38,783] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:44:38,882] adding document #3150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:45:04,963] discarding 31000 tokens: [('alipato', 1), ('banatan', 1), ('bilinan', 1), ('boobita', 1), ('guding', 1), ('hinalay', 1), ('kabilin', 1), ('manupil', 1), ('oooops', 1), ('pasukob', 1)]...\n",
      "[2022-09-14 20:45:04,965] keeping 2000000 tokens which were in no less than 0 and no more than 3160000 (=100.0%) documents\n",
      "[2022-09-14 20:45:10,167] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:45:10,266] adding document #3160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:45:37,485] discarding 30915 tokens: [('nolanriverbridge', 1), ('antiforma', 1), ('granitoides', 1), ('gravimétrico', 1), ('montancil', 1), ('tamuja', 1), ('tances', 1), ('globularly', 1), ('leptinidae', 1), ('platypsyllidae', 1)]...\n",
      "[2022-09-14 20:45:37,488] keeping 2000000 tokens which were in no less than 0 and no more than 3170000 (=100.0%) documents\n",
      "[2022-09-14 20:45:42,296] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:45:42,393] adding document #3170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:46:07,870] discarding 35095 tokens: [('影狼', 1), ('霊夢', 1), ('ipeh', 1), ('kenzinger', 1), ('wünder', 1), ('waintridge', 1), ('kraicameron', 1), ('unnstein', 1), ('yeoeun', 1), ('yongmasan', 1)]...\n",
      "[2022-09-14 20:46:07,872] keeping 2000000 tokens which were in no less than 0 and no more than 3180000 (=100.0%) documents\n",
      "[2022-09-14 20:46:11,307] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:46:11,374] adding document #3180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:46:38,724] discarding 31191 tokens: [('shaugran', 1), ('ceiad', 1), ('shelembe', 1), ('deeltijdfeminisme', 1), ('eerlijke', 1), ('hulpbetoon', 1), ('schaamte', 1), ('tendeloo', 1), ('vlijtige', 1), ('vrouwenarbeidloten', 1)]...\n",
      "[2022-09-14 20:46:38,726] keeping 2000000 tokens which were in no less than 0 and no more than 3190000 (=100.0%) documents\n",
      "[2022-09-14 20:46:43,634] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:46:43,731] adding document #3190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:47:11,699] discarding 31260 tokens: [('pissboy', 1), ('centralbahnplatz', 1), ('jesenik', 1), ('moravskoslezsky', 1), ('mscb', 1), ('opavou', 1), ('valšov', 1), ('vmoravskoslezském', 1), ('würbenthal', 1), ('alpinizmu', 1)]...\n",
      "[2022-09-14 20:47:11,701] keeping 2000000 tokens which were in no less than 0 and no more than 3200000 (=100.0%) documents\n",
      "[2022-09-14 20:47:15,263] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:47:15,331] adding document #3200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:47:47,608] discarding 34454 tokens: [('avionul', 1), ('azarius', 1), ('batrânsea', 1), ('cuvintul', 1), ('norodnice', 1), ('oţelu', 1), ('tragodia', 1), ('zadnipru', 1), ('ţeh', 1), ('erdemit', 1)]...\n",
      "[2022-09-14 20:47:47,609] keeping 2000000 tokens which were in no less than 0 and no more than 3210000 (=100.0%) documents\n",
      "[2022-09-14 20:47:51,361] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:47:51,428] adding document #3210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:48:17,605] discarding 31369 tokens: [('loisafifth', 1), ('manolomaris', 1), ('marisjane', 1), ('marisnichole', 1), ('michelejane', 1), ('nicholeaina', 1), ('nominatedhousemate', 1), ('rantyaina', 1), ('mávépcel', 1), ('vonal', 1)]...\n",
      "[2022-09-14 20:48:17,607] keeping 2000000 tokens which were in no less than 0 and no more than 3220000 (=100.0%) documents\n",
      "[2022-09-14 20:48:22,440] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:48:22,538] adding document #3220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:48:49,264] discarding 33602 tokens: [('purampokku', 1), ('sevvaname', 1), ('shamanth', 1), ('thaakum', 1), ('tharai_thappattai', 1), ('tholil', 1), ('vellaikaari', 1), ('pxbening', 1), ('biscaccianti', 1), ('toccolini', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:48:49,266] keeping 2000000 tokens which were in no less than 0 and no more than 3230000 (=100.0%) documents\n",
      "[2022-09-14 20:48:54,067] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:48:54,164] adding document #3230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:49:19,696] discarding 37308 tokens: [('jorrie', 1), ('maqwelana', 1), ('passens', 1), ('pietman', 1), ('rheeders', 1), ('volschenk', 1), ('angwinballoon', 1), ('benxianbass', 1), ('burdgecello', 1), ('carnwarth', 1)]...\n",
      "[2022-09-14 20:49:19,698] keeping 2000000 tokens which were in no less than 0 and no more than 3240000 (=100.0%) documents\n",
      "[2022-09-14 20:49:24,546] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:49:24,646] adding document #3240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:49:50,456] discarding 39950 tokens: [('bpirf', 1), ('champscon', 1), ('leywalker', 1), ('meytzuv', 1), ('toadifies', 1), ('basikap', 1), ('berlayam', 1), ('cengkaman', 1), ('dinobatkan', 1), ('fattani', 1)]...\n",
      "[2022-09-14 20:49:50,457] keeping 2000000 tokens which were in no less than 0 and no more than 3250000 (=100.0%) documents\n",
      "[2022-09-14 20:49:54,338] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:49:54,438] adding document #3250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:50:19,740] discarding 34643 tokens: [('deorse', 1), ('devenissent', 1), ('durantur', 1), ('ebenesersdóttir', 1), ('emensosque', 1), ('exiise', 1), ('hamblim', 1), ('howgaard', 1), ('huungti', 1), ('intererant', 1)]...\n",
      "[2022-09-14 20:50:19,742] keeping 2000000 tokens which were in no less than 0 and no more than 3260000 (=100.0%) documents\n",
      "[2022-09-14 20:50:23,190] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:50:23,258] adding document #3260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:50:48,402] discarding 34643 tokens: [('vöppstedt', 1), ('areklew', 1), ('advocatereported', 1), ('hardamar', 1), ('nsrpa', 1), ('spielgrund', 1), ('koehnemann', 1), ('limerents', 1), ('avtorstvo', 1), ('ferenčev', 1)]...\n",
      "[2022-09-14 20:50:48,404] keeping 2000000 tokens which were in no less than 0 and no more than 3270000 (=100.0%) documents\n",
      "[2022-09-14 20:50:53,299] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:50:53,399] adding document #3270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:51:19,505] discarding 35698 tokens: [('taxologically', 1), ('tienshui', 1), ('tiānshuǐ', 1), ('wǔwēi', 1), ('zhāngyè', 1), ('岷县秦许乡', 1), ('拉卜楞寺僧舍俯瞰', 1), ('玛曲', 1), ('鸟瞰去陇南的公路', 1), ('黄河湿地', 1)]...\n",
      "[2022-09-14 20:51:19,507] keeping 2000000 tokens which were in no less than 0 and no more than 3280000 (=100.0%) documents\n",
      "[2022-09-14 20:51:24,333] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:51:24,432] adding document #3280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:51:51,013] discarding 28895 tokens: [('finlaw', 1), ('baradit', 1), ('filsa', 1), ('kaifman', 1), ('cbsongs', 1), ('ciptak', 1), ('throckmortan', 1), ('wnis', 1), ('biomagresbank', 1), ('glycanbuilder', 1)]...\n",
      "[2022-09-14 20:51:51,015] keeping 2000000 tokens which were in no less than 0 and no more than 3290000 (=100.0%) documents\n",
      "[2022-09-14 20:51:55,465] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:51:55,536] adding document #3290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:52:22,348] discarding 35946 tokens: [('oláfr', 1), ('rokokoportal', 1), ('stockhol', 1), ('undergjerningane', 1), ('øresundfleet', 1), ('greentopmo', 1), ('courdin', 1), ('monettized', 1), ('smcog', 1), ('pulaskicountymirror', 1)]...\n",
      "[2022-09-14 20:52:22,350] keeping 2000000 tokens which were in no less than 0 and no more than 3300000 (=100.0%) documents\n",
      "[2022-09-14 20:52:25,815] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:52:25,884] adding document #3300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:52:52,289] discarding 38339 tokens: [('modelbuch', 1), ('mundillo_de_moca', 1), ('oidfa', 1), ('rosalibre', 1), ('wyrobu', 1), ('eupd', 1), ('infradiagnosed', 1), ('intropunitive', 1), ('neurometabolites', 1), ('blokelike', 1)]...\n",
      "[2022-09-14 20:52:52,290] keeping 2000000 tokens which were in no less than 0 and no more than 3310000 (=100.0%) documents\n",
      "[2022-09-14 20:52:55,693] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:52:55,760] adding document #3310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:53:20,765] discarding 33491 tokens: [('ˌson', 1), ('ˌsɛ', 1), ('ˌɛ', 1), ('maschhoffs', 1), ('mowmar', 1), ('baireda', 1), ('cluiam', 1), ('fimberhele', 1), ('finnela', 1), ('fíndgaine', 1)]...\n",
      "[2022-09-14 20:53:20,766] keeping 2000000 tokens which were in no less than 0 and no more than 3320000 (=100.0%) documents\n",
      "[2022-09-14 20:53:25,870] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:53:25,988] adding document #3320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:53:51,405] discarding 30394 tokens: [('abbotioannis', 1), ('agrimisargos', 1), ('balourdosnikolaos', 1), ('chaldeosioannis', 1), ('christeasdimitrios', 1), ('daispavlos', 1), ('dekavalasnikolaos', 1), ('diomatarasevangelos', 1), ('dolaspetros', 1), ('drivaskonstantinos', 1)]...\n",
      "[2022-09-14 20:53:51,407] keeping 2000000 tokens which were in no less than 0 and no more than 3330000 (=100.0%) documents\n",
      "[2022-09-14 20:53:56,293] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:53:56,391] adding document #3330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:54:23,431] discarding 38098 tokens: [('nahuta', 1), ('ninnahuta', 1), ('nirabbuda', 1), ('nirabhilapya', 1), ('sahastrá', 1), ('samaptalambha', 1), ('sarvabala', 1), ('soganghika', 1), ('titlambha', 1), ('vibhutangama', 1)]...\n",
      "[2022-09-14 20:54:23,433] keeping 2000000 tokens which were in no less than 0 and no more than 3340000 (=100.0%) documents\n",
      "[2022-09-14 20:54:28,351] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:54:28,452] adding document #3340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:54:52,765] discarding 32514 tokens: [('sretnom', 1), ('zvijezdom', 1), ('cymet', 1), ('flashgamm', 1), ('joonam', 1), ('nabood', 1), ('radiojavan', 1), ('zarebin', 1), ('aydinlar', 1), ('bossae', 1)]...\n",
      "[2022-09-14 20:54:52,766] keeping 2000000 tokens which were in no less than 0 and no more than 3350000 (=100.0%) documents\n",
      "[2022-09-14 20:54:56,272] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:54:56,340] adding document #3350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 20:55:23,305] discarding 32202 tokens: [('euhm', 1), ('rivioli', 1), ('beircheart', 1), ('cagley', 1), ('childrenis', 1), ('deckner', 1), ('gallavich', 1), ('gehlfuss', 1), ('goreshter', 1), ('hermiker', 1)]...\n",
      "[2022-09-14 20:55:23,307] keeping 2000000 tokens which were in no less than 0 and no more than 3360000 (=100.0%) documents\n",
      "[2022-09-14 20:55:28,278] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:55:28,377] adding document #3360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:55:53,601] discarding 34534 tokens: [('reochson', 1), ('rossinish', 1), ('vaðil', 1), ('allisdale', 1), ('chliaid', 1), ('easary', 1), ('metadiorites', 1), ('metatonalites', 1), ('sgurabhal', 1), ('eucula', 1)]...\n",
      "[2022-09-14 20:55:53,603] keeping 2000000 tokens which were in no less than 0 and no more than 3370000 (=100.0%) documents\n",
      "[2022-09-14 20:55:58,458] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:55:58,558] adding document #3370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:56:24,086] discarding 34272 tokens: [('gungarawoni', 1), ('gungoragone', 1), ('gungorologni', 1), ('gungurugoni', 1), ('gungurulgungi', 1), ('gunibidji', 1), ('gunjibidji', 1), ('gurengn', 1), ('gurragunga', 1), ('gurungada', 1)]...\n",
      "[2022-09-14 20:56:24,087] keeping 2000000 tokens which were in no less than 0 and no more than 3380000 (=100.0%) documents\n",
      "[2022-09-14 20:56:28,990] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:56:29,089] adding document #3380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:56:56,317] discarding 35718 tokens: [('ossae', 1), ('shusrushaka', 1), ('zenganno', 1), ('aeasop', 1), ('bonniscken', 1), ('buena_vista', 1), ('columbia_pictures', 1), ('educational_pictures', 1), ('eighth_national_films', 1), ('eighthn', 1)]...\n",
      "[2022-09-14 20:56:56,319] keeping 2000000 tokens which were in no less than 0 and no more than 3390000 (=100.0%) documents\n",
      "[2022-09-14 20:57:01,232] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:57:01,333] adding document #3390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:57:25,953] discarding 40104 tokens: [('gjerpin', 1), ('teagesville', 1), ('cychosz', 1), ('fenwoodhistory', 1), ('szebsdat', 1), ('szeljsdat', 1), ('vallborg', 1), ('wiclarkcountyhistory', 1), ('glendalen', 1), ('wonsil', 1)]...\n",
      "[2022-09-14 20:57:25,955] keeping 2000000 tokens which were in no less than 0 and no more than 3400000 (=100.0%) documents\n",
      "[2022-09-14 20:57:29,410] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:57:29,478] adding document #3400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:57:53,665] discarding 36186 tokens: [('simoton', 1), ('homeroute', 1), ('matlage', 1), ('nalcochampion', 1), ('longserre', 1), ('fcwd', 1), ('pearsalltexasoaks', 1), ('aukds', 1), ('faggard', 1), ('sibernagel', 1)]...\n",
      "[2022-09-14 20:57:53,667] keeping 2000000 tokens which were in no less than 0 and no more than 3410000 (=100.0%) documents\n",
      "[2022-09-14 20:57:58,868] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:57:58,968] adding document #3410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:58:24,896] discarding 32998 tokens: [('octogintuple', 1), ('octovigintuple', 1), ('pentuple', 1), ('quadragintuple', 1), ('quattuordecuple', 1), ('quattuorvigintuple', 1), ('quindecuple', 1), ('quinquagintuple', 1), ('quinvigintuple', 1), ('septendecuple', 1)]...\n",
      "[2022-09-14 20:58:24,898] keeping 2000000 tokens which were in no less than 0 and no more than 3420000 (=100.0%) documents\n",
      "[2022-09-14 20:58:29,788] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:58:29,887] adding document #3420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:58:54,705] discarding 30860 tokens: [('geofisika', 1), ('hidrología', 1), ('klimatologi', 1), ('meteorologi', 1), ('dianah', 1), ('poschman', 1), ('strathford', 1), ('tunell', 1), ('zdinak', 1), ('nachitoch', 1)]...\n",
      "[2022-09-14 20:58:54,707] keeping 2000000 tokens which were in no less than 0 and no more than 3430000 (=100.0%) documents\n",
      "[2022-09-14 20:58:59,455] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:58:59,554] adding document #3430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:59:25,528] discarding 32874 tokens: [('genering', 1), ('juscum', 1), ('petuquapoern', 1), ('ulinesville', 1), ('escotak', 1), ('esquatak', 1), ('skiwia', 1), ('blosers', 1), ('cassiltown', 1), ('clovena', 1)]...\n",
      "[2022-09-14 20:59:25,530] keeping 2000000 tokens which were in no less than 0 and no more than 3440000 (=100.0%) documents\n",
      "[2022-09-14 20:59:30,439] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 20:59:30,539] adding document #3440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:00:00,154] discarding 36073 tokens: [('wakkerendijk', 1), ('aluminiumcentre', 1), ('bikecity', 1), ('groeikern', 1), ('joinn', 1), ('kniphoek', 1), ('loerikseweg', 1), ('movares', 1), ('odijkseweg', 1), ('studiosk', 1)]...\n",
      "[2022-09-14 21:00:00,155] keeping 2000000 tokens which were in no less than 0 and no more than 3450000 (=100.0%) documents\n",
      "[2022-09-14 21:00:03,606] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:00:03,674] adding document #3450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:00:27,990] discarding 31793 tokens: [('medvid', 1), ('mykhalyna', 1), ('mykhaylovich', 1), ('forfeitt', 1), ('kotape', 1), ('kwete', 1), ('magbondoline', 1), ('nyimi', 1), ('angenommenen', 1), ('bienfaisants', 1)]...\n",
      "[2022-09-14 21:00:27,992] keeping 2000000 tokens which were in no less than 0 and no more than 3460000 (=100.0%) documents\n",
      "[2022-09-14 21:00:32,830] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:00:32,931] adding document #3460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:00:56,827] discarding 35015 tokens: [('adambrothersrecordsofficeedinburgh', 1), ('adamfishing', 1), ('comptonverney', 1), ('dalquarran', 1), ('homehouseetruscanroom', 1), ('robertadamlibrarykenwood', 1), ('templepartick', 1), ('liquiding', 1), ('shockinghumor', 1), ('bumbaru', 1)]...\n",
      "[2022-09-14 21:00:56,829] keeping 2000000 tokens which were in no less than 0 and no more than 3470000 (=100.0%) documents\n",
      "[2022-09-14 21:01:01,679] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:01:01,778] adding document #3470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:01:27,124] discarding 33952 tokens: [('riverbender', 1), ('tugboaters', 1), ('lifebrook', 1), ('neimrich', 1), ('spedini', 1), ('abilitiesbeauty', 1), ('amazonsmyrto', 1), ('changethe', 1), ('homelandto', 1), ('languageeven', 1)]...\n",
      "[2022-09-14 21:01:27,126] keeping 2000000 tokens which were in no less than 0 and no more than 3480000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:01:32,055] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:01:32,154] adding document #3480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:02:03,781] discarding 48274 tokens: [('xoodzidaa', 1), ('xoots', 1), ('alejandrosanz', 1), ('asturianada', 1), ('englishspeakers', 1), ('intercelticu', 1), ('pandereteiras', 1), ('reconquistors', 1), ('sinfonism', 1), ('aakdeintaan', 1)]...\n",
      "[2022-09-14 21:02:03,783] keeping 2000000 tokens which were in no less than 0 and no more than 3490000 (=100.0%) documents\n",
      "[2022-09-14 21:02:08,666] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:02:08,769] adding document #3490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:02:35,687] discarding 31736 tokens: [('dyiigurra', 1), ('jiigurru', 1), ('mesingomesia', 1), ('shawadasee', 1), ('wapephani', 1), ('afrasaib', 1), ('aiwiyanghana', 1), ('dugdhova', 1), ('frangrasyan', 1), ('hankana', 1)]...\n",
      "[2022-09-14 21:02:35,689] keeping 2000000 tokens which were in no less than 0 and no more than 3500000 (=100.0%) documents\n",
      "[2022-09-14 21:02:40,584] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:02:40,685] adding document #3500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:03:06,575] discarding 31906 tokens: [('sopronneugassenr', 1), ('awlube', 1), ('besif', 1), ('boboola', 1), ('bodaadi', 1), ('bodaado', 1), ('bororoji', 1), ('boɗaaɗo', 1), ('bukkaru', 1), ('bunaji', 1)]...\n",
      "[2022-09-14 21:03:06,577] keeping 2000000 tokens which were in no less than 0 and no more than 3510000 (=100.0%) documents\n",
      "[2022-09-14 21:03:11,423] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:03:11,523] adding document #3510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:03:37,650] discarding 34388 tokens: [('protoconulid', 1), ('quadritubercular', 1), ('bobbele', 1), ('dekein', 1), ('eisvögel', 1), ('iuce', 1), ('konradius', 1), ('kybfelsen', 1), ('niemmerme', 1), ('rappenpfennigs', 1)]...\n",
      "[2022-09-14 21:03:37,652] keeping 2000000 tokens which were in no less than 0 and no more than 3520000 (=100.0%) documents\n",
      "[2022-09-14 21:03:41,135] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:03:41,203] adding document #3520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:04:06,476] discarding 31206 tokens: [('ehtnographical', 1), ('polyvocal', 1), ('proccalimed', 1), ('confora', 1), ('courtrey', 1), ('wylackie', 1), ('albasham', 1), ('alkhazam', 1), ('alshath', 1), ('asulamy', 1)]...\n",
      "[2022-09-14 21:04:06,478] keeping 2000000 tokens which were in no less than 0 and no more than 3530000 (=100.0%) documents\n",
      "[2022-09-14 21:04:11,674] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:04:11,774] adding document #3530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:04:36,023] discarding 32625 tokens: [('dekechilam', 1), ('kakhon', 1), ('kapileswari', 1), ('kedeche', 1), ('mafin', 1), ('nithur', 1), ('sandipta', 1), ('shankhachil', 1), ('tupur', 1), ('dipsalut', 1)]...\n",
      "[2022-09-14 21:04:36,025] keeping 2000000 tokens which were in no less than 0 and no more than 3540000 (=100.0%) documents\n",
      "[2022-09-14 21:04:40,894] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:04:40,994] adding document #3540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:05:05,497] discarding 30319 tokens: [('siddiki', 1), ('waddies', 1), ('yunkanjini', 1), ('raitel', 1), ('righttel', 1), ('marishal', 1), ('dailysportscar', 1), ('racesports', 1), ('adefisayo', 1), ('akinbile', 1)]...\n",
      "[2022-09-14 21:05:05,498] keeping 2000000 tokens which were in no less than 0 and no more than 3550000 (=100.0%) documents\n",
      "[2022-09-14 21:05:08,991] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:05:09,059] adding document #3550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:05:36,545] discarding 32756 tokens: [('eaglesport', 1), ('eurocellulari', 1), ('lilije', 1), ('orldrini', 1), ('pavello', 1), ('snadeiro', 1), ('corevan', 1), ('francal', 1), ('pulvereitzer', 1), ('bishgada', 1)]...\n",
      "[2022-09-14 21:05:36,547] keeping 2000000 tokens which were in no less than 0 and no more than 3560000 (=100.0%) documents\n",
      "[2022-09-14 21:05:41,463] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:05:41,565] adding document #3560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:06:06,748] discarding 34208 tokens: [('apsay', 1), ('atusc', 1), ('boermeester', 1), ('budrovich', 1), ('darreus', 1), ('edoga', 1), ('imatorbhebhe', 1), ('khaliel', 1), ('kolanz', 1), ('labonty', 1)]...\n",
      "[2022-09-14 21:06:06,749] keeping 2000000 tokens which were in no less than 0 and no more than 3570000 (=100.0%) documents\n",
      "[2022-09-14 21:06:10,238] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:06:10,306] adding document #3570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:06:35,765] discarding 28762 tokens: [('aguibou', 1), ('panathiniakos', 1), ('thriumphic', 1), ('aitebaar', 1), ('banjaara', 1), ('bazain', 1), ('chahatain', 1), ('chahey', 1), ('chalawa', 1), ('chatharay', 1)]...\n",
      "[2022-09-14 21:06:35,768] keeping 2000000 tokens which were in no less than 0 and no more than 3580000 (=100.0%) documents\n",
      "[2022-09-14 21:06:41,004] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:06:41,104] adding document #3580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:07:08,501] discarding 30855 tokens: [('xingyong', 1), ('artschool', 1), ('barbiersbrug', 1), ('israelskade', 1), ('overhoeks', 1), ('terop', 1), ('catronio', 1), ('hindprints', 1), ('münchehagen', 1), ('הסרטן', 1)]...\n",
      "[2022-09-14 21:07:08,503] keeping 2000000 tokens which were in no less than 0 and no more than 3590000 (=100.0%) documents\n",
      "[2022-09-14 21:07:13,371] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:07:13,471] adding document #3590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:07:38,162] discarding 30185 tokens: [('tilter#1', 1), ('corregiduria', 1), ('platanilla', 1), ('illiani', 1), ('ansuman', 1), ('vitmayer', 1), ('magnall', 1), ('torrisholme', 1), ('aroit', 1), ('bachsin', 1)]...\n",
      "[2022-09-14 21:07:38,164] keeping 2000000 tokens which were in no less than 0 and no more than 3600000 (=100.0%) documents\n",
      "[2022-09-14 21:07:43,025] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:07:43,126] adding document #3600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:08:08,910] discarding 35146 tokens: [('creatocracy', 1), ('frederikolsen', 1), ('waitharli', 1), ('cyberattacker', 1), ('cyberattackers', 1), ('gemassmer', 1), ('kholsa', 1), ('tracevector', 1), ('botimet', 1), ('diorasis', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:08:08,912] keeping 2000000 tokens which were in no less than 0 and no more than 3610000 (=100.0%) documents\n",
      "[2022-09-14 21:08:13,902] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:08:14,004] adding document #3610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:08:39,216] discarding 28831 tokens: [('fdba', 1), ('atınç', 1), ('bayrampaşaspor', 1), ('fleetcorp', 1), ('karaal', 1), ('kurumuş', 1), ('küçükköylü', 1), ('majorworx', 1), ('mogaz', 1), ('passolig', 1)]...\n",
      "[2022-09-14 21:08:39,217] keeping 2000000 tokens which were in no less than 0 and no more than 3620000 (=100.0%) documents\n",
      "[2022-09-14 21:08:42,749] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:08:42,818] adding document #3620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:09:05,707] discarding 28013 tokens: [('rosingana', 1), ('ichthyologyahead', 1), ('melanocheilus', 1), ('hellegård', 1), ('erivelto', 1), ('gubiani', 1), ('devanyi', 1), ('inequalitites', 1), ('podesva', 1), ('ndmappingorbit', 1)]...\n",
      "[2022-09-14 21:09:05,708] keeping 2000000 tokens which were in no less than 0 and no more than 3630000 (=100.0%) documents\n",
      "[2022-09-14 21:09:10,921] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:09:11,021] adding document #3630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:09:34,959] discarding 31107 tokens: [('bhupetindra', 1), ('bhupitendra', 1), ('governmentalization', 1), ('arungeorge', 1), ('chettai', 1), ('hallelooya', 1), ('itoope', 1), ('jexson', 1), ('kaaryam', 1), ('karinkannan', 1)]...\n",
      "[2022-09-14 21:09:34,961] keeping 2000000 tokens which were in no less than 0 and no more than 3640000 (=100.0%) documents\n",
      "[2022-09-14 21:09:39,895] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:09:39,995] adding document #3640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:10:02,756] discarding 28549 tokens: [('djiman', 1), ('inspirels', 1), ('michielvmusic', 1), ('adeeka', 1), ('ambaru', 1), ('ayyoor', 1), ('bandiyod', 1), ('bappaithotty', 1), ('chavadikatte', 1), ('cherugoli', 1)]...\n",
      "[2022-09-14 21:10:02,758] keeping 2000000 tokens which were in no less than 0 and no more than 3650000 (=100.0%) documents\n",
      "[2022-09-14 21:10:06,280] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:10:06,347] adding document #3650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:10:32,833] discarding 38225 tokens: [('emersonianism', 1), ('klappert', 1), ('lillevig', 1), ('tincy', 1), ('ereignisvollen', 1), ('fischmarktstrasse', 1), ('jankk', 1), ('kinderhochzeit', 1), ('kultgebräuche', 1), ('kultstätten', 1)]...\n",
      "[2022-09-14 21:10:32,835] keeping 2000000 tokens which were in no less than 0 and no more than 3660000 (=100.0%) documents\n",
      "[2022-09-14 21:10:37,789] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:10:37,891] adding document #3660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:11:02,667] discarding 30307 tokens: [('devnagar', 1), ('dmer', 1), ('doverstorparen', 1), ('fattigvårdsstyrelsen', 1), ('lantmannaskola', 1), ('socialpolitician', 1), ('socialpolitics', 1), ('amirhamzeh', 1), ('chakousari', 1), ('chirei', 1)]...\n",
      "[2022-09-14 21:11:02,668] keeping 2000000 tokens which were in no less than 0 and no more than 3670000 (=100.0%) documents\n",
      "[2022-09-14 21:11:06,144] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:11:06,211] adding document #3670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:11:32,084] discarding 29828 tokens: [('bisichi', 1), ('belfont', 1), ('countlesshaints', 1), ('countyomnibus', 1), ('countyvolume', 1), ('crookjason', 1), ('donecomeback', 1), ('franquiztyler', 1), ('fromharrow', 1), ('gieni', 1)]...\n",
      "[2022-09-14 21:11:32,086] keeping 2000000 tokens which were in no less than 0 and no more than 3680000 (=100.0%) documents\n",
      "[2022-09-14 21:11:36,961] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:11:37,061] adding document #3680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:12:00,945] discarding 31091 tokens: [('churan', 1), ('ggsec', 1), ('nalisha', 1), ('arjohn', 1), ('boitizon', 1), ('ceserio', 1), ('johnrey', 1), ('nonilyn', 1), ('ortinero', 1), ('pamorca', 1)]...\n",
      "[2022-09-14 21:12:00,947] keeping 2000000 tokens which were in no less than 0 and no more than 3690000 (=100.0%) documents\n",
      "[2022-09-14 21:12:04,756] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:12:04,825] adding document #3690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:12:28,923] discarding 31172 tokens: [('hyrve', 1), ('nakagama', 1), ('albdelkader', 1), ('gurukanth', 1), ('gwom', 1), ('haanstad', 1), ('khobaib', 1), ('mohiussunnath', 1), ('mumbreshwar', 1), ('vourey', 1)]...\n",
      "[2022-09-14 21:12:28,925] keeping 2000000 tokens which were in no less than 0 and no more than 3700000 (=100.0%) documents\n",
      "[2022-09-14 21:12:34,076] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:12:34,185] adding document #3700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:12:59,739] discarding 32243 tokens: [('lesahel', 1), ('kuliana', 1), ('tonusured', 1), ('buckso', 1), ('kazemis', 1), ('chahtain', 1), ('nasheman', 1), ('diakonistyrelses', 1), ('husförhör', 1), ('laurelius', 1)]...\n",
      "[2022-09-14 21:12:59,743] keeping 2000000 tokens which were in no less than 0 and no more than 3710000 (=100.0%) documents\n",
      "[2022-09-14 21:13:03,328] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:13:03,398] adding document #3710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:13:31,182] discarding 32937 tokens: [('everedon', 1), ('ganeworth', 1), ('gonshill', 1), ('hemmesby', 1), ('holcott', 1), ('rudolfstadt', 1), ('wryght', 1), ('neighorhoods', 1), ('breutta', 1), ('briueta', 1)]...\n",
      "[2022-09-14 21:13:31,184] keeping 2000000 tokens which were in no less than 0 and no more than 3720000 (=100.0%) documents\n",
      "[2022-09-14 21:13:36,361] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:13:36,464] adding document #3720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:14:04,206] discarding 35283 tokens: [('campuselevation', 1), ('perappearance', 1), ('stadiumelevation', 1), ('totalmwsports', 1), ('priotise', 1), ('adurus', 1), ('alkippē', 1), ('halirrhotius', 1), ('alazygus', 1), ('halirrhóthios', 1)]...\n",
      "[2022-09-14 21:14:04,208] keeping 2000000 tokens which were in no less than 0 and no more than 3730000 (=100.0%) documents\n",
      "[2022-09-14 21:14:09,082] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:14:09,182] adding document #3730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:14:35,043] discarding 32959 tokens: [('absoros', 1), ('anaphanenai', 1), ('estraphe', 1), ('maenalian', 1), ('melantian', 1), ('narycea', 1), ('pelasgican', 1), ('peloponnesse', 1), ('titaressa', 1), ('chrysómallon', 1)]...\n",
      "[2022-09-14 21:14:35,045] keeping 2000000 tokens which were in no less than 0 and no more than 3740000 (=100.0%) documents\n",
      "[2022-09-14 21:14:39,926] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:14:40,026] adding document #3740000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:15:04,807] discarding 37109 tokens: [('testeman', 1), ('trifalatrigeta', 1), ('tufala', 1), ('tufalatugeta', 1), ('yumitri', 1), ('yumitu', 1), ('yutrifala', 1), ('yutufala', 1), ('hanhel', 1), ('meulpolder', 1)]...\n",
      "[2022-09-14 21:15:04,810] keeping 2000000 tokens which were in no less than 0 and no more than 3750000 (=100.0%) documents\n",
      "[2022-09-14 21:15:09,756] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:15:09,858] adding document #3750000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:15:42,382] discarding 31082 tokens: [('大橋鎬志', 1), ('太田謙吉', 1), ('奈良原牛之助', 1), ('宮原克昇', 1), ('宮城俊作', 1), ('宮﨑吾朗', 1), ('小出兼久', 1), ('小口健蔵', 1), ('小口基實', 1), ('小坂立夫', 1)]...\n",
      "[2022-09-14 21:15:42,384] keeping 2000000 tokens which were in no less than 0 and no more than 3760000 (=100.0%) documents\n",
      "[2022-09-14 21:15:47,135] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:15:47,236] adding document #3760000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:16:10,766] discarding 29980 tokens: [('astrolai', 1), ('astrolobe', 1), ('balesilha', 1), ('klauuw', 1), ('pacento', 1), ('λαβ', 1), ('angelesspec', 1), ('chokepear', 1), ('keuscher', 1), ('säverin', 1)]...\n",
      "[2022-09-14 21:16:10,768] keeping 2000000 tokens which were in no less than 0 and no more than 3770000 (=100.0%) documents\n",
      "[2022-09-14 21:16:15,637] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:16:15,737] adding document #3770000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:16:41,571] discarding 33074 tokens: [('boorbung', 1), ('durrumboi', 1), ('zeilke', 1), ('ocellated_turkey', 1), ('snooding', 1), ('μελεαγρις', 1), ('lowiro', 1), ('campagnanese', 1), ('tolumnus', 1), ('veiantanus', 1)]...\n",
      "[2022-09-14 21:16:41,573] keeping 2000000 tokens which were in no less than 0 and no more than 3780000 (=100.0%) documents\n",
      "[2022-09-14 21:16:46,476] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:16:46,578] adding document #3780000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:17:07,784] discarding 27975 tokens: [('ronterious', 1), ('gurierrez', 1), ('sheldo', 1), ('withlacoohee', 1), ('callsville', 1), ('chataocolea', 1), ('grillsmith', 1), ('hitchipucksassa', 1), ('ichepucksassa', 1), ('ichipucksassa', 1)]...\n",
      "[2022-09-14 21:17:07,785] keeping 2000000 tokens which were in no less than 0 and no more than 3790000 (=100.0%) documents\n",
      "[2022-09-14 21:17:11,263] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:17:11,330] adding document #3790000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:17:34,909] discarding 33825 tokens: [('tashbetsey', 1), ('xchequer', 1), ('תשבצי', 1), ('codedword', 1), ('cryptoquiz', 1), ('cryptoquote', 1), ('selenavevo', 1), ('taraporewalla', 1), ('werkstattfunde', 1), ('φειδιου', 1)]...\n",
      "[2022-09-14 21:17:34,910] keeping 2000000 tokens which were in no less than 0 and no more than 3800000 (=100.0%) documents\n",
      "[2022-09-14 21:17:39,813] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:17:39,914] adding document #3800000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:18:08,372] discarding 36713 tokens: [('elethiomel', 1), ('miḏbar', 1), ('תנופה', 1), ('ἀφόρισμα', 1), ('cranberriesin', 1), ('aškəlōṯ', 1), ('innəḇê', 1), ('lāmōw', 1), ('miggep', 1), ('miššaḏmōṯ', 1)]...\n",
      "[2022-09-14 21:18:08,374] keeping 2000000 tokens which were in no less than 0 and no more than 3810000 (=100.0%) documents\n",
      "[2022-09-14 21:18:13,280] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:18:13,383] adding document #3810000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:18:38,343] discarding 34438 tokens: [('bính鄭柄', 1), ('cao莫翱', 1), ('changying朱常瀛', 1), ('chao李超', 1), ('cha孟察', 1), ('cheng朱誠', 1), ('chen郭諶', 1), ('chen陳世祖', 1), ('chen陳太祖', 1), ('chen陳高宗', 1)]...\n",
      "[2022-09-14 21:18:38,345] keeping 2000000 tokens which were in no less than 0 and no more than 3820000 (=100.0%) documents\n",
      "[2022-09-14 21:18:43,307] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:18:43,408] adding document #3820000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:19:07,076] discarding 36331 tokens: [('hellisoe', 1), ('seataka', 1), ('wieve', 1), ('architecturny', 1), ('arhitekturnyi', 1), ('azadeayse', 1), ('ağidel', 1), ('gorkinsko', 1), ('ometievsky', 1), ('susaqlağıçı', 1)]...\n",
      "[2022-09-14 21:19:07,078] keeping 2000000 tokens which were in no less than 0 and no more than 3830000 (=100.0%) documents\n",
      "[2022-09-14 21:19:10,644] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:19:10,713] adding document #3830000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:19:34,452] discarding 29776 tokens: [('ibcb', 1), ('dreamband', 1), ('dzjes', 1), ('jazzhattan', 1), ('noslen', 1), ('nostalia', 1), ('uppababy', 1), ('unnext', 1), ('kwoi', 1), ('seifallah', 1)]...\n",
      "[2022-09-14 21:19:34,454] keeping 2000000 tokens which were in no less than 0 and no more than 3840000 (=100.0%) documents\n",
      "[2022-09-14 21:19:39,320] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:19:39,420] adding document #3840000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:20:05,859] discarding 38908 tokens: [('duxiao', 1), ('déwén', 1), ('dìjié', 1), ('dìkù', 1), ('dìshùn', 1), ('dìyáo', 1), ('dígǔnǎi', 1), ('elgiyengg', 1), ('fenwu', 1), ('fúlín', 1)]...\n",
      "[2022-09-14 21:20:05,861] keeping 2000000 tokens which were in no less than 0 and no more than 3850000 (=100.0%) documents\n",
      "[2022-09-14 21:20:10,756] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:20:10,858] adding document #3850000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:20:35,488] discarding 31701 tokens: [('caschielawes', 1), ('crewallie', 1), ('pacock', 1), ('pinniewinkles', 1), ('schamefullie', 1), ('taillifeir', 1), ('thumbscrew#1', 1), ('unmercifullie', 1), ('ayikoru', 1), ('yosie', 1)]...\n",
      "[2022-09-14 21:20:35,490] keeping 2000000 tokens which were in no less than 0 and no more than 3860000 (=100.0%) documents\n",
      "[2022-09-14 21:20:40,425] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:20:40,527] adding document #3860000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:21:06,370] discarding 31145 tokens: [('lucoque', 1), ('manahsiddhikari', 1), ('morajhari', 1), ('panchapura', 1), ('panjaur', 1), ('pethad', 1), ('suhavadevi', 1), ('altonia', 1), ('baguely', 1), ('ásbjörnsdóttir', 1)]...\n",
      "[2022-09-14 21:21:06,372] keeping 2000000 tokens which were in no less than 0 and no more than 3870000 (=100.0%) documents\n",
      "[2022-09-14 21:21:11,313] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:21:11,416] adding document #3870000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:21:36,705] discarding 32152 tokens: [('dininity', 1), ('אבנים', 1), ('כנוס', 1), ('לאבד', 1), ('לאהב', 1), ('לבכות', 1), ('לבנות', 1), ('לבקש', 1), ('לדבר', 1), ('להרוג', 1)]...\n",
      "[2022-09-14 21:21:36,707] keeping 2000000 tokens which were in no less than 0 and no more than 3880000 (=100.0%) documents\n",
      "[2022-09-14 21:21:40,226] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:21:40,295] adding document #3880000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:22:07,231] discarding 35432 tokens: [('carasului', 1), ('formatiunile', 1), ('fosila', 1), ('ginkgopsida', 1), ('jurasicului', 1), ('mesozoice', 1), ('stájerlak', 1), ('bisztracseres', 1), ('chiciura', 1), ('nándorhegy', 1)]...\n",
      "[2022-09-14 21:22:07,233] keeping 2000000 tokens which were in no less than 0 and no more than 3890000 (=100.0%) documents\n",
      "[2022-09-14 21:22:10,788] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:22:10,859] adding document #3890000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:22:36,233] discarding 32091 tokens: [('baldris', 1), ('bogdziul', 1), ('boroska', 1), ('cetvertak', 1), ('erport', 1), ('heedoo', 1), ('hyounseop', 1), ('laukaitis', 1), ('leeuwesteijn', 1), ('lestarić', 1)]...\n",
      "[2022-09-14 21:22:36,235] keeping 2000000 tokens which were in no less than 0 and no more than 3900000 (=100.0%) documents\n",
      "[2022-09-14 21:22:41,232] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:22:41,332] adding document #3900000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:23:07,355] discarding 33176 tokens: [('posiłki', 1), ('possesores', 1), ('posług', 1), ('powóz', 1), ('połeczeństwo', 1), ('protoplasta', 1), ('przewrotna', 1), ('płockiej', 1), ('romanizm', 1), ('tollensers', 1)]...\n",
      "[2022-09-14 21:23:07,357] keeping 2000000 tokens which were in no less than 0 and no more than 3910000 (=100.0%) documents\n",
      "[2022-09-14 21:23:10,914] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:23:10,983] adding document #3910000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:23:36,214] discarding 33089 tokens: [('enoseos', 1), ('halikoutes', 1), ('kladisos', 1), ('koumbes', 1), ('kountouriotou', 1), ('lentariana', 1), ('ovraiki', 1), ('pasakaki', 1), ('splantzia', 1), ('tafros', 1)]...\n",
      "[2022-09-14 21:23:36,216] keeping 2000000 tokens which were in no less than 0 and no more than 3920000 (=100.0%) documents\n",
      "[2022-09-14 21:23:41,209] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:23:41,311] adding document #3920000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:24:09,586] discarding 32867 tokens: [('wordonga', 1), ('arider', 1), ('rosweood', 1), ('μvt', 1), ('futatabiyama', 1), ('japdirectory', 1), ('kobe_station', 1), ('kobeformersettelment', 1), ('sanjurokkassen', 1), ('twilight_view_of_kobe', 1)]...\n",
      "[2022-09-14 21:24:09,588] keeping 2000000 tokens which were in no less than 0 and no more than 3930000 (=100.0%) documents\n",
      "[2022-09-14 21:24:13,154] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:24:13,255] adding document #3930000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:24:38,071] discarding 33547 tokens: [('amasingkota', 1), ('bacans', 1), ('bachians', 1), ('barnaveld', 1), ('batchians', 1), ('cakasuanggi', 1), ('sayowang', 1), ('taubenkit', 1), ('tomore', 1), ('decroise', 1)]...\n",
      "[2022-09-14 21:24:38,073] keeping 2000000 tokens which were in no less than 0 and no more than 3940000 (=100.0%) documents\n",
      "[2022-09-14 21:24:41,628] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:24:41,697] adding document #3940000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:25:12,387] discarding 30160 tokens: [('witriawan', 1), ('yatul', 1), ('jetairways', 1), ('bauausführungen', 1), ('sitreps', 1), ('zeldzame', 1), ('falafal', 1), ('kebaftas', 1), ('telandros', 1), ('banbat', 1)]...\n",
      "[2022-09-14 21:25:12,389] keeping 2000000 tokens which were in no less than 0 and no more than 3950000 (=100.0%) documents\n",
      "[2022-09-14 21:25:17,251] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:25:17,351] adding document #3950000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:25:45,123] discarding 35670 tokens: [('continuadores', 1), ('daússe', 1), ('gwachene', 1), ('hulene', 1), ('inguice', 1), ('ingwane', 1), ('inhagoia', 1), ('isutc', 1), ('kankhomba', 1), ('magoanine', 1)]...\n",
      "[2022-09-14 21:25:45,125] keeping 2000000 tokens which were in no less than 0 and no more than 3960000 (=100.0%) documents\n",
      "[2022-09-14 21:25:50,012] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:25:50,114] adding document #3960000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:26:15,790] discarding 29327 tokens: [('davidianbrigitte', 1), ('divesrosemary', 1), ('eggingtonalexa', 1), ('fleuryramune', 1), ('freiburgjoelle', 1), ('hassanelizabeth', 1), ('hemmingssharmay', 1), ('hochrystal', 1), ('hulselinda', 1), ('jonesangharad', 1)]...\n",
      "[2022-09-14 21:26:15,792] keeping 2000000 tokens which were in no less than 0 and no more than 3970000 (=100.0%) documents\n",
      "[2022-09-14 21:26:19,302] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:26:19,371] adding document #3970000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:26:44,777] discarding 32683 tokens: [('dentellato', 1), ('ffritt', 1), ('mappina', 1), ('pedamentina', 1), ('pizzafest', 1), ('purpetiello', 1), ('repecchia', 1), ('risvolto', 1), ('golvъ', 1), ('lovrenz', 1)]...\n",
      "[2022-09-14 21:26:44,778] keeping 2000000 tokens which were in no less than 0 and no more than 3980000 (=100.0%) documents\n",
      "[2022-09-14 21:26:49,692] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:26:49,794] adding document #3980000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:27:16,753] discarding 32611 tokens: [('josquinj', 1), ('kessalia', 1), ('langoreulx', 1), ('lebloitte', 1), ('nappés', 1), ('parfons', 1), ('plaist', 1), ('planxit', 1), ('savoyenne', 1), ('souhaitter', 1)]...\n",
      "[2022-09-14 21:27:16,754] keeping 2000000 tokens which were in no less than 0 and no more than 3990000 (=100.0%) documents\n",
      "[2022-09-14 21:27:21,998] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:27:22,101] adding document #3990000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:27:49,138] discarding 33159 tokens: [('ivstitia', 1), ('philipsdaalder', 1), ('zvuanem', 1), ('ldisasm', 1), ('ndisasm', 1), ('banauá', 1), ('banavá', 1), ('belendzher', 1), ('belenjer', 1), ('bülünjar', 1)]...\n",
      "[2022-09-14 21:27:49,141] keeping 2000000 tokens which were in no less than 0 and no more than 4000000 (=100.0%) documents\n",
      "[2022-09-14 21:27:54,516] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:27:54,618] adding document #4000000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:28:20,184] discarding 33095 tokens: [('tuŋu', 1), ('tuɔm', 1), ('twõgo', 1), ('tyurama', 1), ('tàsyẽ', 1), ('tàà', 1), ('támúwá', 1), ('táárí', 1), ('tâadi', 1), ('tâati', 1)]...\n",
      "[2022-09-14 21:28:20,185] keeping 2000000 tokens which were in no less than 0 and no more than 4010000 (=100.0%) documents\n",
      "[2022-09-14 21:28:25,122] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:28:25,224] adding document #4010000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:28:50,727] discarding 34289 tokens: [('econocar', 1), ('iudicatur', 1), ('beedenbender', 1), ('censusamerican', 1), ('islandcompared', 1), ('blutserumtherapie', 1), ('bovivaccine', 1), ('opticociliaris', 1), ('wannkopfstraße', 1), ('terrorangriffe', 1)]...\n",
      "[2022-09-14 21:28:50,729] keeping 2000000 tokens which were in no less than 0 and no more than 4020000 (=100.0%) documents\n",
      "[2022-09-14 21:28:54,238] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:28:54,308] adding document #4020000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:29:21,587] discarding 36147 tokens: [('landstings', 1), ('paalægger', 1), ('skaalholt', 1), ('sörge', 1), ('ödelagte', 1), ('quebecpp', 1), ('crijnsen', 1), ('anylist', 1), ('rgtd', 1), ('karchevskaya', 1)]...\n",
      "[2022-09-14 21:29:21,588] keeping 2000000 tokens which were in no less than 0 and no more than 4030000 (=100.0%) documents\n",
      "[2022-09-14 21:29:25,161] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:29:25,231] adding document #4030000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:29:54,494] discarding 34872 tokens: [('slicky', 1), ('aufrichtung', 1), ('bachereau', 1), ('darstellungsweisen', 1), ('frankurt', 1), ('fügung', 1), ('geselschap', 1), ('historienbild', 1), ('kaiserproklamation', 1), ('kaisertums', 1)]...\n",
      "[2022-09-14 21:29:54,496] keeping 2000000 tokens which were in no less than 0 and no more than 4040000 (=100.0%) documents\n",
      "[2022-09-14 21:29:59,460] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:29:59,563] adding document #4040000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:30:32,586] discarding 56021 tokens: [('단성중고교앞사거리', 1), ('stokvisjuliana', 1), ('당고개', 1), ('도리', 1), ('namalgama', 1), ('동국에스앤씨', 1), ('동단', 1), ('매전교삼거리', 1), ('soubeyrac', 1), ('매전보건지소매전면사무소', 1)]...\n",
      "[2022-09-14 21:30:32,588] keeping 2000000 tokens which were in no less than 0 and no more than 4050000 (=100.0%) documents\n",
      "[2022-09-14 21:30:37,565] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:30:37,673] adding document #4050000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:31:05,803] discarding 33256 tokens: [('春家小姐是讼师', 1), ('李溪芮', 1), ('杨幂', 1), ('迪丽热巴', 1), ('邱天', 1), ('stamus', 1), ('ebarko', 1), ('jupin', 1), ('kruiki', 1), ('dödbok', 1)]...\n",
      "[2022-09-14 21:31:05,805] keeping 2000000 tokens which were in no less than 0 and no more than 4060000 (=100.0%) documents\n",
      "[2022-09-14 21:31:10,759] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:31:10,862] adding document #4060000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:31:35,441] discarding 31121 tokens: [('劉承', 1), ('劉胤', 1), ('劉輯', 1), ('悼王', 1), ('武邑侯', 1), ('殤王', 1), ('鄉侯', 1), ('국화축제', 1), ('마산가고파', 1), ('bagoense', 1)]...\n",
      "[2022-09-14 21:31:35,442] keeping 2000000 tokens which were in no less than 0 and no more than 4070000 (=100.0%) documents\n",
      "[2022-09-14 21:31:40,697] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:31:40,798] adding document #4070000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:32:06,544] discarding 32015 tokens: [('bckp', 1), ('frmpo', 1), ('kyova', 1), ('metroplan', 1), ('ouachata', 1), ('planrva', 1), ('kneževic', 1), ('arismetica', 1), ('conpusicion', 1), ('subtilissimo', 1)]...\n",
      "[2022-09-14 21:32:06,546] keeping 2000000 tokens which were in no less than 0 and no more than 4080000 (=100.0%) documents\n",
      "[2022-09-14 21:32:10,105] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:32:10,174] adding document #4080000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:32:38,629] discarding 40626 tokens: [('seanen', 1), ('frikom', 1), ('absteigend', 1), ('skulpturenweg', 1), ('kokanodna', 1), ('moorlooratchee', 1), ('oartners', 1), ('beatrock', 1), ('popolarissima', 1), ('다음에', 1)]...\n",
      "[2022-09-14 21:32:38,631] keeping 2000000 tokens which were in no less than 0 and no more than 4090000 (=100.0%) documents\n",
      "[2022-09-14 21:32:43,578] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:32:43,682] adding document #4090000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:33:09,512] discarding 34039 tokens: [('odrée', 1), ('segikuchi', 1), ('lançam', 1), ('uss_sam_rayburn_', 1), ('vennor', 1), ('karupaiah', 1), ('nilavinai', 1), ('arabaseques', 1), ('vanadous', 1), ('mccolister', 1)]...\n",
      "[2022-09-14 21:33:09,514] keeping 2000000 tokens which were in no less than 0 and no more than 4100000 (=100.0%) documents\n",
      "[2022-09-14 21:33:14,524] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:33:14,627] adding document #4100000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:33:40,085] discarding 39358 tokens: [('monolid', 1), ('chagando', 1), ('valnea', 1), ('khursevichiae', 1), ('labimarginata', 1), ('levanderi', 1), ('mediaconvexa', 1), ('praefraga', 1), ('zekkyō', 1), ('ramarajyadalli', 1)]...\n",
      "[2022-09-14 21:33:40,087] keeping 2000000 tokens which were in no less than 0 and no more than 4110000 (=100.0%) documents\n",
      "[2022-09-14 21:33:43,689] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:33:43,759] adding document #4110000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:34:09,867] discarding 32799 tokens: [('建设社会主义法制国家', 1), ('权利', 1), ('法制化', 1), ('法治', 1), ('法部', 1), ('daetongryung', 1), ('gobernuko', 1), ('khazānā', 1), ('mahāmantrī', 1), ('neayuk', 1)]...\n",
      "[2022-09-14 21:34:09,868] keeping 2000000 tokens which were in no less than 0 and no more than 4120000 (=100.0%) documents\n",
      "[2022-09-14 21:34:13,414] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:34:13,482] adding document #4120000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:34:39,729] discarding 41857 tokens: [('waziro', 1), ('αβγανο', 1), ('παρσιῆται', 1), ('πασιανοί', 1), ('هډه', 1), ('ecosailingproject', 1), ('junlyu', 1), ('kleinmachnower', 1), ('aphelenchoide', 1), ('crusgali', 1)]...\n",
      "[2022-09-14 21:34:39,731] keeping 2000000 tokens which were in no less than 0 and no more than 4130000 (=100.0%) documents\n",
      "[2022-09-14 21:34:44,679] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:34:44,782] adding document #4130000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:35:10,684] discarding 35616 tokens: [('baarie', 1), ('baasit', 1), ('baasitt', 1), ('baattin', 1), ('baatwin', 1), ('baaʿeith', 1), ('baaʿith', 1), ('badiyʿ', 1), ('bariʿ', 1), ('basitt', 1)]...\n",
      "[2022-09-14 21:35:10,686] keeping 2000000 tokens which were in no less than 0 and no more than 4140000 (=100.0%) documents\n",
      "[2022-09-14 21:35:14,232] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:35:14,303] adding document #4140000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:35:41,862] discarding 52661 tokens: [('artukraine', 1), ('grabowicz', 1), ('klichko', 1), ('ostrovska', 1), ('shershni', 1), ('sosfuture', 1), ('voznytskyi', 1), ('zabolotna', 1), ('заболо', 1), ('лія', 1)]...\n",
      "[2022-09-14 21:35:41,864] keeping 2000000 tokens which were in no less than 0 and no more than 4150000 (=100.0%) documents\n",
      "[2022-09-14 21:35:46,804] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:35:46,910] adding document #4150000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:36:13,551] discarding 41739 tokens: [('naohirokakomi', 1), ('naojin', 1), ('nihonzeneaiyotenkotakoji', 1), ('nobukataarima', 1), ('nobukatsudaughter', 1), ('oseidokantokoji', 1), ('oyamahoei', 1), ('pusthomous', 1), ('renhime', 1), ('ryugenchokookyoshiseiroji', 1)]...\n",
      "[2022-09-14 21:36:13,552] keeping 2000000 tokens which were in no less than 0 and no more than 4160000 (=100.0%) documents\n",
      "[2022-09-14 21:36:17,107] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:36:17,178] adding document #4160000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:36:46,213] discarding 33217 tokens: [('gutartigkeit', 1), ('kinderjahr', 1), ('selbstblendung', 1), ('unreflectingly', 1), ('thermotita', 1), ('tngen', 1), ('αυτόματα', 1), ('brunfiel', 1), ('move_tape_one_square', 1), ('rechow', 1)]...\n",
      "[2022-09-14 21:36:46,216] keeping 2000000 tokens which were in no less than 0 and no more than 4170000 (=100.0%) documents\n",
      "[2022-09-14 21:36:51,219] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:36:51,322] adding document #4170000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:37:19,073] discarding 33326 tokens: [('armégō', 1), ('colaphus', 1), ('connūbium', 1), ('fšāoš', 1), ('kinkʷe', 1), ('pśaws', 1), ('v__v', 1), ('x__y', 1), ('čatwāras', 1), ('čaθwārō', 1)]...\n",
      "[2022-09-14 21:37:19,075] keeping 2000000 tokens which were in no less than 0 and no more than 4180000 (=100.0%) documents\n",
      "[2022-09-14 21:37:22,569] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:37:22,638] adding document #4180000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:37:48,888] discarding 32886 tokens: [('finidings', 1), ('hydroxytriptamine', 1), ('psychotropicalresearch', 1), ('substansia', 1), ('thrombotin', 1), ('vasoactives', 1), ('ccccviii', 1), ('jiltes', 1), ('brêton', 1), ('galtsova', 1)]...\n",
      "[2022-09-14 21:37:48,889] keeping 2000000 tokens which were in no less than 0 and no more than 4190000 (=100.0%) documents\n",
      "[2022-09-14 21:37:52,487] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:37:52,556] adding document #4190000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:38:19,457] discarding 31836 tokens: [('hiyangthang', 1), ('lamkhai', 1), ('wabagai', 1), ('wrongspeak', 1), ('bjäresjö', 1), ('gierhi', 1), ('giardinelli', 1), ('mempo', 1), ('telecino', 1), ('juliandontcheff', 1)]...\n",
      "[2022-09-14 21:38:19,459] keeping 2000000 tokens which were in no less than 0 and no more than 4200000 (=100.0%) documents\n",
      "[2022-09-14 21:38:24,456] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:38:24,559] adding document #4200000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:38:49,564] discarding 33188 tokens: [('aalleh', 1), ('dääd', 1), ('fissääl', 1), ('gesaat', 1), ('saarlandish', 1), ('saarstatut', 1), ('trottwaa', 1), ('urpils', 1), ('ähs', 1), ('aeolian_vision_', 1)]...\n",
      "[2022-09-14 21:38:49,566] keeping 2000000 tokens which were in no less than 0 and no more than 4210000 (=100.0%) documents\n",
      "[2022-09-14 21:38:53,115] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:38:53,184] adding document #4210000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:39:19,329] discarding 36267 tokens: [('professorranchernonprofit', 1), ('programmersoftware', 1), ('stafferlawyer', 1), ('stafferprofessoruniversity', 1), ('staffnonprofit', 1), ('surgeonmedical', 1), ('teacherentrepreneur', 1), ('teacherfarmer', 1), ('teacherlawyer', 1), ('teacherlobbyist', 1)]...\n",
      "[2022-09-14 21:39:19,330] keeping 2000000 tokens which were in no less than 0 and no more than 4220000 (=100.0%) documents\n",
      "[2022-09-14 21:39:22,893] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:39:22,964] adding document #4220000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:39:52,541] discarding 27546 tokens: [('akkkpagtatagpo', 1), ('akkkpangliligaw', 1), ('akkkselosan', 1), ('apektado', 1), ('habulin', 1), ('pagharang', 1), ('paglalapit', 1), ('pagmamakaawa', 1), ('pagtatagpo', 1), ('pangliligaw', 1)]...\n",
      "[2022-09-14 21:39:52,543] keeping 2000000 tokens which were in no less than 0 and no more than 4230000 (=100.0%) documents\n",
      "[2022-09-14 21:39:57,512] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:39:57,614] adding document #4230000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:40:26,136] discarding 31008 tokens: [('càne', 1), ('cànten', 1), ('cēl', 1), ('cōda', 1), ('ddzʲ', 1), ('ddʒʲ', 1), ('delincamos', 1), ('delinquimos', 1), ('desjune', 1), ('dirigimos', 1)]...\n",
      "[2022-09-14 21:40:26,138] keeping 2000000 tokens which were in no less than 0 and no more than 4240000 (=100.0%) documents\n",
      "[2022-09-14 21:40:31,066] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:40:31,168] adding document #4240000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:41:06,026] discarding 33966 tokens: [('cruickshanka', 1), ('disagrrements', 1), ('fisherc', 1), ('fordmr', 1), ('fraynec', 1), ('hillsj', 1), ('jetspress', 1), ('kellettg', 1), ('lensworth', 1), ('messuier', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:41:06,028] keeping 2000000 tokens which were in no less than 0 and no more than 4250000 (=100.0%) documents\n",
      "[2022-09-14 21:41:10,068] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:41:10,171] adding document #4250000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:41:42,920] discarding 35438 tokens: [('cisarska', 1), ('dobřenských', 1), ('gammelnye', 1), ('jihoměstský', 1), ('juditin', 1), ('kolčavka', 1), ('malešický', 1), ('mikropivovar', 1), ('nosticovo', 1), ('pařížskou', 1)]...\n",
      "[2022-09-14 21:41:42,923] keeping 2000000 tokens which were in no less than 0 and no more than 4260000 (=100.0%) documents\n",
      "[2022-09-14 21:41:46,605] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:41:46,676] adding document #4260000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:42:14,414] discarding 29699 tokens: [('rediex', 1), ('cencepcion', 1), ('cinae', 1), ('coapcom', 1), ('comanflot', 1), ('comavan', 1), ('diapser', 1), ('dirmat', 1), ('gaprogen', 1), ('semaer', 1)]...\n",
      "[2022-09-14 21:42:14,416] keeping 2000000 tokens which were in no less than 0 and no more than 4270000 (=100.0%) documents\n",
      "[2022-09-14 21:42:19,796] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:42:19,898] adding document #4270000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:42:46,411] discarding 31543 tokens: [('berardnelli', 1), ('holloland', 1), ('longigo', 1), ('arsenophenylglycine', 1), ('chemotherapia', 1), ('farbenanalytische', 1), ('monophenylrosanilin', 1), ('nongranular', 1), ('sauerstoffbedürfnis', 1), ('sterilisans', 1)]...\n",
      "[2022-09-14 21:42:46,413] keeping 2000000 tokens which were in no less than 0 and no more than 4280000 (=100.0%) documents\n",
      "[2022-09-14 21:42:50,029] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:42:50,099] adding document #4280000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:43:15,355] discarding 28478 tokens: [('addetta', 1), ('subcomittee', 1), ('mahalchik', 1), ('bjorå', 1), ('bøbak', 1), ('granmo', 1), ('klokk', 1), ('lillebjerka', 1), ('lilleslåtten', 1), ('lysebo', 1)]...\n",
      "[2022-09-14 21:43:15,358] keeping 2000000 tokens which were in no less than 0 and no more than 4290000 (=100.0%) documents\n",
      "[2022-09-14 21:43:20,459] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:43:20,560] adding document #4290000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:43:48,508] discarding 28833 tokens: [('drille', 1), ('funbi', 1), ('idowest', 1), ('iskaba', 1), ('klĭtôrĭs', 1), ('legbegbe', 1), ('obadice', 1), ('ponmile', 1), ('shepeteri', 1), ('skuki', 1)]...\n",
      "[2022-09-14 21:43:48,509] keeping 2000000 tokens which were in no less than 0 and no more than 4300000 (=100.0%) documents\n",
      "[2022-09-14 21:43:52,128] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:43:52,197] adding document #4300000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:44:17,490] discarding 31505 tokens: [('godwankar', 1), ('sailasri', 1), ('famalao', 1), ('gracialla', 1), ('absentis', 1), ('acidentatus', 1), ('acroprocessus', 1), ('adligansus', 1), ('baxiantaiensis', 1), ('beloniforis', 1)]...\n",
      "[2022-09-14 21:44:17,492] keeping 2000000 tokens which were in no less than 0 and no more than 4310000 (=100.0%) documents\n",
      "[2022-09-14 21:44:21,107] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:44:21,178] adding document #4310000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:44:46,868] discarding 32277 tokens: [('gerolimatos', 1), ('konmon', 1), ('lazishvili', 1), ('liudvik', 1), ('nohelin', 1), ('pobivanez', 1), ('sholinian', 1), ('africap', 1), ('baraboule', 1), ('bougui', 1)]...\n",
      "[2022-09-14 21:44:46,870] keeping 2000000 tokens which were in no less than 0 and no more than 4320000 (=100.0%) documents\n",
      "[2022-09-14 21:44:51,871] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:44:51,974] adding document #4320000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:45:19,618] discarding 32657 tokens: [('anveeksha', 1), ('arbind', 1), ('atishi', 1), ('bamrah', 1), ('bansiwala', 1), ('bidlan', 1), ('chandolia', 1), ('chandoliya', 1), ('chawaria', 1), ('chawriya', 1)]...\n",
      "[2022-09-14 21:45:19,620] keeping 2000000 tokens which were in no less than 0 and no more than 4330000 (=100.0%) documents\n",
      "[2022-09-14 21:45:24,590] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:45:24,694] adding document #4330000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:45:52,614] discarding 31458 tokens: [('crowdjustice', 1), ('jengba', 1), ('lajc', 1), ('onevirginia', 1), ('mulleras', 1), ('skattum', 1), ('budinov', 1), ('cauly', 1), ('kerchev', 1), ('tchibota', 1)]...\n",
      "[2022-09-14 21:45:52,615] keeping 2000000 tokens which were in no less than 0 and no more than 4340000 (=100.0%) documents\n",
      "[2022-09-14 21:45:56,228] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:45:56,298] adding document #4340000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:46:22,781] discarding 31865 tokens: [('ceppa', 1), ('chièchiaward', 1), ('ecoage', 1), ('madeinitaly', 1), ('magnifeco', 1), ('drsps', 1), ('instrument_id', 1), ('instrument_id_type', 1), ('notation_of_quantity_in_measurement_unit', 1), ('notional_amount', 1)]...\n",
      "[2022-09-14 21:46:22,783] keeping 2000000 tokens which were in no less than 0 and no more than 4350000 (=100.0%) documents\n",
      "[2022-09-14 21:46:27,796] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:46:27,900] adding document #4350000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:46:53,989] discarding 27795 tokens: [('yncm', 1), ('hayslett', 1), ('skyh', 1), ('bawskee', 1), ('errthang', 1), ('glockoma', 1), ('markeyvius', 1), ('bandplay', 1), ('countyii', 1), ('roadlesss', 1)]...\n",
      "[2022-09-14 21:46:53,991] keeping 2000000 tokens which were in no less than 0 and no more than 4360000 (=100.0%) documents\n",
      "[2022-09-14 21:46:59,019] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:46:59,121] adding document #4360000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:47:25,989] discarding 33616 tokens: [('čelinski', 1), ('reapplies', 1), ('basketballnavi', 1), ('shidahe', 1), ('stanced', 1), ('bertholdt', 1), ('areelue', 1), ('chompunuch', 1), ('kaewkalaya', 1), ('nannaphat', 1)]...\n",
      "[2022-09-14 21:47:25,991] keeping 2000000 tokens which were in no less than 0 and no more than 4370000 (=100.0%) documents\n",
      "[2022-09-14 21:47:29,569] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:47:29,638] adding document #4370000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:47:59,709] discarding 32627 tokens: [('如果我沒有傷口', 1), ('最後的承諾', 1), ('畢國建', 1), ('阿冏', 1), ('sholze', 1), ('vezzoni', 1), ('realteam', 1), ('batubhai', 1), ('vasantvijay', 1), ('yuenanlu', 1)]...\n",
      "[2022-09-14 21:47:59,712] keeping 2000000 tokens which were in no less than 0 and no more than 4380000 (=100.0%) documents\n",
      "[2022-09-14 21:48:04,698] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:48:04,800] adding document #4380000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:48:34,757] discarding 32644 tokens: [('qirqisânî', 1), ('qozmím', 1), ('reinstalment', 1), ('retrodating', 1), ('rûfīn', 1), ('sambalut', 1), ('senaccherib', 1), ('shaprūṭ', 1), ('sâmânid', 1), ('tudrun', 1)]...\n",
      "[2022-09-14 21:48:34,758] keeping 2000000 tokens which were in no less than 0 and no more than 4390000 (=100.0%) documents\n",
      "[2022-09-14 21:48:39,766] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:48:39,869] adding document #4390000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:49:06,814] discarding 31130 tokens: [('viscoleantes', 1), ('viukkaat', 1), ('vonkii', 1), ('vossut', 1), ('vridrede', 1), ('vrillant', 1), ('wimmelten', 1), ('wirrten', 1), ('wlizły', 1), ('wocer', 1)]...\n",
      "[2022-09-14 21:49:06,816] keeping 2000000 tokens which were in no less than 0 and no more than 4400000 (=100.0%) documents\n",
      "[2022-09-14 21:49:12,121] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:49:12,224] adding document #4400000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:49:47,771] discarding 85497 tokens: [('darksense', 1), ('durulz', 1), ('eurhol', 1), ('gloranhan', 1), ('gloratha', 1), ('glorontha', 1), ('godtime', 1), ('mostali', 1), ('scorpionmen', 1), ('supplemement', 1)]...\n",
      "[2022-09-14 21:49:47,773] keeping 2000000 tokens which were in no less than 0 and no more than 4410000 (=100.0%) documents\n",
      "[2022-09-14 21:49:51,394] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:49:51,470] adding document #4410000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:50:21,905] discarding 37613 tokens: [('underpronated', 1), ('underpronates', 1), ('underpronator', 1), ('bagiiy', 1), ('bulche', 1), ('daworchig', 1), ('ebaey', 1), ('gatchaper', 1), ('methibaan', 1), ('milignaay', 1)]...\n",
      "[2022-09-14 21:50:21,907] keeping 2000000 tokens which were in no less than 0 and no more than 4420000 (=100.0%) documents\n",
      "[2022-09-14 21:50:26,881] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:50:26,985] adding document #4420000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:50:55,108] discarding 33462 tokens: [('多治氏の娘', 1), ('奇子', 1), ('定省親王', 1), ('布勢氏の娘', 1), ('平等子', 1), ('忠子内親王', 1), ('是忠親王', 1), ('是貞親王', 1), ('桂心女王', 1), ('治部卿', 1)]...\n",
      "[2022-09-14 21:50:55,109] keeping 2000000 tokens which were in no less than 0 and no more than 4430000 (=100.0%) documents\n",
      "[2022-09-14 21:50:58,709] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:50:58,780] adding document #4430000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:51:29,273] discarding 30056 tokens: [('mellcene', 1), ('crosscoders', 1), ('almohaimeed', 1), ('bradyn', 1), ('carmandy', 1), ('farache', 1), ('galiwinku', 1), ('gilledge', 1), ('hevers', 1), ('kotynia', 1)]...\n",
      "[2022-09-14 21:51:29,275] keeping 2000000 tokens which were in no less than 0 and no more than 4440000 (=100.0%) documents\n",
      "[2022-09-14 21:51:33,408] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:51:33,508] adding document #4440000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:52:05,446] discarding 32241 tokens: [('pecou', 1), ('vettorel', 1), ('vfiles', 1), ('عبدالرسول', 1), ('ataev', 1), ('freetape', 1), ('jwal', 1), ('namathi', 1), ('bastlerbeutelhfo', 1), ('dlxxx', 1)]...\n",
      "[2022-09-14 21:52:05,448] keeping 2000000 tokens which were in no less than 0 and no more than 4450000 (=100.0%) documents\n",
      "[2022-09-14 21:52:09,112] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:52:09,182] adding document #4450000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:52:37,115] discarding 29278 tokens: [('feghr', 1), ('specialistion', 1), ('minedamp', 1), ('darcck', 1), ('dmff', 1), ('dnays', 1), ('jintrix', 1), ('遠鍠', 1), ('dictumturned', 1), ('gloucesterwho', 1)]...\n",
      "[2022-09-14 21:52:37,117] keeping 2000000 tokens which were in no less than 0 and no more than 4460000 (=100.0%) documents\n",
      "[2022-09-14 21:52:40,826] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:52:40,895] adding document #4460000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:53:07,818] discarding 32804 tokens: [('dēaþ', 1), ('fɷnetic', 1), ('tubeeround', 1), ('𐐨𐑌𐐲𐑁', 1), ('𐐩', 1), ('𐐪', 1), ('𐐫', 1), ('𐐬', 1), ('𐐮', 1), ('𐐮𐐭', 1)]...\n",
      "[2022-09-14 21:53:07,820] keeping 2000000 tokens which were in no less than 0 and no more than 4470000 (=100.0%) documents\n",
      "[2022-09-14 21:53:11,459] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:53:11,529] adding document #4470000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:53:37,419] discarding 29837 tokens: [('acatholic', 1), ('ayssyrian', 1), ('catholicitas', 1), ('καθολικὴ', 1), ('anninou', 1), ('assgeou', 1), ('atiw', 1), ('btadei', 1), ('ebrye', 1), ('ecopu', 1)]...\n",
      "[2022-09-14 21:53:37,421] keeping 2000000 tokens which were in no less than 0 and no more than 4480000 (=100.0%) documents\n",
      "[2022-09-14 21:53:42,450] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:53:42,551] adding document #4480000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:54:10,799] discarding 32709 tokens: [('dalford', 1), ('owh', 1), ('corvonn', 1), ('erroyl', 1), ('jontae', 1), ('thrivenyc', 1), ('arstark', 1), ('travina', 1), ('balugu', 1), ('basyrah', 1)]...\n",
      "[2022-09-14 21:54:10,801] keeping 2000000 tokens which were in no less than 0 and no more than 4490000 (=100.0%) documents\n",
      "[2022-09-14 21:54:14,413] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:54:14,484] adding document #4490000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:54:42,959] discarding 32345 tokens: [('cotranscriptionally', 1), ('ctif', 1), ('curvispinosus', 1), ('mifd', 1), ('oglcnac', 1), ('petromyzom', 1), ('supervillin', 1), ('svil', 1), ('cereep', 1), ('ecotron', 1)]...\n",
      "[2022-09-14 21:54:42,961] keeping 2000000 tokens which were in no less than 0 and no more than 4500000 (=100.0%) documents\n",
      "[2022-09-14 21:54:47,941] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:54:48,010] adding document #4500000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 21:55:13,592] discarding 35029 tokens: [('kinderschutz', 1), ('arbaʽeen', 1), ('hasania', 1), ('khanqahe', 1), ('kichhauchha', 1), ('sasarami', 1), ('insurtec', 1), ('rightindem', 1), ('campillai', 1), ('funeral_mónica_echeverría_', 1)]...\n",
      "[2022-09-14 21:55:13,593] keeping 2000000 tokens which were in no less than 0 and no more than 4510000 (=100.0%) documents\n",
      "[2022-09-14 21:55:17,159] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:55:17,228] adding document #4510000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:55:42,253] discarding 29354 tokens: [('fmotq', 1), ('idnani', 1), ('anasis', 1), ('kmilsatcom', 1), ('ocaya', 1), ('twae', 1), ('yinwae', 1), ('pagenated', 1), ('chiofaro', 1), ('guidó', 1)]...\n",
      "[2022-09-14 21:55:42,254] keeping 2000000 tokens which were in no less than 0 and no more than 4520000 (=100.0%) documents\n",
      "[2022-09-14 21:55:46,999] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:55:47,101] adding document #4520000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:56:12,918] discarding 29750 tokens: [('ruola', 1), ('ruxuan', 1), ('tiezhang', 1), ('xilun', 1), ('yuebin', 1), ('孤梦', 1), ('山河令', 1), ('山河行', 1), ('无题', 1), ('望天涯', 1)]...\n",
      "[2022-09-14 21:56:12,919] keeping 2000000 tokens which were in no less than 0 and no more than 4530000 (=100.0%) documents\n",
      "[2022-09-14 21:56:16,541] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:56:16,610] adding document #4530000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:56:43,748] discarding 31811 tokens: [('ayrılma', 1), ('hippasilla', 1), ('hippasilta', 1), ('häneltä', 1), ('kamalalta', 1), ('katolta', 1), ('maalta', 1), ('mukavalta', 1), ('postától', 1), ('pátram', 1)]...\n",
      "[2022-09-14 21:56:43,750] keeping 2000000 tokens which were in no less than 0 and no more than 4540000 (=100.0%) documents\n",
      "[2022-09-14 21:56:48,742] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:56:48,843] adding document #4540000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:57:15,399] discarding 35763 tokens: [('educationruth', 1), ('typicalalbedo', 1), ('aanother', 1), ('alabamo', 1), ('albama', 1), ('alebamon', 1), ('alibamou', 1), ('allibamou', 1), ('buddharaksa', 1), ('phoutthavihan', 1)]...\n",
      "[2022-09-14 21:57:15,401] keeping 2000000 tokens which were in no less than 0 and no more than 4550000 (=100.0%) documents\n",
      "[2022-09-14 21:57:20,504] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:57:20,607] adding document #4550000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:57:47,775] discarding 35114 tokens: [('잆', 2), ('잇', 2), ('있', 2), ('잊', 2), ('잋', 2), ('잌', 2), ('잍', 2), ('잎', 2), ('잏', 2), ('작', 2)]...\n",
      "[2022-09-14 21:57:47,777] keeping 2000000 tokens which were in no less than 0 and no more than 4560000 (=100.0%) documents\n",
      "[2022-09-14 21:57:52,838] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:57:52,941] adding document #4560000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:58:19,890] discarding 36680 tokens: [('쒜', 2), ('쒝', 2), ('쒠', 2), ('쒣', 2), ('쒤', 2), ('쒬', 2), ('쒭', 2), ('쒯', 2), ('쒰', 2), ('쒱', 2)]...\n",
      "[2022-09-14 21:58:19,891] keeping 2000000 tokens which were in no less than 0 and no more than 4570000 (=100.0%) documents\n",
      "[2022-09-14 21:58:23,481] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:58:23,551] adding document #4570000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:58:49,221] discarding 31008 tokens: [('숭', 2), ('숮', 2), ('숯', 2), ('숱', 2), ('숳', 2), ('숴', 2), ('숵', 2), ('숸', 2), ('숻', 2), ('숼', 2)]...\n",
      "[2022-09-14 21:58:49,224] keeping 2000000 tokens which were in no less than 0 and no more than 4580000 (=100.0%) documents\n",
      "[2022-09-14 21:58:54,267] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:58:54,369] adding document #4580000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:59:24,621] discarding 42086 tokens: [('봥', 2), ('봬', 2), ('봰', 2), ('봳', 2), ('봴', 2), ('뵀', 2), ('뵈', 2), ('뵉', 2), ('뵊', 2), ('뵌', 2)]...\n",
      "[2022-09-14 21:59:24,623] keeping 2000000 tokens which were in no less than 0 and no more than 4590000 (=100.0%) documents\n",
      "[2022-09-14 21:59:28,641] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:59:28,748] adding document #4590000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:59:55,704] discarding 35189 tokens: [('릴', 2), ('림', 2), ('릾', 2), ('릿', 2), ('맀', 2), ('링', 2), ('맄', 2), ('맆', 2), ('막', 2), ('맊', 2)]...\n",
      "[2022-09-14 21:59:55,705] keeping 2000000 tokens which were in no less than 0 and no more than 4600000 (=100.0%) documents\n",
      "[2022-09-14 21:59:59,279] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 21:59:59,349] adding document #4600000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:00:26,535] discarding 34431 tokens: [('럾', 2), ('럿', 2), ('렀', 2), ('렁', 2), ('렂', 2), ('렄', 2), ('렆', 2), ('렇', 2), ('레', 2), ('렉', 2)]...\n",
      "[2022-09-14 22:00:26,537] keeping 2000000 tokens which were in no less than 0 and no more than 4610000 (=100.0%) documents\n",
      "[2022-09-14 22:00:30,206] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:00:30,277] adding document #4610000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:00:59,089] discarding 35163 tokens: [('댱', 2), ('댸', 2), ('댼', 2), ('덍', 2), ('덕', 2), ('덖', 2), ('던', 2), ('덙', 2), ('덛', 2), ('덜', 2)]...\n",
      "[2022-09-14 22:00:59,090] keeping 2000000 tokens which were in no less than 0 and no more than 4620000 (=100.0%) documents\n",
      "[2022-09-14 22:01:02,705] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:01:02,774] adding document #4620000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:01:29,684] discarding 35124 tokens: [('낌', 2), ('낍', 2), ('낏', 2), ('낐', 2), ('낑', 2), ('낕', 2), ('낙', 2), ('낚', 2), ('낛', 2), ('낟', 2)]...\n",
      "[2022-09-14 22:01:29,686] keeping 2000000 tokens which were in no less than 0 and no more than 4630000 (=100.0%) documents\n",
      "[2022-09-14 22:01:34,722] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:01:34,825] adding document #4630000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:02:00,787] discarding 37524 tokens: [('깽', 2), ('꺁', 2), ('꺄', 2), ('꺅', 2), ('꺈', 2), ('꺋', 2), ('꺌', 2), ('꺍', 2), ('꺗', 2), ('꺙', 2)]...\n",
      "[2022-09-14 22:02:00,789] keeping 2000000 tokens which were in no less than 0 and no more than 4640000 (=100.0%) documents\n",
      "[2022-09-14 22:02:04,374] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:02:04,443] adding document #4640000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 22:02:29,658] discarding 30468 tokens: [('tanpiengco', 2), ('ametuer', 2), ('ghumurishi', 2), ('sagergaio', 2), ('seryogina', 2), ('kaushlesh', 2), ('ווארקא', 2), ('cuales', 2), ('riwsa', 2), ('gainlove', 2)]...\n",
      "[2022-09-14 22:02:29,660] keeping 2000000 tokens which were in no less than 0 and no more than 4650000 (=100.0%) documents\n",
      "[2022-09-14 22:02:33,270] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:02:33,339] adding document #4650000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:02:59,860] discarding 32276 tokens: [('mimograptus', 2), ('tukuhnikivatz', 2), ('comedytheatrical', 2), ('distributorowned', 2), ('erfttal', 2), ('sevenpictures', 2), ('nbš', 2), ('olontigi', 2), ('babawi', 2), ('masakitnakatotohanan', 2)]...\n",
      "[2022-09-14 22:02:59,862] keeping 2000000 tokens which were in no less than 0 and no more than 4660000 (=100.0%) documents\n",
      "[2022-09-14 22:03:04,898] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:03:05,000] adding document #4660000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:03:31,074] discarding 30105 tokens: [('gamelit', 2), ('aawaaraavey', 2), ('annaanenyaa', 2), ('annaathee', 2), ('beynunvi', 2), ('fahathun', 2), ('fenilee', 2), ('gandhemey', 2), ('gothaa', 2), ('haamavaa', 2)]...\n",
      "[2022-09-14 22:03:31,076] keeping 2000000 tokens which were in no less than 0 and no more than 4670000 (=100.0%) documents\n",
      "[2022-09-14 22:03:34,685] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:03:34,754] adding document #4670000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:04:05,076] discarding 31935 tokens: [('boscha', 2), ('gravesfield', 2), ('grudgby', 2), ('palisman', 2), ('palismen', 2), ('wittebane', 2), ('dürüye', 2), ('vettaiyadu', 2), ('villaiyadu', 2), ('randgebiete', 2)]...\n",
      "[2022-09-14 22:04:05,078] keeping 2000000 tokens which were in no less than 0 and no more than 4680000 (=100.0%) documents\n",
      "[2022-09-14 22:04:08,706] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:04:08,777] adding document #4680000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:04:33,511] discarding 32867 tokens: [('yunsung', 2), ('cilenia', 2), ('kwihangana', 2), ('malianov', 2), ('deutschkonservative', 2), ('chotelo', 2), ('mxolisa', 2), ('utatan', 2), ('rudsten', 2), ('daishawn', 2)]...\n",
      "[2022-09-14 22:04:33,513] keeping 2000000 tokens which were in no less than 0 and no more than 4690000 (=100.0%) documents\n",
      "[2022-09-14 22:04:38,830] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:04:38,932] adding document #4690000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:05:02,380] discarding 34150 tokens: [('makolo', 2), ('mbangula', 2), ('muhona', 2), ('brorphine', 2), ('etazen', 2), ('acılar', 2), ('gökkuşağı', 2), ('gülümsemek', 2), ('delabit', 2), ('yabao', 2)]...\n",
      "[2022-09-14 22:05:02,381] keeping 2000000 tokens which were in no less than 0 and no more than 4700000 (=100.0%) documents\n",
      "[2022-09-14 22:05:07,378] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:05:07,480] adding document #4700000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:05:33,560] discarding 35879 tokens: [('nandikecchuram', 2), ('nandivanam', 2), ('sengalvarayapillai', 2), ('soundaranayaki', 2), ('evonymoides', 2), ('kalmayacağım', 2), ('rießler', 2), ('twincore', 2), ('douglasglen', 2), ('seaepisode', 2)]...\n",
      "[2022-09-14 22:05:33,561] keeping 2000000 tokens which were in no less than 0 and no more than 4710000 (=100.0%) documents\n",
      "[2022-09-14 22:05:37,177] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:05:37,247] adding document #4710000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:06:02,194] discarding 32856 tokens: [('shagay', 2), ('soltsem', 2), ('spektakl', 2), ('zhivyot', 2), ('zenitists', 2), ('khadeem', 2), ('istandwithraeesah', 2), ('fracturation', 2), ('pseudoreticulatus', 2), ('staplinisporites', 2)]...\n",
      "[2022-09-14 22:06:02,196] keeping 2000000 tokens which were in no less than 0 and no more than 4720000 (=100.0%) documents\n",
      "[2022-09-14 22:06:06,509] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:06:06,611] adding document #4720000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:06:31,530] discarding 31552 tokens: [('suryoday', 2), ('sithipol', 2), ('baydarov', 2), ('daaye', 2), ('spielhansl', 2), ('двадесетог', 2), ('деветог', 2), ('евро', 2), ('каленић', 2), ('унирекс', 2)]...\n",
      "[2022-09-14 22:06:31,532] keeping 2000000 tokens which were in no less than 0 and no more than 4730000 (=100.0%) documents\n",
      "[2022-09-14 22:06:35,188] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:06:35,259] adding document #4730000 to Dictionary(2000000 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...)\n",
      "[2022-09-14 22:06:37,864] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-14 22:06:38,313] built Dictionary(2001459 unique tokens: ['ability', 'ability#1', 'able', 'abolish', 'abolition']...) from 4730463 documents (total 1993339819 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# loc = 'num'|'lr'|'ent'\n",
    "# pos = True|False\n",
    "# download latest wiki dump\n",
    "#w.download_wiki_dump('en', WIKIXML)\n",
    "\n",
    "# parse wiki dump\n",
    "wiki_sentences = w.WikiSentences(WIKIXML, 'en',lower=True) # Orignal\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='EM',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='DEP',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNS',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNSEM',lower=True,pos=False,loc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'political#1', 'philosophy', 'movement', 'sceptical', 'authority#4', 'reject', 'involuntary', 'coercive', 'form#11', 'hierarchy', 'anarchism', 'call#24', 'abolition', 'state', 'hold#15', 'unnecessary', 'undesirable', 'harmful', 'historically', 'left#7', 'wing', 'movement#3', 'placed', 'farthest', 'left', 'political', 'spectrum', 'usually', 'described', 'alongside', 'libertarian', 'marxism', 'libertarian', 'wing', 'libertarian', 'socialism', 'socialist', 'movement', 'ha', 'strong', 'historical#1', 'association', 'anti', 'capitalism', 'socialism', 'human', 'lived', 'society#1', 'without', 'formal#4', 'hierarchy#1', 'long#5', 'establishment#2', 'formal#3', 'state#3', 'realm', 'empire#1', 'rise#25', 'organised', 'hierarchical', 'body', 'scepticism', 'toward', 'authority#4', 'also', 'rose#18', 'th', 'century', 'self', 'conscious', 'political#1', 'movement#3', 'emerged', 'latter', 'half#1', 'th', 'first#11', 'decade', 'th', 'century', 'anarchist', 'movement', 'flourished', 'part', 'world#5', 'significant', 'role#1', 'worker', 'struggle', 'emancipation', 'various', 'anarchist', 'school#4', 'thought#2', 'formed', 'period', 'anarchist', 'taken', 'part']\n"
     ]
    }
   ],
   "source": [
    "for sent in wiki_sentences:\n",
    "    print(sent[:100])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:34.083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the data\n"
     ]
    }
   ],
   "source": [
    "#sv.save(wiki_sentences,\"wiki_sentences_pos_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_pos\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_loc\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences\") # orignal\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep2\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_uns\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_unsem\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_em\")\n",
    "sv.save(wiki_sentences,\"wiki_sentences_wnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phrase mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:50:46.815932Z",
     "start_time": "2021-05-29T18:50:46.810808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T22:24:31.858045Z",
     "start_time": "2021-05-29T18:51:21.445049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences, min_count=100, threshold=1)\n",
    "frozen_phrases = phrases.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:41:54.697897Z",
     "start_time": "2021-05-30T11:41:38.608475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(phrases,\"gensim_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:06:13.397797Z",
     "start_time": "2020-03-16T22:06:13.394080Z"
    }
   },
   "source": [
    "# Train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:49:02.663973Z",
     "start_time": "2022-03-13T05:49:01.181727Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentences = sv.load(\"wiki_sentences_no\")\n",
    "#temp_sens are cased!!\n",
    "#sentences = sv.load(\"temp_sens\")\n",
    " \n",
    "#sentences = sv.load(\"wiki_sentences\") #Normal sentences using wiki_old.py\n",
    "\n",
    "#Wiki_Sentences_SP are cased\n",
    "#sentences = sv.load(\"Wiki_Sentences_SP\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_loc\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_sp\") #New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_pos\") # not to be used\n",
    "#sentences = sv.load(\"Wiki_sentences_pos_sample\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent\") # New\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent_sample\") # New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_dep\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_dep2\") #New\n",
    "\n",
    "#wiki english sample Cased \n",
    "#sentences = sv.load(\"Wiki_sentences_sp_sample\")\n",
    "#sentences = sv.load(\"wiki_sentences_uns\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_unsem\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_em\") #New\n",
    "sentences = sv.load(\"wiki_sentences_wnet\") #New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:49:02.672912Z",
     "start_time": "2022-03-13T05:49:02.667329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length of token: 1\n"
     ]
    }
   ],
   "source": [
    "#sentences = wiki_sentences\n",
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:09.005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 22:06:54,199] Training model wordnet\n",
      "[2022-09-14 22:06:54,202] collecting all words and their counts\n",
      "[2022-09-14 22:07:01,888] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "[2022-09-14 22:07:56,967] PROGRESS: at sentence #10000, processed 22225402 words, keeping 493671 word types\n",
      "[2022-09-14 22:08:45,212] PROGRESS: at sentence #20000, processed 41943306 words, keeping 699785 word types\n",
      "[2022-09-14 22:09:27,289] PROGRESS: at sentence #30000, processed 58290377 words, keeping 854217 word types\n",
      "[2022-09-14 22:10:05,062] PROGRESS: at sentence #40000, processed 72364821 words, keeping 985354 word types\n",
      "[2022-09-14 22:10:33,961] PROGRESS: at sentence #50000, processed 82317533 words, keeping 1065582 word types\n",
      "[2022-09-14 22:10:50,722] PROGRESS: at sentence #60000, processed 87762646 words, keeping 1085962 word types\n",
      "[2022-09-14 22:11:06,544] PROGRESS: at sentence #70000, processed 92716088 words, keeping 1104451 word types\n",
      "[2022-09-14 22:11:20,963] PROGRESS: at sentence #80000, processed 97063053 words, keeping 1119898 word types\n",
      "[2022-09-14 22:11:55,226] PROGRESS: at sentence #90000, processed 109857833 words, keeping 1217489 word types\n",
      "[2022-09-14 22:12:31,293] PROGRESS: at sentence #100000, processed 123311531 words, keeping 1329619 word types\n",
      "[2022-09-14 22:13:04,049] PROGRESS: at sentence #110000, processed 135129496 words, keeping 1420571 word types\n",
      "[2022-09-14 22:13:34,743] PROGRESS: at sentence #120000, processed 146492745 words, keeping 1510844 word types\n",
      "[2022-09-14 22:14:04,101] PROGRESS: at sentence #130000, processed 157162167 words, keeping 1582757 word types\n",
      "[2022-09-14 22:14:36,533] PROGRESS: at sentence #140000, processed 168503353 words, keeping 1672042 word types\n",
      "[2022-09-14 22:15:05,765] PROGRESS: at sentence #150000, processed 178576587 words, keeping 1757766 word types\n",
      "[2022-09-14 22:15:37,119] PROGRESS: at sentence #160000, processed 189059349 words, keeping 1839623 word types\n",
      "[2022-09-14 22:16:06,362] PROGRESS: at sentence #170000, processed 198799150 words, keeping 1907760 word types\n",
      "[2022-09-14 22:16:32,397] PROGRESS: at sentence #180000, processed 207991006 words, keeping 1968243 word types\n",
      "[2022-09-14 22:16:57,617] PROGRESS: at sentence #190000, processed 216518004 words, keeping 2026793 word types\n",
      "[2022-09-14 22:17:24,957] PROGRESS: at sentence #200000, processed 225298324 words, keeping 2089202 word types\n",
      "[2022-09-14 22:17:51,276] PROGRESS: at sentence #210000, processed 233853089 words, keeping 2141330 word types\n",
      "[2022-09-14 22:18:17,653] PROGRESS: at sentence #220000, processed 242379346 words, keeping 2196461 word types\n",
      "[2022-09-14 22:18:43,188] PROGRESS: at sentence #230000, processed 250583985 words, keeping 2250931 word types\n",
      "[2022-09-14 22:19:08,999] PROGRESS: at sentence #240000, processed 258809291 words, keeping 2298316 word types\n",
      "[2022-09-14 22:19:36,623] PROGRESS: at sentence #250000, processed 267116715 words, keeping 2348166 word types\n",
      "[2022-09-14 22:20:01,973] PROGRESS: at sentence #260000, processed 274875375 words, keeping 2393048 word types\n",
      "[2022-09-14 22:20:37,967] PROGRESS: at sentence #270000, processed 282548869 words, keeping 2442673 word types\n",
      "[2022-09-14 22:21:03,112] PROGRESS: at sentence #280000, processed 289952551 words, keeping 2490393 word types\n",
      "[2022-09-14 22:21:27,726] PROGRESS: at sentence #290000, processed 297453149 words, keeping 2543281 word types\n",
      "[2022-09-14 22:21:51,380] PROGRESS: at sentence #300000, processed 304509322 words, keeping 2591614 word types\n",
      "[2022-09-14 22:22:17,754] PROGRESS: at sentence #310000, processed 311746079 words, keeping 2641804 word types\n",
      "[2022-09-14 22:22:40,903] PROGRESS: at sentence #320000, processed 318838780 words, keeping 2683512 word types\n",
      "[2022-09-14 22:23:03,618] PROGRESS: at sentence #330000, processed 325580253 words, keeping 2722015 word types\n",
      "[2022-09-14 22:23:27,834] PROGRESS: at sentence #340000, processed 332540140 words, keeping 2757331 word types\n",
      "[2022-09-14 22:23:49,682] PROGRESS: at sentence #350000, processed 339237281 words, keeping 2792169 word types\n",
      "[2022-09-14 22:24:12,526] PROGRESS: at sentence #360000, processed 345756712 words, keeping 2827499 word types\n",
      "[2022-09-14 22:24:35,362] PROGRESS: at sentence #370000, processed 352324329 words, keeping 2866143 word types\n",
      "[2022-09-14 22:25:00,059] PROGRESS: at sentence #380000, processed 358941214 words, keeping 2899676 word types\n",
      "[2022-09-14 22:25:21,483] PROGRESS: at sentence #390000, processed 365295812 words, keeping 2934629 word types\n",
      "[2022-09-14 22:25:43,902] PROGRESS: at sentence #400000, processed 371730311 words, keeping 2973874 word types\n",
      "[2022-09-14 22:26:06,062] PROGRESS: at sentence #410000, processed 378071195 words, keeping 3012139 word types\n",
      "[2022-09-14 22:26:28,249] PROGRESS: at sentence #420000, processed 384384853 words, keeping 3044697 word types\n",
      "[2022-09-14 22:26:51,003] PROGRESS: at sentence #430000, processed 390853567 words, keeping 3081020 word types\n",
      "[2022-09-14 22:27:13,066] PROGRESS: at sentence #440000, processed 397022042 words, keeping 3116071 word types\n",
      "[2022-09-14 22:27:35,901] PROGRESS: at sentence #450000, processed 403316708 words, keeping 3152620 word types\n",
      "[2022-09-14 22:27:56,685] PROGRESS: at sentence #460000, processed 409138434 words, keeping 3180903 word types\n",
      "[2022-09-14 22:28:18,322] PROGRESS: at sentence #470000, processed 415005516 words, keeping 3215974 word types\n",
      "[2022-09-14 22:28:39,206] PROGRESS: at sentence #480000, processed 421056967 words, keeping 3246633 word types\n",
      "[2022-09-14 22:29:01,170] PROGRESS: at sentence #490000, processed 427135130 words, keeping 3279927 word types\n",
      "[2022-09-14 22:29:33,059] PROGRESS: at sentence #500000, processed 433174784 words, keeping 3312209 word types\n",
      "[2022-09-14 22:29:55,649] PROGRESS: at sentence #510000, processed 439109641 words, keeping 3344148 word types\n",
      "[2022-09-14 22:30:17,114] PROGRESS: at sentence #520000, processed 444941134 words, keeping 3374619 word types\n",
      "[2022-09-14 22:30:38,951] PROGRESS: at sentence #530000, processed 450658717 words, keeping 3409065 word types\n",
      "[2022-09-14 22:31:00,555] PROGRESS: at sentence #540000, processed 456438290 words, keeping 3438600 word types\n",
      "[2022-09-14 22:31:22,382] PROGRESS: at sentence #550000, processed 462093942 words, keeping 3468403 word types\n",
      "[2022-09-14 22:31:42,673] PROGRESS: at sentence #560000, processed 467664999 words, keeping 3495608 word types\n",
      "[2022-09-14 22:32:03,327] PROGRESS: at sentence #570000, processed 472993364 words, keeping 3520934 word types\n",
      "[2022-09-14 22:32:24,026] PROGRESS: at sentence #580000, processed 478670944 words, keeping 3551141 word types\n",
      "[2022-09-14 22:32:44,390] PROGRESS: at sentence #590000, processed 484030392 words, keeping 3580596 word types\n",
      "[2022-09-14 22:33:06,867] PROGRESS: at sentence #600000, processed 489467500 words, keeping 3609204 word types\n",
      "[2022-09-14 22:33:26,838] PROGRESS: at sentence #610000, processed 494586432 words, keeping 3636745 word types\n",
      "[2022-09-14 22:33:47,569] PROGRESS: at sentence #620000, processed 499869590 words, keeping 3665114 word types\n",
      "[2022-09-14 22:34:10,203] PROGRESS: at sentence #630000, processed 505012029 words, keeping 3692782 word types\n",
      "[2022-09-14 22:34:30,432] PROGRESS: at sentence #640000, processed 510132221 words, keeping 3718399 word types\n",
      "[2022-09-14 22:34:49,284] PROGRESS: at sentence #650000, processed 515061701 words, keeping 3754504 word types\n",
      "[2022-09-14 22:35:07,799] PROGRESS: at sentence #660000, processed 520021433 words, keeping 3780407 word types\n",
      "[2022-09-14 22:35:28,704] PROGRESS: at sentence #670000, processed 525012797 words, keeping 3804499 word types\n",
      "[2022-09-14 22:35:48,122] PROGRESS: at sentence #680000, processed 529979631 words, keeping 3828568 word types\n",
      "[2022-09-14 22:36:09,211] PROGRESS: at sentence #690000, processed 535082089 words, keeping 3871452 word types\n",
      "[2022-09-14 22:36:30,158] PROGRESS: at sentence #700000, processed 540118642 words, keeping 3896818 word types\n",
      "[2022-09-14 22:36:50,867] PROGRESS: at sentence #710000, processed 545134541 words, keeping 3918350 word types\n",
      "[2022-09-14 22:37:15,492] PROGRESS: at sentence #720000, processed 550340584 words, keeping 3943256 word types\n",
      "[2022-09-14 22:37:37,650] PROGRESS: at sentence #730000, processed 555342651 words, keeping 3968827 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 22:37:58,164] PROGRESS: at sentence #740000, processed 560241434 words, keeping 3991909 word types\n",
      "[2022-09-14 22:38:17,680] PROGRESS: at sentence #750000, processed 565232033 words, keeping 4018322 word types\n",
      "[2022-09-14 22:38:38,056] PROGRESS: at sentence #760000, processed 569992094 words, keeping 4041053 word types\n",
      "[2022-09-14 22:38:56,126] PROGRESS: at sentence #770000, processed 574696074 words, keeping 4064701 word types\n",
      "[2022-09-14 22:39:16,133] PROGRESS: at sentence #780000, processed 579515950 words, keeping 4091803 word types\n",
      "[2022-09-14 22:39:39,714] PROGRESS: at sentence #790000, processed 584518584 words, keeping 4115549 word types\n",
      "[2022-09-14 22:39:59,042] PROGRESS: at sentence #800000, processed 589205793 words, keeping 4138292 word types\n",
      "[2022-09-14 22:40:18,902] PROGRESS: at sentence #810000, processed 593965467 words, keeping 4163275 word types\n",
      "[2022-09-14 22:40:38,302] PROGRESS: at sentence #820000, processed 598640393 words, keeping 4186896 word types\n",
      "[2022-09-14 22:40:58,131] PROGRESS: at sentence #830000, processed 603420779 words, keeping 4210630 word types\n",
      "[2022-09-14 22:41:18,555] PROGRESS: at sentence #840000, processed 608173752 words, keeping 4234921 word types\n",
      "[2022-09-14 22:41:36,368] PROGRESS: at sentence #850000, processed 612733687 words, keeping 4257605 word types\n",
      "[2022-09-14 22:41:56,027] PROGRESS: at sentence #860000, processed 617356251 words, keeping 4278252 word types\n",
      "[2022-09-14 22:42:16,974] PROGRESS: at sentence #870000, processed 622102176 words, keeping 4303972 word types\n",
      "[2022-09-14 22:42:36,694] PROGRESS: at sentence #880000, processed 626811145 words, keeping 4324694 word types\n",
      "[2022-09-14 22:42:57,233] PROGRESS: at sentence #890000, processed 631498441 words, keeping 4346730 word types\n",
      "[2022-09-14 22:43:16,148] PROGRESS: at sentence #900000, processed 636040676 words, keeping 4370747 word types\n",
      "[2022-09-14 22:43:35,442] PROGRESS: at sentence #910000, processed 640513571 words, keeping 4390939 word types\n",
      "[2022-09-14 22:43:56,268] PROGRESS: at sentence #920000, processed 645230382 words, keeping 4411131 word types\n",
      "[2022-09-14 22:44:16,774] PROGRESS: at sentence #930000, processed 649938292 words, keeping 4435977 word types\n",
      "[2022-09-14 22:44:35,266] PROGRESS: at sentence #940000, processed 654218846 words, keeping 4458932 word types\n",
      "[2022-09-14 22:44:56,417] PROGRESS: at sentence #950000, processed 658681612 words, keeping 4492271 word types\n",
      "[2022-09-14 22:45:16,209] PROGRESS: at sentence #960000, processed 663092887 words, keeping 4517128 word types\n",
      "[2022-09-14 22:45:36,488] PROGRESS: at sentence #970000, processed 667586598 words, keeping 4543467 word types\n",
      "[2022-09-14 22:45:54,840] PROGRESS: at sentence #980000, processed 671895231 words, keeping 4565522 word types\n",
      "[2022-09-14 22:46:14,689] PROGRESS: at sentence #990000, processed 676196050 words, keeping 4590793 word types\n",
      "[2022-09-14 22:46:31,683] PROGRESS: at sentence #1000000, processed 680100705 words, keeping 4619774 word types\n",
      "[2022-09-14 22:46:52,668] PROGRESS: at sentence #1010000, processed 684572046 words, keeping 4646323 word types\n",
      "[2022-09-14 22:47:13,010] PROGRESS: at sentence #1020000, processed 688858346 words, keeping 4668976 word types\n",
      "[2022-09-14 22:47:31,658] PROGRESS: at sentence #1030000, processed 693013146 words, keeping 4688321 word types\n",
      "[2022-09-14 22:47:49,642] PROGRESS: at sentence #1040000, processed 697167972 words, keeping 4708250 word types\n",
      "[2022-09-14 22:48:08,114] PROGRESS: at sentence #1050000, processed 701234684 words, keeping 4726379 word types\n",
      "[2022-09-14 22:48:27,102] PROGRESS: at sentence #1060000, processed 705581106 words, keeping 4746313 word types\n",
      "[2022-09-14 22:48:43,833] PROGRESS: at sentence #1070000, processed 709406545 words, keeping 4765143 word types\n",
      "[2022-09-14 22:49:01,815] PROGRESS: at sentence #1080000, processed 713434299 words, keeping 4785697 word types\n",
      "[2022-09-14 22:49:19,594] PROGRESS: at sentence #1090000, processed 717454859 words, keeping 4806258 word types\n",
      "[2022-09-14 22:49:36,214] PROGRESS: at sentence #1100000, processed 721378347 words, keeping 4825698 word types\n",
      "[2022-09-14 22:49:55,291] PROGRESS: at sentence #1110000, processed 725691109 words, keeping 4844630 word types\n",
      "[2022-09-14 22:50:14,408] PROGRESS: at sentence #1120000, processed 729822226 words, keeping 4862630 word types\n",
      "[2022-09-14 22:50:34,604] PROGRESS: at sentence #1130000, processed 733868900 words, keeping 4882046 word types\n",
      "[2022-09-14 22:50:55,316] PROGRESS: at sentence #1140000, processed 737956650 words, keeping 4902307 word types\n",
      "[2022-09-14 22:51:13,479] PROGRESS: at sentence #1150000, processed 741915228 words, keeping 4921139 word types\n",
      "[2022-09-14 22:51:31,277] PROGRESS: at sentence #1160000, processed 745994482 words, keeping 4943718 word types\n",
      "[2022-09-14 22:51:49,417] PROGRESS: at sentence #1170000, processed 750027273 words, keeping 4961646 word types\n",
      "[2022-09-14 22:52:08,242] PROGRESS: at sentence #1180000, processed 754009847 words, keeping 4981158 word types\n",
      "[2022-09-14 22:52:26,997] PROGRESS: at sentence #1190000, processed 757836954 words, keeping 5005067 word types\n",
      "[2022-09-14 22:52:48,410] PROGRESS: at sentence #1200000, processed 762109132 words, keeping 5022603 word types\n",
      "[2022-09-14 22:53:07,416] PROGRESS: at sentence #1210000, processed 766078015 words, keeping 5041694 word types\n",
      "[2022-09-14 22:53:26,654] PROGRESS: at sentence #1220000, processed 770137758 words, keeping 5059762 word types\n",
      "[2022-09-14 22:53:46,953] PROGRESS: at sentence #1230000, processed 774158691 words, keeping 5076532 word types\n",
      "[2022-09-14 22:54:06,252] PROGRESS: at sentence #1240000, processed 778196573 words, keeping 5094118 word types\n",
      "[2022-09-14 22:54:26,130] PROGRESS: at sentence #1250000, processed 782219922 words, keeping 5112334 word types\n",
      "[2022-09-14 22:54:46,177] PROGRESS: at sentence #1260000, processed 786475447 words, keeping 5131361 word types\n",
      "[2022-09-14 22:55:05,724] PROGRESS: at sentence #1270000, processed 790602432 words, keeping 5152723 word types\n",
      "[2022-09-14 22:55:25,376] PROGRESS: at sentence #1280000, processed 794706125 words, keeping 5171059 word types\n",
      "[2022-09-14 22:55:43,802] PROGRESS: at sentence #1290000, processed 798676315 words, keeping 5195237 word types\n",
      "[2022-09-14 22:56:01,691] PROGRESS: at sentence #1300000, processed 802490517 words, keeping 5211497 word types\n",
      "[2022-09-14 22:56:19,512] PROGRESS: at sentence #1310000, processed 806413569 words, keeping 5229344 word types\n",
      "[2022-09-14 22:56:39,170] PROGRESS: at sentence #1320000, processed 810428371 words, keeping 5248819 word types\n",
      "[2022-09-14 22:56:56,723] PROGRESS: at sentence #1330000, processed 814232832 words, keeping 5265522 word types\n",
      "[2022-09-14 22:57:16,445] PROGRESS: at sentence #1340000, processed 818250133 words, keeping 5285951 word types\n",
      "[2022-09-14 22:57:38,003] PROGRESS: at sentence #1350000, processed 822284675 words, keeping 5304707 word types\n",
      "[2022-09-14 22:57:57,537] PROGRESS: at sentence #1360000, processed 826187223 words, keeping 5324824 word types\n",
      "[2022-09-14 22:58:17,478] PROGRESS: at sentence #1370000, processed 830074016 words, keeping 5347818 word types\n",
      "[2022-09-14 22:58:35,871] PROGRESS: at sentence #1380000, processed 833854613 words, keeping 5365635 word types\n",
      "[2022-09-14 22:58:54,276] PROGRESS: at sentence #1390000, processed 837830677 words, keeping 5383770 word types\n",
      "[2022-09-14 22:59:13,015] PROGRESS: at sentence #1400000, processed 841873808 words, keeping 5401027 word types\n",
      "[2022-09-14 22:59:33,245] PROGRESS: at sentence #1410000, processed 846519058 words, keeping 5419475 word types\n",
      "[2022-09-14 22:59:53,246] PROGRESS: at sentence #1420000, processed 850566719 words, keeping 5435113 word types\n",
      "[2022-09-14 23:00:12,234] PROGRESS: at sentence #1430000, processed 854492939 words, keeping 5452876 word types\n",
      "[2022-09-14 23:00:31,854] PROGRESS: at sentence #1440000, processed 858708932 words, keeping 5470242 word types\n",
      "[2022-09-14 23:00:50,660] PROGRESS: at sentence #1450000, processed 862367445 words, keeping 5486700 word types\n",
      "[2022-09-14 23:01:06,842] PROGRESS: at sentence #1460000, processed 865711104 words, keeping 5502580 word types\n",
      "[2022-09-14 23:01:27,509] PROGRESS: at sentence #1470000, processed 869634068 words, keeping 5522973 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 23:01:47,382] PROGRESS: at sentence #1480000, processed 873553867 words, keeping 5539175 word types\n",
      "[2022-09-14 23:02:05,917] PROGRESS: at sentence #1490000, processed 877320658 words, keeping 5555018 word types\n",
      "[2022-09-14 23:02:25,443] PROGRESS: at sentence #1500000, processed 880903221 words, keeping 5573630 word types\n",
      "[2022-09-14 23:02:44,791] PROGRESS: at sentence #1510000, processed 884553019 words, keeping 5590212 word types\n",
      "[2022-09-14 23:03:02,748] PROGRESS: at sentence #1520000, processed 887829588 words, keeping 5604631 word types\n",
      "[2022-09-14 23:03:19,299] PROGRESS: at sentence #1530000, processed 890984556 words, keeping 5616846 word types\n",
      "[2022-09-14 23:03:39,887] PROGRESS: at sentence #1540000, processed 894572785 words, keeping 5632142 word types\n",
      "[2022-09-14 23:03:59,326] PROGRESS: at sentence #1550000, processed 898200468 words, keeping 5651069 word types\n",
      "[2022-09-14 23:04:17,822] PROGRESS: at sentence #1560000, processed 901869151 words, keeping 5670898 word types\n",
      "[2022-09-14 23:04:37,603] PROGRESS: at sentence #1570000, processed 905690654 words, keeping 5691331 word types\n",
      "[2022-09-14 23:04:58,215] PROGRESS: at sentence #1580000, processed 909276563 words, keeping 5706882 word types\n",
      "[2022-09-14 23:05:19,305] PROGRESS: at sentence #1590000, processed 912942198 words, keeping 5726095 word types\n",
      "[2022-09-14 23:05:38,153] PROGRESS: at sentence #1600000, processed 916525601 words, keeping 5740661 word types\n",
      "[2022-09-14 23:05:58,116] PROGRESS: at sentence #1610000, processed 920283892 words, keeping 5757473 word types\n",
      "[2022-09-14 23:06:17,615] PROGRESS: at sentence #1620000, processed 923939235 words, keeping 5773207 word types\n",
      "[2022-09-14 23:06:35,897] PROGRESS: at sentence #1630000, processed 927459477 words, keeping 5795891 word types\n",
      "[2022-09-14 23:06:54,229] PROGRESS: at sentence #1640000, processed 931085341 words, keeping 5819014 word types\n",
      "[2022-09-14 23:07:12,546] PROGRESS: at sentence #1650000, processed 934808091 words, keeping 5832396 word types\n",
      "[2022-09-14 23:07:32,221] PROGRESS: at sentence #1660000, processed 938702091 words, keeping 5848935 word types\n",
      "[2022-09-14 23:07:54,310] PROGRESS: at sentence #1670000, processed 942588390 words, keeping 5865304 word types\n",
      "[2022-09-14 23:08:12,497] PROGRESS: at sentence #1680000, processed 946164182 words, keeping 5883535 word types\n",
      "[2022-09-14 23:08:31,137] PROGRESS: at sentence #1690000, processed 949595094 words, keeping 5897680 word types\n",
      "[2022-09-14 23:08:50,960] PROGRESS: at sentence #1700000, processed 952707250 words, keeping 5919774 word types\n",
      "[2022-09-14 23:09:13,364] PROGRESS: at sentence #1710000, processed 956116844 words, keeping 5946218 word types\n",
      "[2022-09-14 23:09:33,760] PROGRESS: at sentence #1720000, processed 959762408 words, keeping 5968355 word types\n",
      "[2022-09-14 23:09:53,197] PROGRESS: at sentence #1730000, processed 963304948 words, keeping 5984587 word types\n",
      "[2022-09-14 23:10:16,099] PROGRESS: at sentence #1740000, processed 966857153 words, keeping 6000498 word types\n",
      "[2022-09-14 23:10:37,226] PROGRESS: at sentence #1750000, processed 970461761 words, keeping 6016252 word types\n",
      "[2022-09-14 23:10:59,789] PROGRESS: at sentence #1760000, processed 974110953 words, keeping 6030771 word types\n",
      "[2022-09-14 23:11:21,082] PROGRESS: at sentence #1770000, processed 977760911 words, keeping 6046641 word types\n",
      "[2022-09-14 23:11:43,858] PROGRESS: at sentence #1780000, processed 981587715 words, keeping 6065288 word types\n",
      "[2022-09-14 23:12:10,526] PROGRESS: at sentence #1790000, processed 985259048 words, keeping 6079596 word types\n",
      "[2022-09-14 23:12:39,593] PROGRESS: at sentence #1800000, processed 988805810 words, keeping 6092206 word types\n",
      "[2022-09-14 23:12:59,108] PROGRESS: at sentence #1810000, processed 992442244 words, keeping 6106908 word types\n",
      "[2022-09-14 23:13:17,258] PROGRESS: at sentence #1820000, processed 995956001 words, keeping 6121125 word types\n",
      "[2022-09-14 23:13:36,632] PROGRESS: at sentence #1830000, processed 999663310 words, keeping 6138095 word types\n",
      "[2022-09-14 23:13:55,335] PROGRESS: at sentence #1840000, processed 1003289660 words, keeping 6151756 word types\n",
      "[2022-09-14 23:14:14,471] PROGRESS: at sentence #1850000, processed 1006940968 words, keeping 6165093 word types\n",
      "[2022-09-14 23:14:31,454] PROGRESS: at sentence #1860000, processed 1010355382 words, keeping 6179368 word types\n",
      "[2022-09-14 23:14:51,278] PROGRESS: at sentence #1870000, processed 1014038506 words, keeping 6194095 word types\n",
      "[2022-09-14 23:15:11,077] PROGRESS: at sentence #1880000, processed 1017568023 words, keeping 6208438 word types\n",
      "[2022-09-14 23:15:30,238] PROGRESS: at sentence #1890000, processed 1021054049 words, keeping 6223631 word types\n",
      "[2022-09-14 23:15:51,092] PROGRESS: at sentence #1900000, processed 1024825150 words, keeping 6239044 word types\n",
      "[2022-09-14 23:16:11,617] PROGRESS: at sentence #1910000, processed 1028324989 words, keeping 6253310 word types\n",
      "[2022-09-14 23:16:30,701] PROGRESS: at sentence #1920000, processed 1031844835 words, keeping 6268424 word types\n",
      "[2022-09-14 23:16:52,660] PROGRESS: at sentence #1930000, processed 1035463989 words, keeping 6298628 word types\n",
      "[2022-09-14 23:17:11,696] PROGRESS: at sentence #1940000, processed 1038763448 words, keeping 6311386 word types\n",
      "[2022-09-14 23:17:35,744] PROGRESS: at sentence #1950000, processed 1043008551 words, keeping 6328175 word types\n",
      "[2022-09-14 23:17:57,980] PROGRESS: at sentence #1960000, processed 1046660241 words, keeping 6343220 word types\n",
      "[2022-09-14 23:18:19,713] PROGRESS: at sentence #1970000, processed 1050654885 words, keeping 6358354 word types\n",
      "[2022-09-14 23:18:39,413] PROGRESS: at sentence #1980000, processed 1054446170 words, keeping 6374563 word types\n",
      "[2022-09-14 23:19:02,075] PROGRESS: at sentence #1990000, processed 1058076256 words, keeping 6392485 word types\n",
      "[2022-09-14 23:19:27,139] PROGRESS: at sentence #2000000, processed 1061774732 words, keeping 6409045 word types\n",
      "[2022-09-14 23:19:48,524] PROGRESS: at sentence #2010000, processed 1065280048 words, keeping 6424869 word types\n",
      "[2022-09-14 23:20:07,317] PROGRESS: at sentence #2020000, processed 1068744639 words, keeping 6438001 word types\n",
      "[2022-09-14 23:20:26,445] PROGRESS: at sentence #2030000, processed 1072347676 words, keeping 6454208 word types\n",
      "[2022-09-14 23:20:44,855] PROGRESS: at sentence #2040000, processed 1075907244 words, keeping 6471861 word types\n",
      "[2022-09-14 23:21:04,600] PROGRESS: at sentence #2050000, processed 1079531952 words, keeping 6487746 word types\n",
      "[2022-09-14 23:21:25,443] PROGRESS: at sentence #2060000, processed 1083218311 words, keeping 6506122 word types\n",
      "[2022-09-14 23:21:46,061] PROGRESS: at sentence #2070000, processed 1086965673 words, keeping 6521447 word types\n",
      "[2022-09-14 23:22:08,885] PROGRESS: at sentence #2080000, processed 1090765418 words, keeping 6536799 word types\n",
      "[2022-09-14 23:22:27,857] PROGRESS: at sentence #2090000, processed 1094360943 words, keeping 6552967 word types\n",
      "[2022-09-14 23:22:48,499] PROGRESS: at sentence #2100000, processed 1098119479 words, keeping 6572500 word types\n",
      "[2022-09-14 23:23:09,323] PROGRESS: at sentence #2110000, processed 1101802383 words, keeping 6588232 word types\n",
      "[2022-09-14 23:23:28,679] PROGRESS: at sentence #2120000, processed 1105233721 words, keeping 6606035 word types\n",
      "[2022-09-14 23:23:47,003] PROGRESS: at sentence #2130000, processed 1108409150 words, keeping 6617716 word types\n",
      "[2022-09-14 23:24:05,350] PROGRESS: at sentence #2140000, processed 1111774961 words, keeping 6633883 word types\n",
      "[2022-09-14 23:24:24,975] PROGRESS: at sentence #2150000, processed 1115134853 words, keeping 6663478 word types\n",
      "[2022-09-14 23:24:45,536] PROGRESS: at sentence #2160000, processed 1118779214 words, keeping 6680192 word types\n",
      "[2022-09-14 23:25:04,713] PROGRESS: at sentence #2170000, processed 1122165401 words, keeping 6696642 word types\n",
      "[2022-09-14 23:25:24,532] PROGRESS: at sentence #2180000, processed 1125750565 words, keeping 6713875 word types\n",
      "[2022-09-14 23:25:46,125] PROGRESS: at sentence #2190000, processed 1129345674 words, keeping 6726888 word types\n",
      "[2022-09-14 23:26:05,127] PROGRESS: at sentence #2200000, processed 1132818328 words, keeping 6741194 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 23:26:26,650] PROGRESS: at sentence #2210000, processed 1136525524 words, keeping 6754740 word types\n",
      "[2022-09-14 23:26:48,322] PROGRESS: at sentence #2220000, processed 1140118919 words, keeping 6771361 word types\n",
      "[2022-09-14 23:27:17,768] PROGRESS: at sentence #2230000, processed 1143988367 words, keeping 6787536 word types\n",
      "[2022-09-14 23:27:41,145] PROGRESS: at sentence #2240000, processed 1147606940 words, keeping 6803192 word types\n",
      "[2022-09-14 23:27:59,975] PROGRESS: at sentence #2250000, processed 1150948558 words, keeping 6827112 word types\n",
      "[2022-09-14 23:28:21,116] PROGRESS: at sentence #2260000, processed 1154576694 words, keeping 6843641 word types\n",
      "[2022-09-14 23:28:41,197] PROGRESS: at sentence #2270000, processed 1158112213 words, keeping 6861814 word types\n",
      "[2022-09-14 23:29:01,272] PROGRESS: at sentence #2280000, processed 1161618690 words, keeping 6877688 word types\n",
      "[2022-09-14 23:29:21,622] PROGRESS: at sentence #2290000, processed 1165220484 words, keeping 6892859 word types\n",
      "[2022-09-14 23:29:43,996] PROGRESS: at sentence #2300000, processed 1168976713 words, keeping 6908747 word types\n",
      "[2022-09-14 23:30:04,460] PROGRESS: at sentence #2310000, processed 1172621742 words, keeping 6924229 word types\n",
      "[2022-09-14 23:30:23,557] PROGRESS: at sentence #2320000, processed 1176076958 words, keeping 6943932 word types\n",
      "[2022-09-14 23:30:43,805] PROGRESS: at sentence #2330000, processed 1179751565 words, keeping 6959677 word types\n",
      "[2022-09-14 23:31:03,137] PROGRESS: at sentence #2340000, processed 1183199266 words, keeping 6975500 word types\n",
      "[2022-09-14 23:31:24,248] PROGRESS: at sentence #2350000, processed 1186820777 words, keeping 6991647 word types\n",
      "[2022-09-14 23:31:45,899] PROGRESS: at sentence #2360000, processed 1190818253 words, keeping 7004592 word types\n",
      "[2022-09-14 23:32:04,843] PROGRESS: at sentence #2370000, processed 1194298072 words, keeping 7017775 word types\n",
      "[2022-09-14 23:32:27,554] PROGRESS: at sentence #2380000, processed 1197889779 words, keeping 7032291 word types\n",
      "[2022-09-14 23:32:49,549] PROGRESS: at sentence #2390000, processed 1201638427 words, keeping 7047836 word types\n",
      "[2022-09-14 23:33:07,858] PROGRESS: at sentence #2400000, processed 1204971125 words, keeping 7064304 word types\n",
      "[2022-09-14 23:33:26,959] PROGRESS: at sentence #2410000, processed 1208507332 words, keeping 7080436 word types\n",
      "[2022-09-14 23:33:46,965] PROGRESS: at sentence #2420000, processed 1211981447 words, keeping 7095120 word types\n",
      "[2022-09-14 23:34:06,213] PROGRESS: at sentence #2430000, processed 1215654459 words, keeping 7111847 word types\n",
      "[2022-09-14 23:34:26,394] PROGRESS: at sentence #2440000, processed 1219220477 words, keeping 7127290 word types\n",
      "[2022-09-14 23:34:43,808] PROGRESS: at sentence #2450000, processed 1222546065 words, keeping 7139906 word types\n",
      "[2022-09-14 23:35:02,664] PROGRESS: at sentence #2460000, processed 1225801562 words, keeping 7152587 word types\n",
      "[2022-09-14 23:35:23,668] PROGRESS: at sentence #2470000, processed 1229066789 words, keeping 7166463 word types\n",
      "[2022-09-14 23:35:42,825] PROGRESS: at sentence #2480000, processed 1232462622 words, keeping 7179597 word types\n",
      "[2022-09-14 23:36:02,839] PROGRESS: at sentence #2490000, processed 1236155246 words, keeping 7237241 word types\n",
      "[2022-09-14 23:36:29,906] PROGRESS: at sentence #2500000, processed 1239830985 words, keeping 7255749 word types\n",
      "[2022-09-14 23:36:51,897] PROGRESS: at sentence #2510000, processed 1243410409 words, keeping 7274611 word types\n",
      "[2022-09-14 23:37:13,123] PROGRESS: at sentence #2520000, processed 1246789793 words, keeping 7291231 word types\n",
      "[2022-09-14 23:37:33,860] PROGRESS: at sentence #2530000, processed 1250315357 words, keeping 7307770 word types\n",
      "[2022-09-14 23:37:52,195] PROGRESS: at sentence #2540000, processed 1253718400 words, keeping 7320998 word types\n",
      "[2022-09-14 23:38:10,179] PROGRESS: at sentence #2550000, processed 1257095546 words, keeping 7335087 word types\n",
      "[2022-09-14 23:38:35,239] PROGRESS: at sentence #2560000, processed 1260453226 words, keeping 7351491 word types\n",
      "[2022-09-14 23:38:59,513] PROGRESS: at sentence #2570000, processed 1263956905 words, keeping 7369927 word types\n",
      "[2022-09-14 23:39:21,049] PROGRESS: at sentence #2580000, processed 1267594828 words, keeping 7399667 word types\n",
      "[2022-09-14 23:39:42,293] PROGRESS: at sentence #2590000, processed 1271100424 words, keeping 7414886 word types\n",
      "[2022-09-14 23:40:03,823] PROGRESS: at sentence #2600000, processed 1274522225 words, keeping 7429356 word types\n",
      "[2022-09-14 23:40:22,186] PROGRESS: at sentence #2610000, processed 1277703963 words, keeping 7441696 word types\n",
      "[2022-09-14 23:40:41,249] PROGRESS: at sentence #2620000, processed 1280714813 words, keeping 7453418 word types\n",
      "[2022-09-14 23:41:05,588] PROGRESS: at sentence #2630000, processed 1284048547 words, keeping 7466227 word types\n",
      "[2022-09-14 23:41:34,876] PROGRESS: at sentence #2640000, processed 1287710950 words, keeping 7482963 word types\n",
      "[2022-09-14 23:41:56,573] PROGRESS: at sentence #2650000, processed 1291209533 words, keeping 7499941 word types\n",
      "[2022-09-14 23:42:18,683] PROGRESS: at sentence #2660000, processed 1294832625 words, keeping 7516364 word types\n",
      "[2022-09-14 23:42:38,782] PROGRESS: at sentence #2670000, processed 1298396244 words, keeping 7531136 word types\n",
      "[2022-09-14 23:42:58,694] PROGRESS: at sentence #2680000, processed 1301903548 words, keeping 7546225 word types\n",
      "[2022-09-14 23:43:20,405] PROGRESS: at sentence #2690000, processed 1306854968 words, keeping 7565434 word types\n",
      "[2022-09-14 23:43:41,862] PROGRESS: at sentence #2700000, processed 1310462941 words, keeping 7581240 word types\n",
      "[2022-09-14 23:44:00,653] PROGRESS: at sentence #2710000, processed 1314022398 words, keeping 7597641 word types\n",
      "[2022-09-14 23:44:22,913] PROGRESS: at sentence #2720000, processed 1317677932 words, keeping 7615199 word types\n",
      "[2022-09-14 23:44:42,755] PROGRESS: at sentence #2730000, processed 1321398546 words, keeping 7630750 word types\n",
      "[2022-09-14 23:45:03,348] PROGRESS: at sentence #2740000, processed 1325089869 words, keeping 7645528 word types\n",
      "[2022-09-14 23:45:24,213] PROGRESS: at sentence #2750000, processed 1328784593 words, keeping 7661467 word types\n",
      "[2022-09-14 23:45:46,270] PROGRESS: at sentence #2760000, processed 1332454953 words, keeping 7676017 word types\n",
      "[2022-09-14 23:46:08,285] PROGRESS: at sentence #2770000, processed 1336063795 words, keeping 7691869 word types\n",
      "[2022-09-14 23:46:29,244] PROGRESS: at sentence #2780000, processed 1339556046 words, keeping 7708055 word types\n",
      "[2022-09-14 23:46:54,531] PROGRESS: at sentence #2790000, processed 1343057442 words, keeping 7724154 word types\n",
      "[2022-09-14 23:47:16,517] PROGRESS: at sentence #2800000, processed 1346444713 words, keeping 7744089 word types\n",
      "[2022-09-14 23:47:40,053] PROGRESS: at sentence #2810000, processed 1349893580 words, keeping 7759314 word types\n",
      "[2022-09-14 23:48:00,734] PROGRESS: at sentence #2820000, processed 1353088219 words, keeping 7773421 word types\n",
      "[2022-09-14 23:48:23,870] PROGRESS: at sentence #2830000, processed 1356606228 words, keeping 7789478 word types\n",
      "[2022-09-14 23:48:46,476] PROGRESS: at sentence #2840000, processed 1360103585 words, keeping 7806034 word types\n",
      "[2022-09-14 23:49:07,555] PROGRESS: at sentence #2850000, processed 1363599337 words, keeping 7822227 word types\n",
      "[2022-09-14 23:49:30,668] PROGRESS: at sentence #2860000, processed 1367196138 words, keeping 7838325 word types\n",
      "[2022-09-14 23:49:51,008] PROGRESS: at sentence #2870000, processed 1370665735 words, keeping 7852748 word types\n",
      "[2022-09-14 23:50:12,286] PROGRESS: at sentence #2880000, processed 1374208465 words, keeping 7868465 word types\n",
      "[2022-09-14 23:50:35,080] PROGRESS: at sentence #2890000, processed 1377860557 words, keeping 7882332 word types\n",
      "[2022-09-14 23:50:56,134] PROGRESS: at sentence #2900000, processed 1381403690 words, keeping 7897233 word types\n",
      "[2022-09-14 23:51:15,810] PROGRESS: at sentence #2910000, processed 1384817930 words, keeping 7910493 word types\n",
      "[2022-09-14 23:51:39,548] PROGRESS: at sentence #2920000, processed 1388432175 words, keeping 7924547 word types\n",
      "[2022-09-14 23:52:03,324] PROGRESS: at sentence #2930000, processed 1391897502 words, keeping 7939145 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-14 23:52:24,665] PROGRESS: at sentence #2940000, processed 1395535912 words, keeping 7953384 word types\n",
      "[2022-09-14 23:52:46,916] PROGRESS: at sentence #2950000, processed 1399068831 words, keeping 7970715 word types\n",
      "[2022-09-14 23:53:08,069] PROGRESS: at sentence #2960000, processed 1402587199 words, keeping 7985385 word types\n",
      "[2022-09-14 23:53:29,660] PROGRESS: at sentence #2970000, processed 1406168781 words, keeping 8001924 word types\n",
      "[2022-09-14 23:53:51,934] PROGRESS: at sentence #2980000, processed 1409767963 words, keeping 8017041 word types\n",
      "[2022-09-14 23:54:14,260] PROGRESS: at sentence #2990000, processed 1413254212 words, keeping 8028778 word types\n",
      "[2022-09-14 23:54:38,413] PROGRESS: at sentence #3000000, processed 1416750382 words, keeping 8043647 word types\n",
      "[2022-09-14 23:54:58,853] PROGRESS: at sentence #3010000, processed 1419970080 words, keeping 8057128 word types\n",
      "[2022-09-14 23:55:20,826] PROGRESS: at sentence #3020000, processed 1423371147 words, keeping 8072362 word types\n",
      "[2022-09-14 23:55:42,614] PROGRESS: at sentence #3030000, processed 1426678972 words, keeping 8084419 word types\n",
      "[2022-09-14 23:56:06,209] PROGRESS: at sentence #3040000, processed 1430181285 words, keeping 8103237 word types\n",
      "[2022-09-14 23:56:27,033] PROGRESS: at sentence #3050000, processed 1433480444 words, keeping 8118374 word types\n",
      "[2022-09-14 23:56:48,940] PROGRESS: at sentence #3060000, processed 1436952996 words, keeping 8132114 word types\n",
      "[2022-09-14 23:57:11,930] PROGRESS: at sentence #3070000, processed 1440276893 words, keeping 8146294 word types\n",
      "[2022-09-14 23:57:38,405] PROGRESS: at sentence #3080000, processed 1444006980 words, keeping 8160786 word types\n",
      "[2022-09-14 23:57:59,955] PROGRESS: at sentence #3090000, processed 1447611358 words, keeping 8175515 word types\n",
      "[2022-09-14 23:58:25,084] PROGRESS: at sentence #3100000, processed 1451211332 words, keeping 8189629 word types\n",
      "[2022-09-14 23:58:52,980] PROGRESS: at sentence #3110000, processed 1454982588 words, keeping 8204379 word types\n",
      "[2022-09-14 23:59:16,627] PROGRESS: at sentence #3120000, processed 1458755395 words, keeping 8219412 word types\n",
      "[2022-09-14 23:59:39,313] PROGRESS: at sentence #3130000, processed 1462224949 words, keeping 8232419 word types\n",
      "[2022-09-15 00:00:10,488] PROGRESS: at sentence #3140000, processed 1465897550 words, keeping 8247570 word types\n",
      "[2022-09-15 00:00:35,462] PROGRESS: at sentence #3150000, processed 1469563659 words, keeping 8260214 word types\n",
      "[2022-09-15 00:00:58,296] PROGRESS: at sentence #3160000, processed 1472973817 words, keeping 8273467 word types\n",
      "[2022-09-15 00:01:21,758] PROGRESS: at sentence #3170000, processed 1476560379 words, keeping 8285987 word types\n",
      "[2022-09-15 00:01:42,870] PROGRESS: at sentence #3180000, processed 1480180878 words, keeping 8300341 word types\n",
      "[2022-09-15 00:02:06,567] PROGRESS: at sentence #3190000, processed 1483859816 words, keeping 8312886 word types\n",
      "[2022-09-15 00:02:30,491] PROGRESS: at sentence #3200000, processed 1487635858 words, keeping 8325584 word types\n",
      "[2022-09-15 00:02:59,089] PROGRESS: at sentence #3210000, processed 1491566182 words, keeping 8340223 word types\n",
      "[2022-09-15 00:03:21,915] PROGRESS: at sentence #3220000, processed 1495257595 words, keeping 8353273 word types\n",
      "[2022-09-15 00:03:44,032] PROGRESS: at sentence #3230000, processed 1498791148 words, keeping 8367902 word types\n",
      "[2022-09-15 00:04:05,502] PROGRESS: at sentence #3240000, processed 1502200211 words, keeping 8382390 word types\n",
      "[2022-09-15 00:04:27,570] PROGRESS: at sentence #3250000, processed 1505682298 words, keeping 8400086 word types\n",
      "[2022-09-15 00:04:49,116] PROGRESS: at sentence #3260000, processed 1509103246 words, keeping 8415166 word types\n",
      "[2022-09-15 00:05:10,627] PROGRESS: at sentence #3270000, processed 1512660258 words, keeping 8428238 word types\n",
      "[2022-09-15 00:05:32,101] PROGRESS: at sentence #3280000, processed 1516072828 words, keeping 8442419 word types\n",
      "[2022-09-15 00:05:53,819] PROGRESS: at sentence #3290000, processed 1519516280 words, keeping 8453947 word types\n",
      "[2022-09-15 00:06:16,860] PROGRESS: at sentence #3300000, processed 1523110372 words, keeping 8469075 word types\n",
      "[2022-09-15 00:06:39,950] PROGRESS: at sentence #3310000, processed 1526802233 words, keeping 8484871 word types\n",
      "[2022-09-15 00:07:00,824] PROGRESS: at sentence #3320000, processed 1530194346 words, keeping 8497143 word types\n",
      "[2022-09-15 00:07:21,803] PROGRESS: at sentence #3330000, processed 1533533242 words, keeping 8508709 word types\n",
      "[2022-09-15 00:07:44,303] PROGRESS: at sentence #3340000, processed 1537082285 words, keeping 8524362 word types\n",
      "[2022-09-15 00:08:04,067] PROGRESS: at sentence #3350000, processed 1540411115 words, keeping 8536855 word types\n",
      "[2022-09-15 00:08:26,924] PROGRESS: at sentence #3360000, processed 1543958816 words, keeping 8548947 word types\n",
      "[2022-09-15 00:08:48,035] PROGRESS: at sentence #3370000, processed 1547478289 words, keeping 8562475 word types\n",
      "[2022-09-15 00:09:10,133] PROGRESS: at sentence #3380000, processed 1551050947 words, keeping 8576496 word types\n",
      "[2022-09-15 00:09:33,225] PROGRESS: at sentence #3390000, processed 1554561527 words, keeping 8591310 word types\n",
      "[2022-09-15 00:09:54,297] PROGRESS: at sentence #3400000, processed 1557989064 words, keeping 8608094 word types\n",
      "[2022-09-15 00:10:14,387] PROGRESS: at sentence #3410000, processed 1561197160 words, keeping 8622801 word types\n",
      "[2022-09-15 00:10:36,564] PROGRESS: at sentence #3420000, processed 1564431480 words, keeping 8635185 word types\n",
      "[2022-09-15 00:10:57,375] PROGRESS: at sentence #3430000, processed 1567733581 words, keeping 8647096 word types\n",
      "[2022-09-15 00:11:19,895] PROGRESS: at sentence #3440000, processed 1571116894 words, keeping 8659806 word types\n",
      "[2022-09-15 00:11:45,942] PROGRESS: at sentence #3450000, processed 1574586610 words, keeping 8674776 word types\n",
      "[2022-09-15 00:12:07,080] PROGRESS: at sentence #3460000, processed 1577755423 words, keeping 8687656 word types\n",
      "[2022-09-15 00:12:26,715] PROGRESS: at sentence #3470000, processed 1581024765 words, keeping 8702246 word types\n",
      "[2022-09-15 00:12:47,182] PROGRESS: at sentence #3480000, processed 1584369679 words, keeping 8715883 word types\n",
      "[2022-09-15 00:13:10,735] PROGRESS: at sentence #3490000, processed 1587900671 words, keeping 8739762 word types\n",
      "[2022-09-15 00:13:32,489] PROGRESS: at sentence #3500000, processed 1591407095 words, keeping 8751617 word types\n",
      "[2022-09-15 00:13:54,789] PROGRESS: at sentence #3510000, processed 1594718091 words, keeping 8763912 word types\n",
      "[2022-09-15 00:14:17,309] PROGRESS: at sentence #3520000, processed 1598174706 words, keeping 8778161 word types\n",
      "[2022-09-15 00:14:38,504] PROGRESS: at sentence #3530000, processed 1601552386 words, keeping 8789840 word types\n",
      "[2022-09-15 00:14:58,242] PROGRESS: at sentence #3540000, processed 1605004635 words, keeping 8801607 word types\n",
      "[2022-09-15 00:15:19,187] PROGRESS: at sentence #3550000, processed 1608423229 words, keeping 8813491 word types\n",
      "[2022-09-15 00:15:42,730] PROGRESS: at sentence #3560000, processed 1611893018 words, keeping 8827307 word types\n",
      "[2022-09-15 00:16:04,759] PROGRESS: at sentence #3570000, processed 1615371871 words, keeping 8839408 word types\n",
      "[2022-09-15 00:16:33,516] PROGRESS: at sentence #3580000, processed 1618822943 words, keeping 8850201 word types\n",
      "[2022-09-15 00:16:57,368] PROGRESS: at sentence #3590000, processed 1622337550 words, keeping 8862015 word types\n",
      "[2022-09-15 00:17:18,749] PROGRESS: at sentence #3600000, processed 1625673729 words, keeping 8873390 word types\n",
      "[2022-09-15 00:17:39,962] PROGRESS: at sentence #3610000, processed 1628826583 words, keeping 8887728 word types\n",
      "[2022-09-15 00:18:00,883] PROGRESS: at sentence #3620000, processed 1631945846 words, keeping 8898440 word types\n",
      "[2022-09-15 00:18:20,556] PROGRESS: at sentence #3630000, processed 1634939357 words, keeping 8909900 word types\n",
      "[2022-09-15 00:18:40,377] PROGRESS: at sentence #3640000, processed 1637962579 words, keeping 8922290 word types\n",
      "[2022-09-15 00:18:58,232] PROGRESS: at sentence #3650000, processed 1640891558 words, keeping 8933500 word types\n",
      "[2022-09-15 00:19:21,062] PROGRESS: at sentence #3660000, processed 1644481742 words, keeping 8948739 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-15 00:19:42,909] PROGRESS: at sentence #3670000, processed 1647834352 words, keeping 8960541 word types\n",
      "[2022-09-15 00:20:05,273] PROGRESS: at sentence #3680000, processed 1651205762 words, keeping 8971849 word types\n",
      "[2022-09-15 00:20:26,362] PROGRESS: at sentence #3690000, processed 1654536101 words, keeping 8983907 word types\n",
      "[2022-09-15 00:20:46,496] PROGRESS: at sentence #3700000, processed 1657794552 words, keeping 8995818 word types\n",
      "[2022-09-15 00:21:08,105] PROGRESS: at sentence #3710000, processed 1661189683 words, keeping 9008698 word types\n",
      "[2022-09-15 00:21:33,882] PROGRESS: at sentence #3720000, processed 1664767763 words, keeping 9021713 word types\n",
      "[2022-09-15 00:21:57,241] PROGRESS: at sentence #3730000, processed 1668231750 words, keeping 9035295 word types\n",
      "[2022-09-15 00:22:18,292] PROGRESS: at sentence #3740000, processed 1671590727 words, keeping 9048410 word types\n",
      "[2022-09-15 00:22:38,068] PROGRESS: at sentence #3750000, processed 1674869710 words, keeping 9064300 word types\n",
      "[2022-09-15 00:23:08,404] PROGRESS: at sentence #3760000, processed 1678112567 words, keeping 9076445 word types\n",
      "[2022-09-15 00:23:27,851] PROGRESS: at sentence #3770000, processed 1681288150 words, keeping 9088099 word types\n",
      "[2022-09-15 00:23:49,866] PROGRESS: at sentence #3780000, processed 1684452284 words, keeping 9101715 word types\n",
      "[2022-09-15 00:24:06,935] PROGRESS: at sentence #3790000, processed 1687415222 words, keeping 9112144 word types\n",
      "[2022-09-15 00:24:27,491] PROGRESS: at sentence #3800000, processed 1690548874 words, keeping 9124795 word types\n",
      "[2022-09-15 00:24:52,055] PROGRESS: at sentence #3810000, processed 1693942817 words, keeping 9139114 word types\n",
      "[2022-09-15 00:25:11,894] PROGRESS: at sentence #3820000, processed 1697237238 words, keeping 9152989 word types\n",
      "[2022-09-15 00:25:31,833] PROGRESS: at sentence #3830000, processed 1700390084 words, keeping 9166505 word types\n",
      "[2022-09-15 00:25:51,302] PROGRESS: at sentence #3840000, processed 1703393069 words, keeping 9177745 word types\n",
      "[2022-09-15 00:26:12,664] PROGRESS: at sentence #3850000, processed 1706623354 words, keeping 9195124 word types\n",
      "[2022-09-15 00:26:32,831] PROGRESS: at sentence #3860000, processed 1709783329 words, keeping 9207410 word types\n",
      "[2022-09-15 00:26:55,286] PROGRESS: at sentence #3870000, processed 1712925858 words, keeping 9219882 word types\n",
      "[2022-09-15 00:27:18,162] PROGRESS: at sentence #3880000, processed 1716189260 words, keeping 9231998 word types\n",
      "[2022-09-15 00:27:41,622] PROGRESS: at sentence #3890000, processed 1719765909 words, keeping 9245572 word types\n",
      "[2022-09-15 00:28:02,818] PROGRESS: at sentence #3900000, processed 1722981903 words, keeping 9258067 word types\n",
      "[2022-09-15 00:28:24,185] PROGRESS: at sentence #3910000, processed 1726064631 words, keeping 9271003 word types\n",
      "[2022-09-15 00:28:45,913] PROGRESS: at sentence #3920000, processed 1729281928 words, keeping 9283831 word types\n",
      "[2022-09-15 00:29:10,765] PROGRESS: at sentence #3930000, processed 1732762199 words, keeping 9296853 word types\n",
      "[2022-09-15 00:29:31,912] PROGRESS: at sentence #3940000, processed 1735978465 words, keeping 9310683 word types\n",
      "[2022-09-15 00:29:52,915] PROGRESS: at sentence #3950000, processed 1739079826 words, keeping 9321714 word types\n",
      "[2022-09-15 00:30:16,570] PROGRESS: at sentence #3960000, processed 1742377150 words, keeping 9335043 word types\n",
      "[2022-09-15 00:30:37,095] PROGRESS: at sentence #3970000, processed 1745568881 words, keeping 9345825 word types\n",
      "[2022-09-15 00:30:59,987] PROGRESS: at sentence #3980000, processed 1748836196 words, keeping 9358907 word types\n",
      "[2022-09-15 00:31:22,518] PROGRESS: at sentence #3990000, processed 1752274171 words, keeping 9371101 word types\n",
      "[2022-09-15 00:31:46,160] PROGRESS: at sentence #4000000, processed 1755587991 words, keeping 9384653 word types\n",
      "[2022-09-15 00:32:06,784] PROGRESS: at sentence #4010000, processed 1758659650 words, keeping 9397302 word types\n",
      "[2022-09-15 00:32:28,664] PROGRESS: at sentence #4020000, processed 1761778215 words, keeping 9410717 word types\n",
      "[2022-09-15 00:32:59,034] PROGRESS: at sentence #4030000, processed 1765066298 words, keeping 9423786 word types\n",
      "[2022-09-15 00:33:22,692] PROGRESS: at sentence #4040000, processed 1768331169 words, keeping 9437591 word types\n",
      "[2022-09-15 00:33:51,141] PROGRESS: at sentence #4050000, processed 1771779584 words, keeping 9460647 word types\n",
      "[2022-09-15 00:34:15,406] PROGRESS: at sentence #4060000, processed 1775113175 words, keeping 9473724 word types\n",
      "[2022-09-15 00:34:37,071] PROGRESS: at sentence #4070000, processed 1778265350 words, keeping 9485484 word types\n",
      "[2022-09-15 00:34:59,430] PROGRESS: at sentence #4080000, processed 1781482697 words, keeping 9498768 word types\n",
      "[2022-09-15 00:35:24,377] PROGRESS: at sentence #4090000, processed 1784960784 words, keeping 9512597 word types\n",
      "[2022-09-15 00:35:46,241] PROGRESS: at sentence #4100000, processed 1788201490 words, keeping 9524644 word types\n",
      "[2022-09-15 00:36:07,102] PROGRESS: at sentence #4110000, processed 1791370794 words, keeping 9539814 word types\n",
      "[2022-09-15 00:36:29,779] PROGRESS: at sentence #4120000, processed 1794600088 words, keeping 9553212 word types\n",
      "[2022-09-15 00:36:51,943] PROGRESS: at sentence #4130000, processed 1797749781 words, keeping 9570611 word types\n",
      "[2022-09-15 00:37:14,242] PROGRESS: at sentence #4140000, processed 1801014246 words, keeping 9585553 word types\n",
      "[2022-09-15 00:37:39,700] PROGRESS: at sentence #4150000, processed 1804233104 words, keeping 9608477 word types\n",
      "[2022-09-15 00:38:03,240] PROGRESS: at sentence #4160000, processed 1807405750 words, keeping 9626482 word types\n",
      "[2022-09-15 00:38:29,350] PROGRESS: at sentence #4170000, processed 1810925205 words, keeping 9639192 word types\n",
      "[2022-09-15 00:38:53,396] PROGRESS: at sentence #4180000, processed 1814447937 words, keeping 9651604 word types\n",
      "[2022-09-15 00:39:16,566] PROGRESS: at sentence #4190000, processed 1817791853 words, keeping 9663539 word types\n",
      "[2022-09-15 00:39:38,551] PROGRESS: at sentence #4200000, processed 1821065818 words, keeping 9675293 word types\n",
      "[2022-09-15 00:40:00,127] PROGRESS: at sentence #4210000, processed 1824336422 words, keeping 9688068 word types\n",
      "[2022-09-15 00:40:22,079] PROGRESS: at sentence #4220000, processed 1827832427 words, keeping 9703127 word types\n",
      "[2022-09-15 00:40:49,167] PROGRESS: at sentence #4230000, processed 1831195932 words, keeping 9713661 word types\n",
      "[2022-09-15 00:41:13,818] PROGRESS: at sentence #4240000, processed 1834530254 words, keeping 9725933 word types\n",
      "[2022-09-15 00:41:46,304] PROGRESS: at sentence #4250000, processed 1838188630 words, keeping 9738542 word types\n",
      "[2022-09-15 00:42:15,563] PROGRESS: at sentence #4260000, processed 1841807007 words, keeping 9751932 word types\n",
      "[2022-09-15 00:42:39,821] PROGRESS: at sentence #4270000, processed 1845126466 words, keeping 9762985 word types\n",
      "[2022-09-15 00:43:02,444] PROGRESS: at sentence #4280000, processed 1848362663 words, keeping 9774325 word types\n",
      "[2022-09-15 00:43:23,021] PROGRESS: at sentence #4290000, processed 1851533385 words, keeping 9784576 word types\n",
      "[2022-09-15 00:43:46,752] PROGRESS: at sentence #4300000, processed 1854853334 words, keeping 9795133 word types\n",
      "[2022-09-15 00:44:09,373] PROGRESS: at sentence #4310000, processed 1858147455 words, keeping 9807257 word types\n",
      "[2022-09-15 00:44:32,196] PROGRESS: at sentence #4320000, processed 1861569283 words, keeping 9819582 word types\n",
      "[2022-09-15 00:44:55,637] PROGRESS: at sentence #4330000, processed 1864873684 words, keeping 9832098 word types\n",
      "[2022-09-15 00:45:20,326] PROGRESS: at sentence #4340000, processed 1868147735 words, keeping 9843403 word types\n",
      "[2022-09-15 00:45:43,441] PROGRESS: at sentence #4350000, processed 1871403935 words, keeping 9854986 word types\n",
      "[2022-09-15 00:46:05,338] PROGRESS: at sentence #4360000, processed 1874554712 words, keeping 9865087 word types\n",
      "[2022-09-15 00:46:28,144] PROGRESS: at sentence #4370000, processed 1877761330 words, keeping 9878025 word types\n",
      "[2022-09-15 00:46:53,469] PROGRESS: at sentence #4380000, processed 1881114032 words, keeping 9890758 word types\n",
      "[2022-09-15 00:47:19,508] PROGRESS: at sentence #4390000, processed 1884413002 words, keeping 9903704 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-15 00:47:43,013] PROGRESS: at sentence #4400000, processed 1887662242 words, keeping 9916099 word types\n",
      "[2022-09-15 00:48:09,354] PROGRESS: at sentence #4410000, processed 1891376063 words, keeping 9967928 word types\n",
      "[2022-09-15 00:48:36,559] PROGRESS: at sentence #4420000, processed 1894867412 words, keeping 9981518 word types\n",
      "[2022-09-15 00:49:02,236] PROGRESS: at sentence #4430000, processed 1898177609 words, keeping 9993638 word types\n",
      "[2022-09-15 00:49:29,055] PROGRESS: at sentence #4440000, processed 1901647529 words, keeping 10004658 word types\n",
      "[2022-09-15 00:49:59,769] PROGRESS: at sentence #4450000, processed 1905380846 words, keeping 10016277 word types\n",
      "[2022-09-15 00:50:24,033] PROGRESS: at sentence #4460000, processed 1908657844 words, keeping 10027745 word types\n",
      "[2022-09-15 00:50:47,865] PROGRESS: at sentence #4470000, processed 1911868096 words, keeping 10040513 word types\n",
      "[2022-09-15 00:51:09,114] PROGRESS: at sentence #4480000, processed 1915174304 words, keeping 10051584 word types\n",
      "[2022-09-15 00:51:43,057] PROGRESS: at sentence #4490000, processed 1918649545 words, keeping 10063858 word types\n",
      "[2022-09-15 00:52:07,424] PROGRESS: at sentence #4500000, processed 1922028921 words, keeping 10076578 word types\n",
      "[2022-09-15 00:52:31,106] PROGRESS: at sentence #4510000, processed 1925269576 words, keeping 10089084 word types\n",
      "[2022-09-15 00:52:53,109] PROGRESS: at sentence #4520000, processed 1928378745 words, keeping 10099758 word types\n",
      "[2022-09-15 00:53:15,087] PROGRESS: at sentence #4530000, processed 1931377356 words, keeping 10111262 word types\n",
      "[2022-09-15 00:53:38,374] PROGRESS: at sentence #4540000, processed 1934486438 words, keeping 10124816 word types\n",
      "[2022-09-15 00:54:00,845] PROGRESS: at sentence #4550000, processed 1937788972 words, keeping 10139710 word types\n",
      "[2022-09-15 00:54:24,333] PROGRESS: at sentence #4560000, processed 1941032517 words, keeping 10153243 word types\n",
      "[2022-09-15 00:54:48,925] PROGRESS: at sentence #4570000, processed 1944509354 words, keeping 10166065 word types\n",
      "[2022-09-15 00:55:10,841] PROGRESS: at sentence #4580000, processed 1947624881 words, keeping 10177639 word types\n",
      "[2022-09-15 00:55:38,104] PROGRESS: at sentence #4590000, processed 1951133443 words, keeping 10192218 word types\n",
      "[2022-09-15 00:56:02,935] PROGRESS: at sentence #4600000, processed 1954339107 words, keeping 10205549 word types\n",
      "[2022-09-15 00:56:29,123] PROGRESS: at sentence #4610000, processed 1957616905 words, keeping 10219129 word types\n",
      "[2022-09-15 00:56:53,882] PROGRESS: at sentence #4620000, processed 1960818778 words, keeping 10233291 word types\n",
      "[2022-09-15 00:57:18,418] PROGRESS: at sentence #4630000, processed 1963981571 words, keeping 10247235 word types\n",
      "[2022-09-15 00:57:41,157] PROGRESS: at sentence #4640000, processed 1967120324 words, keeping 10262890 word types\n",
      "[2022-09-15 00:58:02,243] PROGRESS: at sentence #4650000, processed 1970168974 words, keeping 10273668 word types\n",
      "[2022-09-15 00:58:25,574] PROGRESS: at sentence #4660000, processed 1973186652 words, keeping 10286268 word types\n",
      "[2022-09-15 00:58:48,306] PROGRESS: at sentence #4670000, processed 1976165322 words, keeping 10298018 word types\n",
      "[2022-09-15 00:59:16,176] PROGRESS: at sentence #4680000, processed 1979072664 words, keeping 10310365 word types\n",
      "[2022-09-15 00:59:37,674] PROGRESS: at sentence #4690000, processed 1981887687 words, keeping 10323188 word types\n",
      "[2022-09-15 00:59:57,576] PROGRESS: at sentence #4700000, processed 1984671704 words, keeping 10337134 word types\n",
      "[2022-09-15 01:00:20,085] PROGRESS: at sentence #4710000, processed 1987668419 words, keeping 10349771 word types\n",
      "[2022-09-15 01:00:41,513] PROGRESS: at sentence #4720000, processed 1990461749 words, keeping 10362717 word types\n",
      "[2022-09-15 01:01:02,534] PROGRESS: at sentence #4730000, processed 1993218622 words, keeping 10374768 word types\n",
      "[2022-09-15 01:01:05,445] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-15 01:01:06,151] collected 10375361 word types from a corpus of 1993339819 raw words and 4730463 sentences\n",
      "[2022-09-15 01:01:06,152] Loading a fresh vocabulary\n",
      "[2022-09-15 01:01:18,623] effective_min_count=5 retains 2571899 unique words (24% of original 10375361, drops 7803462)\n",
      "[2022-09-15 01:01:18,624] effective_min_count=5 leaves 1981479700 word corpus (99% of original 1993339819, drops 11860119)\n",
      "[2022-09-15 01:01:26,838] deleting the raw counts dictionary of 10375361 items\n",
      "[2022-09-15 01:01:27,331] sample=0.001 downsamples 4 most-common words\n",
      "[2022-09-15 01:01:27,332] downsampling leaves estimated 1956804741 word corpus (98.8% of prior 1981479700)\n",
      "[2022-09-15 01:01:37,773] estimated required memory for 2571899 words and 300 dimensions: 7458507100 bytes\n",
      "[2022-09-15 01:01:37,774] resetting layer weights\n",
      "[2022-09-15 01:10:41,013] training model with 40 workers on 2571899 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "[2022-09-15 01:19:04,091] EPOCHs No. 1 - PROGRESS: at 1.00% examples, 157265 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:22:43,993] EPOCHs No. 1 - PROGRESS: at 2.00% examples, 158629 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:28:07,901] EPOCHs No. 1 - PROGRESS: at 3.00% examples, 160204 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:32:37,022] EPOCHs No. 1 - PROGRESS: at 4.00% examples, 161249 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:36:36,293] EPOCHs No. 1 - PROGRESS: at 5.00% examples, 161783 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:40:24,883] EPOCHs No. 1 - PROGRESS: at 6.00% examples, 161425 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-15 01:43:44,663] EPOCHs No. 1 - PROGRESS: at 7.00% examples, 161724 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:46:55,118] EPOCHs No. 1 - PROGRESS: at 8.00% examples, 161800 words/s, in_qsize 0, out_qsize 2\n",
      "[2022-09-15 01:49:58,945] EPOCHs No. 1 - PROGRESS: at 9.00% examples, 161810 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:52:56,549] EPOCHs No. 1 - PROGRESS: at 10.00% examples, 161613 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:55:53,347] EPOCHs No. 1 - PROGRESS: at 11.00% examples, 161304 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 01:58:39,000] EPOCHs No. 1 - PROGRESS: at 12.00% examples, 161115 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-15 02:01:15,305] EPOCHs No. 1 - PROGRESS: at 13.00% examples, 161033 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-15 02:03:45,878] EPOCHs No. 1 - PROGRESS: at 14.00% examples, 160806 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:06:16,276] EPOCHs No. 1 - PROGRESS: at 15.00% examples, 160550 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:08:49,887] EPOCHs No. 1 - PROGRESS: at 16.00% examples, 160141 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:11:14,984] EPOCHs No. 1 - PROGRESS: at 17.00% examples, 159882 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:13:38,006] EPOCHs No. 1 - PROGRESS: at 18.00% examples, 159602 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:15:58,845] EPOCHs No. 1 - PROGRESS: at 19.00% examples, 159402 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:18:18,318] EPOCHs No. 1 - PROGRESS: at 20.00% examples, 159108 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:20:32,456] EPOCHs No. 1 - PROGRESS: at 21.00% examples, 158812 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:22:46,152] EPOCHs No. 1 - PROGRESS: at 22.00% examples, 158435 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:24:52,464] EPOCHs No. 1 - PROGRESS: at 23.00% examples, 158168 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:27:02,620] EPOCHs No. 1 - PROGRESS: at 24.00% examples, 157862 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-15 02:29:05,958] EPOCHs No. 1 - PROGRESS: at 25.00% examples, 157659 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:31:15,262] EPOCHs No. 1 - PROGRESS: at 26.00% examples, 157323 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:33:27,968] EPOCHs No. 1 - PROGRESS: at 27.00% examples, 156957 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:35:31,848] EPOCHs No. 1 - PROGRESS: at 28.00% examples, 156716 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:37:42,312] EPOCHs No. 1 - PROGRESS: at 29.00% examples, 156310 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-15 02:39:54,565] EPOCHs No. 1 - PROGRESS: at 30.00% examples, 156019 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:41:58,633] EPOCHs No. 1 - PROGRESS: at 31.00% examples, 155714 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:44:06,063] EPOCHs No. 1 - PROGRESS: at 32.00% examples, 155280 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:46:04,274] EPOCHs No. 1 - PROGRESS: at 33.00% examples, 154872 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:48:12,855] EPOCHs No. 1 - PROGRESS: at 34.00% examples, 154386 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:50:17,400] EPOCHs No. 1 - PROGRESS: at 35.00% examples, 153997 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:52:27,978] EPOCHs No. 1 - PROGRESS: at 36.00% examples, 153414 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:54:39,067] EPOCHs No. 1 - PROGRESS: at 37.00% examples, 152833 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:57:07,602] EPOCHs No. 1 - PROGRESS: at 38.00% examples, 151955 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 02:59:13,267] EPOCHs No. 1 - PROGRESS: at 39.00% examples, 151605 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:01:15,683] EPOCHs No. 1 - PROGRESS: at 40.00% examples, 151288 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:03:20,500] EPOCHs No. 1 - PROGRESS: at 41.00% examples, 150916 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:05:43,296] EPOCHs No. 1 - PROGRESS: at 42.00% examples, 150409 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:07:54,933] EPOCHs No. 1 - PROGRESS: at 43.00% examples, 149947 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:10:06,389] EPOCHs No. 1 - PROGRESS: at 44.00% examples, 149590 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:12:12,595] EPOCHs No. 1 - PROGRESS: at 45.00% examples, 149256 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:14:17,626] EPOCHs No. 1 - PROGRESS: at 46.00% examples, 148900 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:16:29,596] EPOCHs No. 1 - PROGRESS: at 47.00% examples, 148508 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:18:44,801] EPOCHs No. 1 - PROGRESS: at 48.00% examples, 148056 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:20:54,417] EPOCHs No. 1 - PROGRESS: at 49.00% examples, 147745 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:23:08,013] EPOCHs No. 1 - PROGRESS: at 50.00% examples, 147389 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:25:15,996] EPOCHs No. 1 - PROGRESS: at 51.00% examples, 147086 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:27:19,199] EPOCHs No. 1 - PROGRESS: at 52.00% examples, 146836 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:29:31,078] EPOCHs No. 1 - PROGRESS: at 53.00% examples, 146461 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:31:38,655] EPOCHs No. 1 - PROGRESS: at 54.00% examples, 146146 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:33:57,221] EPOCHs No. 1 - PROGRESS: at 55.00% examples, 145671 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:36:11,157] EPOCHs No. 1 - PROGRESS: at 56.00% examples, 145198 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:38:29,158] EPOCHs No. 1 - PROGRESS: at 57.00% examples, 144966 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:40:43,213] EPOCHs No. 1 - PROGRESS: at 58.00% examples, 144686 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:42:59,026] EPOCHs No. 1 - PROGRESS: at 59.00% examples, 144355 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:45:13,873] EPOCHs No. 1 - PROGRESS: at 60.00% examples, 143964 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:47:28,299] EPOCHs No. 1 - PROGRESS: at 61.00% examples, 143655 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:49:43,453] EPOCHs No. 1 - PROGRESS: at 62.00% examples, 143333 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:51:58,589] EPOCHs No. 1 - PROGRESS: at 63.00% examples, 143038 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:54:12,509] EPOCHs No. 1 - PROGRESS: at 64.00% examples, 142685 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:56:30,021] EPOCHs No. 1 - PROGRESS: at 65.00% examples, 142316 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 03:58:56,126] EPOCHs No. 1 - PROGRESS: at 66.00% examples, 141946 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:01:26,681] EPOCHs No. 1 - PROGRESS: at 67.00% examples, 141477 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:03:53,483] EPOCHs No. 1 - PROGRESS: at 68.00% examples, 141163 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:06:07,896] EPOCHs No. 1 - PROGRESS: at 69.00% examples, 140881 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:08:25,532] EPOCHs No. 1 - PROGRESS: at 70.00% examples, 140604 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:10:38,759] EPOCHs No. 1 - PROGRESS: at 71.00% examples, 140343 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-15 04:12:51,243] EPOCHs No. 1 - PROGRESS: at 72.00% examples, 140117 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:15:05,053] EPOCHs No. 1 - PROGRESS: at 73.00% examples, 139812 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:17:14,789] EPOCHs No. 1 - PROGRESS: at 74.00% examples, 139595 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:19:24,614] EPOCHs No. 1 - PROGRESS: at 75.00% examples, 139390 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:21:41,088] EPOCHs No. 1 - PROGRESS: at 76.00% examples, 139134 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:23:46,195] EPOCHs No. 1 - PROGRESS: at 77.00% examples, 138863 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:26:01,269] EPOCHs No. 1 - PROGRESS: at 78.00% examples, 138584 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:28:16,738] EPOCHs No. 1 - PROGRESS: at 79.00% examples, 138330 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:30:32,544] EPOCHs No. 1 - PROGRESS: at 80.00% examples, 138006 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:32:39,490] EPOCHs No. 1 - PROGRESS: at 81.00% examples, 137781 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:34:48,784] EPOCHs No. 1 - PROGRESS: at 82.00% examples, 137523 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:37:04,260] EPOCHs No. 1 - PROGRESS: at 83.00% examples, 137259 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:39:18,429] EPOCHs No. 1 - PROGRESS: at 84.00% examples, 136981 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:41:32,173] EPOCHs No. 1 - PROGRESS: at 85.00% examples, 136721 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:43:57,500] EPOCHs No. 1 - PROGRESS: at 86.00% examples, 136359 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:46:13,881] EPOCHs No. 1 - PROGRESS: at 87.00% examples, 136099 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:48:32,908] EPOCHs No. 1 - PROGRESS: at 88.00% examples, 135787 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:51:01,817] EPOCHs No. 1 - PROGRESS: at 89.00% examples, 135445 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:53:41,041] EPOCHs No. 1 - PROGRESS: at 90.00% examples, 135048 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:55:58,444] EPOCHs No. 1 - PROGRESS: at 91.00% examples, 134798 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 04:58:20,705] EPOCHs No. 1 - PROGRESS: at 92.00% examples, 134517 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:00:45,157] EPOCHs No. 1 - PROGRESS: at 93.00% examples, 134203 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:03:25,716] EPOCHs No. 1 - PROGRESS: at 94.00% examples, 133838 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:05:47,647] EPOCHs No. 1 - PROGRESS: at 95.00% examples, 133578 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:08:04,490] EPOCHs No. 1 - PROGRESS: at 96.00% examples, 133322 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:10:29,084] EPOCHs No. 1 - PROGRESS: at 97.00% examples, 133054 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:12:52,511] EPOCHs No. 1 - PROGRESS: at 98.00% examples, 132760 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:15:12,249] EPOCHs No. 1 - PROGRESS: at 99.00% examples, 132437 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:17:21,568] finished iterating over Wikipedia corpus of 4730463 documents with 1993339819 positions (total 21639335 articles, 2074445613 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-15 05:17:22,159] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-15 05:17:22,199] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-15 05:17:22,200] worker thread finished; awaiting finish of 37 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-15 05:17:22,201] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-15 05:17:22,201] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-15 05:17:22,202] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-15 05:17:22,202] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-15 05:17:22,204] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-15 05:17:22,204] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-15 05:17:22,205] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-15 05:17:22,206] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-15 05:17:22,206] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-15 05:17:22,207] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-15 05:17:22,207] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-15 05:17:22,208] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-15 05:17:22,209] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-15 05:17:22,209] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-15 05:17:22,210] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-15 05:17:22,210] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-15 05:17:22,211] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-15 05:17:22,211] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-15 05:17:22,212] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-15 05:17:22,212] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-15 05:17:22,213] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-15 05:17:22,213] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-15 05:17:22,213] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-15 05:17:22,214] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-15 05:17:22,214] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-15 05:17:22,215] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-15 05:17:22,215] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-15 05:17:22,216] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-15 05:17:22,216] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-15 05:17:22,217] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-15 05:17:22,217] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-15 05:17:22,218] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-15 05:17:22,218] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-15 05:17:22,219] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-15 05:17:22,219] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-15 05:17:22,345] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-15 05:17:23,065] EPOCHs No. 1 - PROGRESS: at 100.00% examples, 132153 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-15 05:17:23,066] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-15 05:17:23,067] EPOCH - 1 : training on 1993339819 raw words (1956134106 effective words) took 14802.0s, 132153 effective words/s\n",
      "[2022-09-15 05:25:45,504] EPOCHs No. 2 - PROGRESS: at 1.00% examples, 157583 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:29:26,140] EPOCHs No. 2 - PROGRESS: at 2.00% examples, 158640 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:34:52,529] EPOCHs No. 2 - PROGRESS: at 3.00% examples, 159814 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:39:22,209] EPOCHs No. 2 - PROGRESS: at 4.00% examples, 160869 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:43:20,729] EPOCHs No. 2 - PROGRESS: at 5.00% examples, 161536 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:47:07,923] EPOCHs No. 2 - PROGRESS: at 6.00% examples, 161338 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:50:29,547] EPOCHs No. 2 - PROGRESS: at 7.00% examples, 161486 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:53:41,538] EPOCHs No. 2 - PROGRESS: at 8.00% examples, 161485 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 05:56:44,478] EPOCHs No. 2 - PROGRESS: at 9.00% examples, 161572 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-15 05:59:40,496] EPOCHs No. 2 - PROGRESS: at 10.00% examples, 161493 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-15 06:02:37,333] EPOCHs No. 2 - PROGRESS: at 11.00% examples, 161189 words/s, in_qsize 0, out_qsize 0\n"
     ]
    }
   ],
   "source": [
    "logging.info('Training model %s', 'wordnet')\n",
    "model = word2vec.Word2Vec(sentences, window=10, sg=1, hs=0, negative=5, size=300, workers=40, iter=5)\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:14.273Z"
    }
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_filtered_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc1_epoch5_300_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2R_mc1_epoch5_300_filtered.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2w2v_mc1_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_con1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2S_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2B_mc100_epoch5_300_sub3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2TB_mc100_epoch5_300_LR.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2POS_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2DEP_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LRM3_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LOC_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_loc.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_pos.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_ent_w10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_dep2_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_ent_static_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_unsem.txt'\n",
    "#emb_file = '/home/manni/embs/en_wiki_spx_mc100_epoch5_300_em.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns_w1.txt'\n",
    "emb_file = '/home/manni/embs/en_wiki_wnet_epoch5_300_w10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:10:11.484505Z",
     "start_time": "2022-09-19T02:52:06.068419Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wnet = '/home/manni/embs/en_wiki_wnet_epoch5_300_w10_new.txt'\n",
    "model = KeyedVectors.load_word2vec_format(wnet, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:10:11.490763Z",
     "start_time": "2022-09-19T03:10:11.487163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31350/1054457911.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vocab = model.wv.vocab\n"
     ]
    }
   ],
   "source": [
    "vocab = model.wv.vocab #dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:10:11.525361Z",
     "start_time": "2022-09-19T03:10:11.492273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2702310"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:13:45.613538Z",
     "start_time": "2022-09-19T03:13:45.608791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2702310"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.pop('[', None)\n",
    "vocab.pop(']', None)\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:16:14.880610Z",
     "start_time": "2022-09-19T03:16:13.181586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2614118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_vocab = list(vocab.keys())\n",
    "vocab = set()\n",
    "for word in _vocab:\n",
    "    if '§' in word:\n",
    "        continue\n",
    "    vocab.add(word)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:17:10.007146Z",
     "start_time": "2022-09-19T03:17:10.004550Z"
    }
   },
   "outputs": [],
   "source": [
    "out_file = '/home/manni/embs/en_wiki_wnet_epoch5_300_w10_explicit.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T03:33:50.039385Z",
     "start_time": "2022-09-19T03:17:42.458396Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-19 04:17:42,460] Save trained word vectors\n",
      "  0%|                                                                                                                                                                     | 0/2614118 [00:00<?, ?it/s]/tmp/ipykernel_31350/2372546197.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2614118/2614118 [16:05<00:00, 2708.09it/s]\n",
      "[2022-09-19 04:33:50,036] Done\n"
     ]
    }
   ],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(out_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab, position=0):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
