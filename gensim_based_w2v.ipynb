{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T05:53:32.480228Z",
     "start_time": "2022-03-23T05:53:30.736021Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "#import wiki_old as w # old wiki\n",
    "import wiki as w \n",
    " \n",
    "#from gensim.models import word2vec # for orignal w2v\n",
    "from localgensim.gensim2.models import word2vec \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models.fasttext import FastText\n",
    "#from gensim.models.word2vec import Word2Vec # not in use\n",
    "#from localgensim.gensim2.models.word2vec import Word2Vec # not in use\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream.xml.bz2'\n",
    "#WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream1.xml-p1p41242.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T05:53:32.483163Z",
     "start_time": "2022-03-23T05:53:30.879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manni/ner-s2s/word_embedding/wiki.py\n",
      "/home/manni/ner-s2s/word_embedding/localgensim/gensim2/models/word2vec.py\n"
     ]
    }
   ],
   "source": [
    "print(w.__file__)\n",
    "print(word2vec.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T05:53:32.484929Z",
     "start_time": "2022-03-23T05:53:31.194Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T05:53:32.486613Z",
     "start_time": "2022-03-23T05:53:32.303Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO)\n",
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:25:05.774307Z",
     "start_time": "2020-10-25T13:25:05.608721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Training model %s', 'word2vec')\n",
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=10, size=300, sample=0, \n",
    "                 workers=1, iter=1, min_count=1)\n",
    "\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:48:45.834994Z",
     "start_time": "2020-10-25T13:47:14.780675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conll corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"conll_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:52:12.732111Z",
     "start_time": "2020-10-25T13:48:45.838043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=1, iter=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_spx2g.txt'\n",
    "emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip if sentence made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:32.868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:07:44,221] Parsing wiki corpus Altered...\n",
      "[2022-09-12 04:07:52,104] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "[2022-09-12 04:08:48,790] adding document #10000 to Dictionary(416518 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:09:37,922] adding document #20000 to Dictionary(590924 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:10:20,896] adding document #30000 to Dictionary(733312 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:10:59,818] adding document #40000 to Dictionary(856740 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:11:31,650] adding document #50000 to Dictionary(934779 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:11:50,884] adding document #60000 to Dictionary(955808 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:12:08,595] adding document #70000 to Dictionary(974931 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:12:25,433] adding document #80000 to Dictionary(990589 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:13:03,151] adding document #90000 to Dictionary(1082983 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:13:41,261] adding document #100000 to Dictionary(1190222 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:14:16,671] adding document #110000 to Dictionary(1279316 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:14:50,616] adding document #120000 to Dictionary(1367558 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:15:22,734] adding document #130000 to Dictionary(1437582 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:15:58,504] adding document #140000 to Dictionary(1523736 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:16:32,106] adding document #150000 to Dictionary(1608566 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:17:04,855] adding document #160000 to Dictionary(1682913 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:17:35,505] adding document #170000 to Dictionary(1749667 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:18:04,795] adding document #180000 to Dictionary(1807439 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:18:31,900] adding document #190000 to Dictionary(1864528 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:19:00,949] adding document #200000 to Dictionary(1924675 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:19:31,016] adding document #210000 to Dictionary(1977267 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:20:00,886] discarding 31125 tokens: [('deom', 1), ('goldtoken', 1), ('gwangsanghui', 1), ('jangki', 1), ('meonggun', 1), ('playsites', 1), ('toeng', 1), ('광상희', 1), ('졸', 1), ('차', 1)]...\n",
      "[2022-09-12 04:20:00,888] keeping 2000000 tokens which were in no less than 0 and no more than 220000 (=100.0%) documents\n",
      "[2022-09-12 04:20:05,332] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:20:05,364] adding document #220000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:20:35,692] discarding 54096 tokens: [('protectaid', 1), ('spongeworthiness', 1), ('blunt#1', 1), ('oligoribonucleotides', 1), ('chemchemi', 1), ('enekwe', 1), ('eskia', 1), ('ezenzeleni', 1), ('gamphahlele', 1), ('kyeyune', 1)]...\n",
      "[2022-09-12 04:20:35,695] keeping 2000000 tokens which were in no less than 0 and no more than 230000 (=100.0%) documents\n",
      "[2022-09-12 04:20:40,091] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:20:40,152] adding document #230000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:21:11,098] discarding 48306 tokens: [('définition', 1), ('vuaridel', 1), ('brinckmanniherz', 1), ('বগ', 1), ('feminizes', 1), ('gērāʾ', 1), ('məqērāh', 1), ('paršədōnāh', 1), ('yēṣē', 1), ('ʾēhūḏ', 1)]...\n",
      "[2022-09-12 04:21:11,102] keeping 2000000 tokens which were in no less than 0 and no more than 240000 (=100.0%) documents\n",
      "[2022-09-12 04:21:15,378] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:21:15,436] adding document #240000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:21:46,967] discarding 51237 tokens: [('伊佐奈弥尊', 1), ('伊我理比女命', 1), ('伊我理神社', 1), ('伊雑宮', 1), ('伊雑宮の御神田', 1), ('佐佐津比古命宇加乃御魂御祖命伊加利比売命', 1), ('佐美長御前神', 1), ('佐美長御前神社', 1), ('佐美長神社', 1), ('佐見都比女命', 1)]...\n",
      "[2022-09-12 04:21:46,970] keeping 2000000 tokens which were in no less than 0 and no more than 250000 (=100.0%) documents\n",
      "[2022-09-12 04:21:51,379] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:21:51,440] adding document #250000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:22:20,552] discarding 48199 tokens: [('suduvians', 1), ('suduwite', 1), ('suvalkijos', 1), ('sūd', 1), ('totoraitis', 1), ('ugrier', 1), ('verhaeltnisse', 1), ('wallburganlagen', 1), ('westgebiete', 1), ('wohnsitz', 1)]...\n",
      "[2022-09-12 04:22:20,553] keeping 2000000 tokens which were in no less than 0 and no more than 260000 (=100.0%) documents\n",
      "[2022-09-12 04:22:23,684] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:22:23,736] adding document #260000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:25:06,135] discarding 53065 tokens: [('minv', 1), ('mygenerator', 1), ('myiterable', 1), ('pygen', 1), ('stackful', 1), ('stoutamire', 1), ('string_chars', 1), ('unsupportedoperationexception', 1), ('spicyness', 1), ('sutility', 1)]...\n",
      "[2022-09-12 04:25:06,136] keeping 2000000 tokens which were in no less than 0 and no more than 310000 (=100.0%) documents\n",
      "[2022-09-12 04:25:09,096] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:25:09,140] adding document #310000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:25:37,294] discarding 48280 tokens: [('druchs', 1), ('kopelkin', 1), ('lazick', 1), ('taliostra', 1), ('trainsarefun', 1), ('ascetisme', 1), ('austward', 1), ('austérité', 1), ('cochoy', 1), ('fogelmann', 1)]...\n",
      "[2022-09-12 04:25:37,296] keeping 2000000 tokens which were in no less than 0 and no more than 320000 (=100.0%) documents\n",
      "[2022-09-12 04:25:41,935] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:25:41,999] adding document #320000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:26:08,451] discarding 43827 tokens: [('zlatuse', 1), ('ampelon', 1), ('estelynn', 1), ('ciger', 1), ('hygrograph', 1), ('gyalsem', 1), ('kathok', 1), ('kumutha', 1), ('lhazen', 1), ('namden', 1)]...\n",
      "[2022-09-12 04:26:08,452] keeping 2000000 tokens which were in no less than 0 and no more than 330000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:26:11,431] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:26:11,475] adding document #330000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:26:39,368] discarding 41986 tokens: [('melvega', 1), ('mpkiis', 1), ('ptvclassic', 1), ('tttgold', 1), ('uzmagic', 1), ('wrinklys', 1), ('claireaux', 1), ('esnol', 1), ('nsarkozy', 1), ('vendasi', 1)]...\n",
      "[2022-09-12 04:26:39,372] keeping 2000000 tokens which were in no less than 0 and no more than 340000 (=100.0%) documents\n",
      "[2022-09-12 04:26:43,804] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:26:43,869] adding document #340000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:27:10,469] discarding 40748 tokens: [('orocoto', 1), ('ororicó', 1), ('otaima', 1), ('oyaricule', 1), ('padamo', 1), ('paima', 1), ('palanc', 1), ('panemá', 1), ('pantagora', 1), ('panáre', 1)]...\n",
      "[2022-09-12 04:27:10,471] keeping 2000000 tokens which were in no less than 0 and no more than 350000 (=100.0%) documents\n",
      "[2022-09-12 04:27:14,752] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:27:14,817] adding document #350000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:27:41,081] discarding 39778 tokens: [('vanderzwagg', 1), ('allees', 1), ('ayroule', 1), ('bruilhols', 1), ('cadirac', 1), ('cardié', 1), ('chapeliers', 1), ('cpam', 1), ('durroux', 1), ('fondere', 1)]...\n",
      "[2022-09-12 04:27:41,083] keeping 2000000 tokens which were in no less than 0 and no more than 360000 (=100.0%) documents\n",
      "[2022-09-12 04:27:45,230] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:27:45,294] adding document #360000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:28:12,595] discarding 46536 tokens: [('vinesh', 1), ('anthelius', 1), ('anticatholicism', 1), ('katoliker', 1), ('katolska', 1), ('liunga', 1), ('pretendent', 1), ('romersk', 1), ('stinissen', 1), ('zackarias', 1)]...\n",
      "[2022-09-12 04:28:12,598] keeping 2000000 tokens which were in no less than 0 and no more than 370000 (=100.0%) documents\n",
      "[2022-09-12 04:28:16,955] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:28:17,022] adding document #370000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:28:45,777] discarding 40605 tokens: [('parchum', 1), ('psebai', 1), ('agrolet', 1), ('caiber', 1), ('doccolo', 1), ('helmey', 1), ('mcreery', 1), ('midand', 1), ('naysa', 1), ('peparo', 1)]...\n",
      "[2022-09-12 04:28:45,779] keeping 2000000 tokens which were in no less than 0 and no more than 380000 (=100.0%) documents\n",
      "[2022-09-12 04:28:50,482] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:28:50,547] adding document #380000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:29:16,095] discarding 43170 tokens: [('sadala', 1), ('speices', 1), ('budeşti', 1), ('kiean', 1), ('manthem', 1), ('thickburger', 1), ('whopperito', 1), ('madlibs', 1), ('dikmik', 1), ('discriminatingly', 1)]...\n",
      "[2022-09-12 04:29:16,096] keeping 2000000 tokens which were in no less than 0 and no more than 390000 (=100.0%) documents\n",
      "[2022-09-12 04:29:19,112] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:29:19,159] adding document #390000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:29:45,393] discarding 46597 tokens: [('kidlings', 1), ('ovisher', 1), ('nombor', 1), ('voladas', 1), ('клавиатура', 1), ('любава', 1), ('машинка', 1), ('cyrpus', 1), ('perhydrous', 1), ('ediția', 1)]...\n",
      "[2022-09-12 04:29:45,395] keeping 2000000 tokens which were in no less than 0 and no more than 400000 (=100.0%) documents\n",
      "[2022-09-12 04:29:50,114] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:29:50,183] adding document #400000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:30:16,277] discarding 47318 tokens: [('sarcich', 1), ('descuidado', 1), ('timebandit', 1), ('utungun', 1), ('walliss', 1), ('anuther', 1), ('bridlecull', 1), ('conjo', 1), ('dicksey', 1), ('expeditioneers', 1)]...\n",
      "[2022-09-12 04:30:16,279] keeping 2000000 tokens which were in no less than 0 and no more than 410000 (=100.0%) documents\n",
      "[2022-09-12 04:30:20,616] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:30:20,683] adding document #410000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:30:46,604] discarding 42260 tokens: [('gpanz', 1), ('dkbonds', 1), ('chuundar', 1), ('freyyr', 1), ('jolee', 1), ('komad', 1), ('lestin', 1), ('tokare', 1), ('uthar', 1), ('vulkar', 1)]...\n",
      "[2022-09-12 04:30:46,606] keeping 2000000 tokens which were in no less than 0 and no more than 420000 (=100.0%) documents\n",
      "[2022-09-12 04:30:49,670] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:30:49,717] adding document #420000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:31:16,349] discarding 46515 tokens: [('pgdis', 1), ('poliploidies', 1), ('preimplantational', 1), ('rehybridize', 1), ('throphectoderm', 1), ('champul', 1), ('lochash', 1), ('techignite', 1), ('dernbrant', 1), ('doftar', 1)]...\n",
      "[2022-09-12 04:31:16,350] keeping 2000000 tokens which were in no less than 0 and no more than 430000 (=100.0%) documents\n",
      "[2022-09-12 04:31:19,356] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:31:19,403] adding document #430000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:31:45,651] discarding 44557 tokens: [('artimis', 1), ('farradyne', 1), ('travinfo', 1), ('misenhimer', 1), ('seitsinger', 1), ('pugas', 1), ('antirecession', 1), ('dardhunter', 1), ('becheurs', 1), ('diego_velázquez_', 1)]...\n",
      "[2022-09-12 04:31:45,653] keeping 2000000 tokens which were in no less than 0 and no more than 440000 (=100.0%) documents\n",
      "[2022-09-12 04:31:49,983] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:31:50,051] adding document #440000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:32:16,232] discarding 47384 tokens: [('alolga', 1), ('widescope', 1), ('aśōka', 1), ('kusīnagara', 1), ('pāțaliputra', 1), ('śataka', 1), ('mcfeggan', 1), ('mellmoth', 1), ('pecachard', 1), ('protobiography', 1)]...\n",
      "[2022-09-12 04:32:16,234] keeping 2000000 tokens which were in no less than 0 and no more than 450000 (=100.0%) documents\n",
      "[2022-09-12 04:32:20,380] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:32:20,450] adding document #450000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:32:44,904] discarding 37921 tokens: [('stereoregularly', 1), ('tetraphenylallyl', 1), ('vielgliedrige', 1), ('finnlay', 1), ('bartebly', 1), ('chrematismos', 1), ('neuropsychoanalyst', 1), ('seelenlebens', 1), ('carbureto', 1), ('cálcio', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:32:44,906] keeping 2000000 tokens which were in no less than 0 and no more than 460000 (=100.0%) documents\n",
      "[2022-09-12 04:32:49,304] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:32:49,373] adding document #460000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:33:14,455] discarding 46012 tokens: [('arekat', 1), ('erikat', 1), ('neameh', 1), ('ʻrēqāt', 1), ('ʻurayqāt', 1), ('ṣāʼib', 1), ('spikily', 1), ('mammalology', 1), ('ornithognomon', 1), ('bakamatsu', 1)]...\n",
      "[2022-09-12 04:33:14,457] keeping 2000000 tokens which were in no less than 0 and no more than 470000 (=100.0%) documents\n",
      "[2022-09-12 04:33:18,822] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:33:18,893] adding document #470000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:33:44,289] discarding 40336 tokens: [('corosimo', 1), ('efráin', 1), ('guaiqueris', 1), ('guaranoco', 1), ('guasi', 1), ('guasina', 1), ('janejo', 1), ('kotoch', 1), ('lizeta', 1), ('macareo', 1)]...\n",
      "[2022-09-12 04:33:44,290] keeping 2000000 tokens which were in no less than 0 and no more than 480000 (=100.0%) documents\n",
      "[2022-09-12 04:33:48,546] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:33:48,615] adding document #480000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:34:14,362] discarding 44239 tokens: [('mandyphos', 1), ('nickon', 1), ('nonacyclo', 1), ('octacirculene', 1), ('oxoolean', 1), ('penguinone', 1), ('pentacyclo', 1), ('pentafluoroethyl', 1), ('performic', 1), ('periplanarity', 1)]...\n",
      "[2022-09-12 04:34:14,364] keeping 2000000 tokens which were in no less than 0 and no more than 490000 (=100.0%) documents\n",
      "[2022-09-12 04:34:17,619] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:34:17,669] adding document #490000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:34:45,391] discarding 42342 tokens: [('qiladars', 1), ('rughunathrao', 1), ('salabai', 1), ('shindeshahi', 1), ('sinhagadh', 1), ('trimabkji', 1), ('shanmker', 1), ('pešikan', 1), ('yugoslaviascg', 1), ('covinni', 1)]...\n",
      "[2022-09-12 04:34:45,394] keeping 2000000 tokens which were in no less than 0 and no more than 500000 (=100.0%) documents\n",
      "[2022-09-12 04:34:49,839] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:34:49,917] adding document #500000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:35:15,495] discarding 43459 tokens: [('caurangipa', 1), ('celukapa', 1), ('chittamatrin', 1), ('darikapa', 1), ('dengipa', 1), ('dhahulipa', 1), ('dharmapa', 1), ('dhilipa', 1), ('dhobipa', 1), ('dhokaripa', 1)]...\n",
      "[2022-09-12 04:35:15,497] keeping 2000000 tokens which were in no less than 0 and no more than 510000 (=100.0%) documents\n",
      "[2022-09-12 04:35:18,664] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:35:18,714] adding document #510000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:35:43,227] discarding 41783 tokens: [('piedades', 1), ('platanar', 1), ('tonadora', 1), ('tonjibe', 1), ('urruca', 1), ('basilica_de_los_angeles', 1), ('pacayas', 1), ('represacachí', 1), ('tapanti', 1), ('eulimidae', 1)]...\n",
      "[2022-09-12 04:35:43,229] keeping 2000000 tokens which were in no less than 0 and no more than 520000 (=100.0%) documents\n",
      "[2022-09-12 04:35:46,299] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:35:46,349] adding document #520000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:36:11,256] discarding 44748 tokens: [('satipatṭhāna', 1), ('satipațțhāna', 1), ('saṁyuttas', 1), ('upaṭṭhāna', 1), ('vipassanāvāda', 1), ('ābhidharma', 1), ('śāriputr', 1), ('apportionable', 1), ('abdoon', 1), ('dhulum', 1)]...\n",
      "[2022-09-12 04:36:11,258] keeping 2000000 tokens which were in no less than 0 and no more than 530000 (=100.0%) documents\n",
      "[2022-09-12 04:36:15,690] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:36:15,761] adding document #530000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:36:41,186] discarding 39952 tokens: [('stuckyi', 1), ('takaw', 1), ('thalioides', 1), ('trachystachys', 1), ('wakaensis', 1), ('waltersiae', 1), ('yuccifolia', 1), ('arcuately', 1), ('tormentiol', 1), ('conchiferous', 1)]...\n",
      "[2022-09-12 04:36:41,188] keeping 2000000 tokens which were in no less than 0 and no more than 540000 (=100.0%) documents\n",
      "[2022-09-12 04:36:45,626] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:36:45,697] adding document #540000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:37:09,838] discarding 42156 tokens: [('benlolo', 1), ('deinternetman', 1), ('droa', 1), ('droe', 1), ('klemann', 1), ('namejuice', 1), ('salmaid', 1), ('salmaster', 1), ('salmoor', 1), ('meanspirited', 1)]...\n",
      "[2022-09-12 04:37:09,840] keeping 2000000 tokens which were in no less than 0 and no more than 550000 (=100.0%) documents\n",
      "[2022-09-12 04:37:14,216] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:37:14,287] adding document #550000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:37:38,569] discarding 38276 tokens: [('กษานอกระบบและการศ', 1), ('การศ', 1), ('ธยาศ', 1), ('ยอำเภอแม', 1), ('ลาว', 1), ('bascii', 1), ('binlah', 1), ('borwornsak', 1), ('chanatip', 1), ('chatborirak', 1)]...\n",
      "[2022-09-12 04:37:38,570] keeping 2000000 tokens which were in no less than 0 and no more than 560000 (=100.0%) documents\n",
      "[2022-09-12 04:37:41,700] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:37:41,748] adding document #560000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:38:06,056] discarding 36474 tokens: [('porfirianos', 1), ('prominentes', 1), ('secreatria', 1), ('asjs', 1), ('histel', 1), ('malmso', 1), ('demisemihemidemisemiquavers', 1), ('semidemisemiquaver', 1), ('semihemidemisemiquavers', 1), ('boald', 1)]...\n",
      "[2022-09-12 04:38:06,058] keeping 2000000 tokens which were in no less than 0 and no more than 570000 (=100.0%) documents\n",
      "[2022-09-12 04:38:10,516] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:38:10,588] adding document #570000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:38:35,715] discarding 43598 tokens: [('raisins_and_health_', 1), ('scantian', 1), ('stragany', 1), ('alogically', 1), ('extrasystemic', 1), ('radarlauncher', 1), ('пc', 1), ('пму', 1), ('риф', 1), ('pharyngoglottal', 1)]...\n",
      "[2022-09-12 04:38:35,717] keeping 2000000 tokens which were in no less than 0 and no more than 580000 (=100.0%) documents\n",
      "[2022-09-12 04:38:40,183] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:38:40,257] adding document #580000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:39:04,522] discarding 41323 tokens: [('qinguangwang', 1), ('songdiwang', 1), ('taishanwang', 1), ('wuguanwang', 1), ('zhuanlunwang', 1), ('五官王', 1), ('十殿阎罗王', 1), ('卞城王', 1), ('宋帝王', 1), ('平等王', 1)]...\n",
      "[2022-09-12 04:39:04,524] keeping 2000000 tokens which were in no less than 0 and no more than 590000 (=100.0%) documents\n",
      "[2022-09-12 04:39:09,283] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:39:09,355] adding document #590000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:39:34,027] discarding 39922 tokens: [('noaltris', 1), ('nvingă', 1), ('nţelepţeşce', 1), ('obowiązkom', 1), ('obsije', 1), ('obvari', 1), ('ochotností', 1), ('ochroń', 1), ('oferind', 1), ('ognora', 1)]...\n",
      "[2022-09-12 04:39:34,029] keeping 2000000 tokens which were in no less than 0 and no more than 600000 (=100.0%) documents\n",
      "[2022-09-12 04:39:38,531] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:39:38,604] adding document #600000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:40:01,824] discarding 38477 tokens: [('fakāna', 1), ('famā', 1), ('fidāki', 1), ('hamalnāki', 1), ('hudāti', 1), ('hunalika', 1), ('ibā', 1), ('ifrīqīa', 1), ('jabīniki', 1), ('janān', 1)]...\n",
      "[2022-09-12 04:40:01,826] keeping 2000000 tokens which were in no less than 0 and no more than 610000 (=100.0%) documents\n",
      "[2022-09-12 04:40:06,258] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:40:06,329] adding document #610000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:40:29,984] discarding 40562 tokens: [('tədj', 1), ('uwaṩəxəm', 1), ('uwaṩəxəmrək', 1), ('worəndj', 1), ('wuşıt', 1), ('xweyxəş', 1), ('xwəfaşəxəm', 1), ('xwəfaşəxəmrək', 1), ('yafəş', 1), ('yatlıtənığərə', 1)]...\n",
      "[2022-09-12 04:40:29,986] keeping 2000000 tokens which were in no less than 0 and no more than 620000 (=100.0%) documents\n",
      "[2022-09-12 04:40:34,478] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:40:34,553] adding document #620000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:40:58,952] discarding 39949 tokens: [('cellaenovae', 1), ('unstartling', 1), ('diciccos', 1), ('kubbie', 1), ('photodrug', 1), ('tanorexia', 1), ('conaset', 1), ('ircobi', 1), ('koornstra', 1), ('expansiel', 1)]...\n",
      "[2022-09-12 04:40:58,953] keeping 2000000 tokens which were in no less than 0 and no more than 630000 (=100.0%) documents\n",
      "[2022-09-12 04:41:03,721] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:41:03,795] adding document #630000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:41:27,796] discarding 37684 tokens: [('our_lady_of_ljeviš', 1), ('prvovencani', 1), ('restauracije', 1), ('carderay', 1), ('trippingham', 1), ('crescentview', 1), ('kiboshed', 1), ('yellowquill', 1), ('astintagh', 1), ('daxihaizi', 1)]...\n",
      "[2022-09-12 04:41:27,798] keeping 2000000 tokens which were in no less than 0 and no more than 640000 (=100.0%) documents\n",
      "[2022-09-12 04:41:30,977] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:41:31,028] adding document #640000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:41:53,004] discarding 43686 tokens: [('godmundingaham', 1), ('baseballteam', 1), ('geyen', 1), ('pulheimer', 1), ('sinnersdorf', 1), ('sinthern', 1), ('chantelauze', 1), ('secrettes', 1), ('scffdabm', 1), ('isulu', 1)]...\n",
      "[2022-09-12 04:41:53,005] keeping 2000000 tokens which were in no less than 0 and no more than 650000 (=100.0%) documents\n",
      "[2022-09-12 04:41:56,192] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:41:56,243] adding document #650000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:42:18,909] discarding 37720 tokens: [('flisykowski', 1), ('goręczyno', 1), ('justizmords', 1), ('antidiaplokí', 1), ('marathonis', 1), ('pellatv', 1), ('vriones', 1), ('αντιδιαπλοκή', 1), ('alguimou', 1), ('mesigig', 1)]...\n",
      "[2022-09-12 04:42:18,911] keeping 2000000 tokens which were in no less than 0 and no more than 660000 (=100.0%) documents\n",
      "[2022-09-12 04:42:22,040] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:42:22,092] adding document #660000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:42:44,768] discarding 35880 tokens: [('takalmawa', 1), ('tanziynul', 1), ('taramniya', 1), ('tourismroi', 1), ('usmau', 1), ('wammakko', 1), ('yardole', 1), ('zabarmawa', 1), ('zoramawa', 1), ('praxcelis', 1)]...\n",
      "[2022-09-12 04:42:44,770] keeping 2000000 tokens which were in no less than 0 and no more than 670000 (=100.0%) documents\n",
      "[2022-09-12 04:42:49,236] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:42:49,310] adding document #670000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:43:12,641] discarding 36880 tokens: [('domarin', 1), ('détourbe', 1), ('estrablin', 1), ('eydoche', 1), ('eyzin', 1), ('flachère', 1), ('flachères', 1), ('fontanil', 1), ('frontonas', 1), ('gillonnay', 1)]...\n",
      "[2022-09-12 04:43:12,642] keeping 2000000 tokens which were in no less than 0 and no more than 680000 (=100.0%) documents\n",
      "[2022-09-12 04:43:15,788] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:43:15,839] adding document #680000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:43:37,789] discarding 55098 tokens: [('ardenghesca', 1), ('nobilta', 1), ('fasco', 1), ('turone', 1), ('dėpartement', 1), ('impedit', 1), ('asmis', 1), ('informanda', 1), ('kαvώv', 1), ('pharmacopia', 1)]...\n",
      "[2022-09-12 04:43:37,791] keeping 2000000 tokens which were in no less than 0 and no more than 690000 (=100.0%) documents\n",
      "[2022-09-12 04:43:40,930] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:43:40,983] adding document #690000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:44:04,627] discarding 37307 tokens: [('ostromecki', 1), ('ripstra', 1), ('stuppler', 1), ('iliotropion', 1), ('nieżyn', 1), ('wasylkowski', 1), ('autoflag', 1), ('wigwag#1', 1), ('anwaltsprüfung', 1), ('pravosudni', 1)]...\n",
      "[2022-09-12 04:44:04,628] keeping 2000000 tokens which were in no less than 0 and no more than 700000 (=100.0%) documents\n",
      "[2022-09-12 04:44:07,795] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:44:07,847] adding document #700000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:44:30,857] discarding 36886 tokens: [('woomun', 1), ('panicogenic', 1), ('peraqueductal', 1), ('preprocholecystokinin', 1), ('aadaludan', 1), ('anandaraman', 1), ('aragimpave', 1), ('avarohaṇa', 1), ('bhajamaanasa', 1), ('chesinadella', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:44:30,859] keeping 2000000 tokens which were in no less than 0 and no more than 710000 (=100.0%) documents\n",
      "[2022-09-12 04:44:34,038] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:44:34,089] adding document #710000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:45:00,663] discarding 37586 tokens: [('akbn', 1), ('philidas', 1), ('titanine', 1), ('bulgarianmilitary', 1), ('bouchareine', 1), ('d_fortify_source', 1), ('frasunek', 1), ('lamagra', 1), ('tymm', 1), ('vararg', 1)]...\n",
      "[2022-09-12 04:45:00,665] keeping 2000000 tokens which were in no less than 0 and no more than 720000 (=100.0%) documents\n",
      "[2022-09-12 04:45:05,168] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:45:05,244] adding document #720000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:45:29,303] discarding 38574 tokens: [('molisanne', 1), ('ghiardelli', 1), ('daleforeman', 1), ('ellencraswell', 1), ('garylocke', 1), ('jayinslee', 1), ('jimwaldo', 1), ('nonabrazier', 1), ('normmaleng', 1), ('normrice', 1)]...\n",
      "[2022-09-12 04:45:29,305] keeping 2000000 tokens which were in no less than 0 and no more than 730000 (=100.0%) documents\n",
      "[2022-09-12 04:45:32,777] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:45:32,830] adding document #730000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:45:55,266] discarding 35448 tokens: [('cidlowski', 1), ('lifethreatening', 1), ('akkersdijk', 1), ('rejall', 1), ('spinogenesis', 1), ('hunfalu', 1), ('elcis', 1), ('elpd', 1), ('fiskizze', 1), ('iδn', 1)]...\n",
      "[2022-09-12 04:45:55,268] keeping 2000000 tokens which were in no less than 0 and no more than 740000 (=100.0%) documents\n",
      "[2022-09-12 04:45:58,435] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:45:58,487] adding document #740000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:46:21,449] discarding 39574 tokens: [('allostimulation', 1), ('crossreacting', 1), ('photoimmune', 1), ('proliferatives', 1), ('retransplant', 1), ('bywidth', 1), ('harmunt', 1), ('alanarkintiffsept', 1), ('fedwa', 1), ('sgorbati', 1)]...\n",
      "[2022-09-12 04:46:21,451] keeping 2000000 tokens which were in no less than 0 and no more than 750000 (=100.0%) documents\n",
      "[2022-09-12 04:46:24,595] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:46:24,647] adding document #750000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:46:47,141] discarding 35273 tokens: [('slackfest', 1), ('tourbooking', 1), ('toxkäpp', 1), ('anandarajasingh', 1), ('chandrarajah', 1), ('chandrarajan', 1), ('christoffelsz', 1), ('hirvanamuthu', 1), ('kulendran', 1), ('liyanagama', 1)]...\n",
      "[2022-09-12 04:46:47,143] keeping 2000000 tokens which were in no less than 0 and no more than 760000 (=100.0%) documents\n",
      "[2022-09-12 04:46:51,669] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:46:51,745] adding document #760000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:47:14,001] discarding 36374 tokens: [('wihyu', 1), ('yakaitɨn', 1), ('yoyoti', 1), ('yɨtsɨ', 1), ('ɨkkoi', 1), ('ɨtɨinna', 1), ('ˈnazattamaxandɨ', 1), ('hxdf', 1), ('stiavelli', 1), ('pacmidtraron', 1)]...\n",
      "[2022-09-12 04:47:14,003] keeping 2000000 tokens which were in no less than 0 and no more than 770000 (=100.0%) documents\n",
      "[2022-09-12 04:47:18,497] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:47:18,573] adding document #770000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:47:40,718] discarding 40613 tokens: [('allloc', 1), ('asatkan', 1), ('bejehorse', 1), ('digun', 1), ('ditk', 1), ('ditkreindeer', 1), ('dukuwuntin', 1), ('dukwho', 1), ('dulisnow', 1), ('evengus', 1)]...\n",
      "[2022-09-12 04:47:40,719] keeping 2000000 tokens which were in no less than 0 and no more than 780000 (=100.0%) documents\n",
      "[2022-09-12 04:47:43,923] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:47:43,976] adding document #780000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:48:07,767] discarding 38031 tokens: [('losai', 1), ('lutmatavi', 1), ('mabaraboraboa', 1), ('mabneian', 1), ('madaua', 1), ('madiboiboi', 1), ('magehau', 1), ('maggiau', 1), ('managun', 1), ('manihuna', 1)]...\n",
      "[2022-09-12 04:48:07,769] keeping 2000000 tokens which were in no less than 0 and no more than 790000 (=100.0%) documents\n",
      "[2022-09-12 04:48:10,957] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:48:11,010] adding document #790000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:48:33,747] discarding 35731 tokens: [('carthright', 1), ('chocofuss', 1), ('trelak', 1), ('chythlook', 1), ('sifsof', 1), ('aden_protectorate', 1), ('bipperty', 1), ('bopperty', 1), ('guytones', 1), ('issachor', 1)]...\n",
      "[2022-09-12 04:48:33,749] keeping 2000000 tokens which were in no less than 0 and no more than 800000 (=100.0%) documents\n",
      "[2022-09-12 04:48:36,988] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:48:37,042] adding document #800000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:48:59,406] discarding 39881 tokens: [('chiliagram', 1), ('quintisected', 1), ('ardgeeha', 1), ('boherduff', 1), ('nayd', 1), ('redmondstown', 1), ('sandybanks', 1), ('sepam', 1), ('tippfm', 1), ('myriagons', 1)]...\n",
      "[2022-09-12 04:48:59,407] keeping 2000000 tokens which were in no less than 0 and no more than 810000 (=100.0%) documents\n",
      "[2022-09-12 04:49:04,338] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:49:04,418] adding document #810000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:49:26,545] discarding 36864 tokens: [('doldrin', 1), ('duilin', 1), ('galdon', 1), ('goenglin', 1), ('goenlin', 1), ('gondobar', 1), ('gondothlimbar', 1), ('gothodrum', 1), ('gwarestrin', 1), ('gwareth', 1)]...\n",
      "[2022-09-12 04:49:26,547] keeping 2000000 tokens which were in no less than 0 and no more than 820000 (=100.0%) documents\n",
      "[2022-09-12 04:49:31,098] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:49:31,176] adding document #820000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:49:53,827] discarding 37995 tokens: [('tolkienology', 1), ('verkkoyhteisö', 1), ('арды', 1), ('кольценосец', 1), ('aginskiy', 1), ('alanija', 1), ('altajskij', 1), ('amurskaja', 1), ('astrahanskaja', 1), ('avtonomnaja', 1)]...\n",
      "[2022-09-12 04:49:53,829] keeping 2000000 tokens which were in no less than 0 and no more than 830000 (=100.0%) documents\n",
      "[2022-09-12 04:49:58,203] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:49:58,281] adding document #830000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:50:20,511] discarding 38659 tokens: [('markiesje', 1), ('spanieljournal', 1), ('wachtelhund', 1), ('охотничий', 1), ('спаниель', 1), ('hindels', 1), ('kepmen', 1), ('schachstrategie', 1), ('yasugoro', 1), ('swonk', 1)]...\n",
      "[2022-09-12 04:50:20,513] keeping 2000000 tokens which were in no less than 0 and no more than 840000 (=100.0%) documents\n",
      "[2022-09-12 04:50:23,729] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:50:23,783] adding document #840000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:50:45,251] discarding 36205 tokens: [('mapagpalayang', 1), ('gestev', 1), ('numériq', 1), ('publistar', 1), ('sogides', 1), ('tabloïd', 1), ('alaṅkāraśāstra', 1), ('anushtubha', 1), ('anushtupchandas', 1), ('anuṣṭup', 1)]...\n",
      "[2022-09-12 04:50:45,252] keeping 2000000 tokens which were in no less than 0 and no more than 850000 (=100.0%) documents\n",
      "[2022-09-12 04:50:50,202] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:50:50,285] adding document #850000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:51:12,161] discarding 34039 tokens: [('kōmu', 1), ('little_anchor', 1), ('loewengraben', 1), ('madiasu', 1), ('mahisabu', 1), ('maruchisu', 1), ('mesugurohyōmonchō', 1), ('namiba', 1), ('odō', 1), ('othersalso', 1)]...\n",
      "[2022-09-12 04:51:12,163] keeping 2000000 tokens which were in no less than 0 and no more than 860000 (=100.0%) documents\n",
      "[2022-09-12 04:51:15,413] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:51:15,466] adding document #860000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:51:38,481] discarding 40732 tokens: [('배방', 1), ('봉명', 1), ('삽교', 1), ('서천', 1), ('신례원', 1), ('신창', 1), ('아산', 1), ('예산', 1), ('오산리', 1), ('온양온천', 1)]...\n",
      "[2022-09-12 04:51:38,483] keeping 2000000 tokens which were in no less than 0 and no more than 870000 (=100.0%) documents\n",
      "[2022-09-12 04:51:41,697] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:51:41,752] adding document #870000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:52:03,704] discarding 34000 tokens: [('oiics', 1), ('osics', 1), ('melilitites', 1), ('gbwells', 1), ('metasign', 1), ('appogansett', 1), ('qinhuang', 1), ('affecteds', 1), ('pjrtc', 1), ('thinbox', 1)]...\n",
      "[2022-09-12 04:52:03,705] keeping 2000000 tokens which were in no less than 0 and no more than 880000 (=100.0%) documents\n",
      "[2022-09-12 04:52:08,415] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:52:08,493] adding document #880000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:52:31,146] discarding 35089 tokens: [('podlesnoe', 1), ('podollag', 1), ('polyanlag', 1), ('ponyshsky', 1), ('ponyslag', 1), ('poszukiwawczej', 1), ('poszukująca', 1), ('prikaspisky', 1), ('privolzhlag', 1), ('promzhilstroy', 1)]...\n",
      "[2022-09-12 04:52:31,148] keeping 2000000 tokens which were in no less than 0 and no more than 890000 (=100.0%) documents\n",
      "[2022-09-12 04:52:36,040] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:52:36,131] adding document #890000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:52:58,098] discarding 34946 tokens: [('shameth', 1), ('sometyme', 1), ('thaire', 1), ('wastein', 1), ('wylful', 1), ('boombule', 1), ('chillaz', 1), ('ffett', 1), ('flashnizm', 1), ('fäule', 1)]...\n",
      "[2022-09-12 04:52:58,100] keeping 2000000 tokens which were in no less than 0 and no more than 900000 (=100.0%) documents\n",
      "[2022-09-12 04:53:01,351] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:53:01,410] adding document #900000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:53:23,635] discarding 33641 tokens: [('tsga', 1), ('hōʻalu', 1), ('squareneck', 1), ('balerenos', 1), ('bijasas', 1), ('bitongs', 1), ('buluag', 1), ('carrascos', 1), ('cucueco', 1), ('dicasalarin', 1)]...\n",
      "[2022-09-12 04:53:23,637] keeping 2000000 tokens which were in no less than 0 and no more than 910000 (=100.0%) documents\n",
      "[2022-09-12 04:53:26,903] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:53:26,957] adding document #910000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:53:50,520] discarding 33482 tokens: [('mengels', 1), ('sissimocksink', 1), ('trewellyn', 1), ('tulipiferum', 1), ('wahank', 1), ('whissahickon', 1), ('wiessahitkonk', 1), ('wissachgamen', 1), ('wissahiccon', 1), ('wissha', 1)]...\n",
      "[2022-09-12 04:53:50,522] keeping 2000000 tokens which were in no less than 0 and no more than 920000 (=100.0%) documents\n",
      "[2022-09-12 04:53:55,214] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:53:55,292] adding document #920000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:54:17,715] discarding 39771 tokens: [('gkcv', 1), ('carème', 1), ('métromanie', 1), ('wogue', 1), ('finmecannica', 1), ('bonnechaux', 1), ('bridayne', 1), ('frénade', 1), ('gravissimis', 1), ('xayb', 1)]...\n",
      "[2022-09-12 04:54:17,717] keeping 2000000 tokens which were in no less than 0 and no more than 930000 (=100.0%) documents\n",
      "[2022-09-12 04:54:20,979] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:54:21,034] adding document #930000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:54:42,115] discarding 36827 tokens: [('ὀνειροκριτικά', 1), ('intertoys', 1), ('jellej', 1), ('luderix', 1), ('lunderix', 1), ('mscpo', 1), ('namebrand', 1), ('picwic', 1), ('picwictoys', 1), ('speelhoorn', 1)]...\n",
      "[2022-09-12 04:54:42,117] keeping 2000000 tokens which were in no less than 0 and no more than 940000 (=100.0%) documents\n",
      "[2022-09-12 04:54:45,430] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:54:45,484] adding document #940000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:55:08,670] discarding 44398 tokens: [('parsetext', 1), ('prependhello', 1), ('propertymissing', 1), ('rcurry', 1), ('scriptrunner', 1), ('streamingmarkupbuilder', 1), ('stringcolor', 1), ('valuetoadd', 1), ('vcalc', 1), ('xmlslurper', 1)]...\n",
      "[2022-09-12 04:55:08,672] keeping 2000000 tokens which were in no less than 0 and no more than 950000 (=100.0%) documents\n",
      "[2022-09-12 04:55:13,610] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:55:13,698] adding document #950000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:55:35,728] discarding 40171 tokens: [('feelings', 1), ('garibelle', 1), ('hestian', 1), ('ilainus', 1), ('ioloaus', 1), ('klonus', 1), ('korning', 1), ('krykus', 1), ('leanea', 1), ('patronius', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 04:55:35,730] keeping 2000000 tokens which were in no less than 0 and no more than 960000 (=100.0%) documents\n",
      "[2022-09-12 04:55:39,127] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:55:39,182] adding document #960000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:56:02,200] discarding 42745 tokens: [('ˈfːo', 1), ('ˈfːoo', 1), ('χəmsa', 1), ('rudol', 1), ('allicocke', 1), ('phripp', 1), ('tatefromspillers', 1), ('essigsäure', 1), ('excremente', 1), ('fettsäuren', 1)]...\n",
      "[2022-09-12 04:56:02,202] keeping 2000000 tokens which were in no less than 0 and no more than 970000 (=100.0%) documents\n",
      "[2022-09-12 04:56:05,465] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:56:05,520] adding document #970000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:56:27,163] discarding 36324 tokens: [('bweteoaru', 1), ('bwidin', 1), ('daubugingarawa', 1), ('eanuawirieria', 1), ('eatabwerik', 1), ('eatamebure', 1), ('eatebibido', 1), ('eatedeta', 1), ('eatedogi', 1), ('eateduna', 1)]...\n",
      "[2022-09-12 04:56:27,166] keeping 2000000 tokens which were in no less than 0 and no more than 980000 (=100.0%) documents\n",
      "[2022-09-12 04:56:32,161] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:56:32,243] adding document #980000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:56:53,203] discarding 39501 tokens: [('を編入する', 1), ('三和町', 1), ('三坂の各一部が合併', 1), ('亀石', 1), ('光信各村が合併', 1), ('光末', 1), ('及び小野村全域', 1), ('古川両村が合併', 1), ('大正村桑木', 1), ('小畠', 1)]...\n",
      "[2022-09-12 04:56:53,204] keeping 2000000 tokens which were in no less than 0 and no more than 990000 (=100.0%) documents\n",
      "[2022-09-12 04:56:56,528] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:56:56,583] adding document #990000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:57:16,818] discarding 46737 tokens: [('dinghoua', 1), ('elachyptera', 1), ('euonymaceae', 1), ('euonymopsis', 1), ('fraunhofera', 1), ('glyptopetalum', 1), ('goniodiscus', 1), ('hartogiella', 1), ('hartogiopsis', 1), ('hedraianthera', 1)]...\n",
      "[2022-09-12 04:57:16,820] keeping 2000000 tokens which were in no less than 0 and no more than 1000000 (=100.0%) documents\n",
      "[2022-09-12 04:57:21,536] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:57:21,618] adding document #1000000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:57:44,677] discarding 41132 tokens: [('riedelsburg', 1), ('riesenwald', 1), ('rimpach', 1), ('ruchberg', 1), ('unterbers', 1), ('willmé', 1), ('loginsoft', 1), ('okumanchouja', 1), ('yottette', 1), ('alitschur', 1)]...\n",
      "[2022-09-12 04:57:44,679] keeping 2000000 tokens which were in no less than 0 and no more than 1010000 (=100.0%) documents\n",
      "[2022-09-12 04:57:49,375] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:57:49,457] adding document #1010000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:58:11,832] discarding 35822 tokens: [('plesiopenaeus', 1), ('sadayoshia', 1), ('salmincola', 1), ('sergestes', 1), ('teuchopora', 1), ('zalgo', 1), ('gronder', 1), ('ketoalkyl', 1), ('multiheteromacrocycles', 1), ('effzeh', 1)]...\n",
      "[2022-09-12 04:58:11,835] keeping 2000000 tokens which were in no less than 0 and no more than 1020000 (=100.0%) documents\n",
      "[2022-09-12 04:58:16,879] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:58:16,960] adding document #1020000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:58:38,538] discarding 32873 tokens: [('qawarir', 1), ('ʻarish', 1), ('ϫⲟⲣϣⲁ', 1), ('reinertson', 1), ('bysewo', 1), ('smęgorzyno', 1), ('henryviii', 1), ('meiwai', 1), ('bwraf', 1), ('prebreathing', 1)]...\n",
      "[2022-09-12 04:58:38,540] keeping 2000000 tokens which were in no less than 0 and no more than 1030000 (=100.0%) documents\n",
      "[2022-09-12 04:58:43,540] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:58:43,621] adding document #1030000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:59:04,674] discarding 32987 tokens: [('青のレクイエ', 1), ('青のレクイエム', 1), ('静夜曲', 1), ('音色七色', 1), ('風と歌と祈り', 1), ('wirepulling', 1), ('baywmg', 1), ('magnadoodle', 1), ('magnanimals', 1), ('mazeroni', 1)]...\n",
      "[2022-09-12 04:59:04,676] keeping 2000000 tokens which were in no less than 0 and no more than 1040000 (=100.0%) documents\n",
      "[2022-09-12 04:59:08,039] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:59:08,095] adding document #1040000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:59:29,715] discarding 31968 tokens: [('savandapur', 1), ('karakán', 1), ('narangba', 1), ('pashifiko', 1), ('パシフィコ横浜', 1), ('blagoveshchenskaya', 1), ('issakievsky', 1), ('aaharamangudi', 1), ('ayyampettai', 1), ('chakkarappalli', 1)]...\n",
      "[2022-09-12 04:59:29,717] keeping 2000000 tokens which were in no less than 0 and no more than 1050000 (=100.0%) documents\n",
      "[2022-09-12 04:59:34,639] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:59:34,727] adding document #1050000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 04:59:56,634] discarding 34070 tokens: [('tademaru', 1), ('tatsulot', 1), ('tetoa', 1), ('tgurneu', 1), ('tokishige', 1), ('tommyrod', 1), ('tsukijigaoka', 1), ('wistalia', 1), ('yuudaiji', 1), ('ぬぷぬぷっ', 1)]...\n",
      "[2022-09-12 04:59:56,636] keeping 2000000 tokens which were in no less than 0 and no more than 1060000 (=100.0%) documents\n",
      "[2022-09-12 05:00:01,776] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:00:01,865] adding document #1060000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:00:22,232] discarding 32115 tokens: [('rivercruise', 1), ('権小僧都', 1), ('bonislawski', 1), ('terewai', 1), ('ensaystone', 1), ('filori', 1), ('psykohed', 1), ('tvtjapan', 1), ('slægtsbog', 1), ('videoraati', 1)]...\n",
      "[2022-09-12 05:00:22,234] keeping 2000000 tokens which were in no less than 0 and no more than 1070000 (=100.0%) documents\n",
      "[2022-09-12 05:00:25,537] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:00:25,598] adding document #1070000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:00:46,772] discarding 35804 tokens: [('romdoul', 1), ('theab', 1), ('vẹt', 1), ('chanrith', 1), ('pannasastra', 1), ('ponhearith', 1), ('koosis', 1), ('borgards', 1), ('masonischer', 1), ('stiftlers', 1)]...\n",
      "[2022-09-12 05:00:46,773] keeping 2000000 tokens which were in no less than 0 and no more than 1080000 (=100.0%) documents\n",
      "[2022-09-12 05:00:51,598] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:00:51,680] adding document #1080000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:01:13,403] discarding 35011 tokens: [('dunkelblau', 1), ('dunkelbraun', 1), ('dunkelgrün', 1), ('grauviolett', 1), ('hellgrün', 1), ('lichtgrau', 1), ('lichtgrün', 1), ('olivgrün', 1), ('reichsluftfahrt', 1), ('sandbraun', 1)]...\n",
      "[2022-09-12 05:01:13,405] keeping 2000000 tokens which were in no less than 0 and no more than 1090000 (=100.0%) documents\n",
      "[2022-09-12 05:01:18,159] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:01:18,243] adding document #1090000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:01:38,830] discarding 33956 tokens: [('wastezero', 1), ('yĭn', 1), ('ülemlinnapea', 1), ('bottlenum', 1), ('bottlenumber', 1), ('numberbottles', 1), ('aviationmaps', 1), ('avilution', 1), ('droidefb', 1), ('flightpro', 1)]...\n",
      "[2022-09-12 05:01:38,832] keeping 2000000 tokens which were in no less than 0 and no more than 1100000 (=100.0%) documents\n",
      "[2022-09-12 05:01:43,551] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:01:43,633] adding document #1100000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:02:04,992] discarding 33481 tokens: [('martíntorrijos', 1), ('mireyamoscoso', 1), ('national_patriotic_coalition', 1), ('nicolásarditobarlettavallarino', 1), ('omartorrijos', 1), ('pabloarosemena', 1), ('panameñista_party', 1), ('pedroantoniodíaz', 1), ('ramónmaximilianovaldés', 1), ('ricardoadolfodelaguardiaarango', 1)]...\n",
      "[2022-09-12 05:02:04,994] keeping 2000000 tokens which were in no less than 0 and no more than 1110000 (=100.0%) documents\n",
      "[2022-09-12 05:02:08,256] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:02:08,313] adding document #1110000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:02:29,715] discarding 32149 tokens: [('hawkeyeland', 1), ('lyftogt', 1), ('soike', 1), ('yankonais', 1), ('faichuk', 1), ('pattiw', 1), ('ipmssg', 1), ('msconnections', 1), ('msif', 1), ('infofield', 1)]...\n",
      "[2022-09-12 05:02:29,715] keeping 2000000 tokens which were in no less than 0 and no more than 1120000 (=100.0%) documents\n",
      "[2022-09-12 05:02:33,013] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:02:33,069] adding document #1120000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:02:55,708] discarding 34070 tokens: [('中嶋', 1), ('善輝', 1), ('基礎トゥヴァ語文法', 1), ('小辞典', 1), ('尚生', 1), ('政彦', 1), ('最古の可能性のあるトゥバ語語彙について', 1), ('東洋文化研究所紀要', 1), ('烏里蘇台志略にみえる', 1), ('užok', 1)]...\n",
      "[2022-09-12 05:02:55,710] keeping 2000000 tokens which were in no less than 0 and no more than 1130000 (=100.0%) documents\n",
      "[2022-09-12 05:03:00,440] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:03:00,523] adding document #1130000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:03:22,543] discarding 35281 tokens: [('ɦɯ', 1), ('ʔdoŋ²', 1), ('ʔdəʔ', 1), ('ʔɔʔ', 1), ('低头', 1), ('吊', 1), ('地名用', 1), ('巫师', 1), ('悬挂', 1), ('拴', 1)]...\n",
      "[2022-09-12 05:03:22,545] keeping 2000000 tokens which were in no less than 0 and no more than 1140000 (=100.0%) documents\n",
      "[2022-09-12 05:03:27,228] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:03:27,310] adding document #1140000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:03:48,833] discarding 33333 tokens: [('godee', 1), ('golqah', 1), ('indexicalityworkshop', 1), ('kólí', 1), ('kúlú', 1), ('kúú', 1), ('lingdept', 1), ('locheaux', 1), ('mackenzian', 1), ('nahagot', 1)]...\n",
      "[2022-09-12 05:03:48,834] keeping 2000000 tokens which were in no less than 0 and no more than 1150000 (=100.0%) documents\n",
      "[2022-09-12 05:03:53,559] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:03:53,643] adding document #1150000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:04:14,884] discarding 38112 tokens: [('hekkla', 1), ('jaglowitz', 1), ('ajindivik', 1), ('awessex', 1), ('dakotac', 1), ('fireflyas', 1), ('fireflyfr', 1), ('fireflyt', 1), ('fireflytt', 1), ('gannetas', 1)]...\n",
      "[2022-09-12 05:04:14,886] keeping 2000000 tokens which were in no less than 0 and no more than 1160000 (=100.0%) documents\n",
      "[2022-09-12 05:04:19,552] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:04:19,636] adding document #1160000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:04:40,577] discarding 32304 tokens: [('domelets', 1), ('tupauan', 1), ('aequatorianus', 1), ('austronomus', 1), ('bakarii', 1), ('bemmeleni', 1), ('cabreramops', 1), ('cheiromelini', 1), ('cuvierimops', 1), ('femorosaccus', 1)]...\n",
      "[2022-09-12 05:04:40,579] keeping 2000000 tokens which were in no less than 0 and no more than 1170000 (=100.0%) documents\n",
      "[2022-09-12 05:04:45,330] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:04:45,413] adding document #1170000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:05:06,464] discarding 35601 tokens: [('coulumbe', 1), ('josephsbrau', 1), ('ahmansontheatre', 1), ('fomaハイスピード', 1), ('fomaプラスエリア', 1), ('archmaps', 1), ('artespaña', 1), ('kolonihaven', 1), ('atafalati', 1), ('bhagurayana', 1)]...\n",
      "[2022-09-12 05:05:06,467] keeping 2000000 tokens which were in no less than 0 and no more than 1180000 (=100.0%) documents\n",
      "[2022-09-12 05:05:11,267] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:05:11,352] adding document #1180000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:05:32,613] discarding 40669 tokens: [('landcroft', 1), ('marlepit', 1), ('bursia', 1), ('chitranetra', 1), ('melanosternus', 1), ('peetanetra', 1), ('peetapaad', 1), ('saarika', 1), ('ακριδος', 1), ('θηρας', 1)]...\n",
      "[2022-09-12 05:05:32,615] keeping 2000000 tokens which were in no less than 0 and no more than 1190000 (=100.0%) documents\n",
      "[2022-09-12 05:05:37,298] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:05:37,382] adding document #1190000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:05:59,313] discarding 31984 tokens: [('free_content', 1), ('themountainmail', 1), ('bokobá', 1), ('cacalchén', 1), ('cantamayec', 1), ('chacsinkín', 1), ('chankom', 1), ('chikindzonot', 1), ('cuncunul', 1), ('dzidzantún', 1)]...\n",
      "[2022-09-12 05:05:59,314] keeping 2000000 tokens which were in no less than 0 and no more than 1200000 (=100.0%) documents\n",
      "[2022-09-12 05:06:04,035] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:06:04,119] adding document #1200000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:06:25,596] discarding 33673 tokens: [('albocollaris', 1), ('antoninv', 1), ('fecervnt', 1), ('ensodata', 1), ('ensosleep', 1), ('ensōs', 1), ('公投入聯國', 1), ('民進黨不退縮', 1), ('美國反對', 1), ('dobeis', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:06:25,598] keeping 2000000 tokens which were in no less than 0 and no more than 1210000 (=100.0%) documents\n",
      "[2022-09-12 05:06:28,951] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:06:29,008] adding document #1210000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:06:50,287] discarding 34107 tokens: [('patenschaft', 1), ('pipapo', 1), ('ringelband', 1), ('rinnentorturm', 1), ('rodensteinstraße', 1), ('sachwitz', 1), ('schlinck', 1), ('schloßbergschule', 1), ('schulhäuser', 1), ('schües', 1)]...\n",
      "[2022-09-12 05:06:50,289] keeping 2000000 tokens which were in no less than 0 and no more than 1220000 (=100.0%) documents\n",
      "[2022-09-12 05:06:55,032] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:06:55,116] adding document #1220000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:07:17,924] discarding 30208 tokens: [('dindze', 1), ('dingas', 1), ('dingion', 1), ('dingupite', 1), ('dravěnopołabski', 1), ('dumen', 1), ('duridinov', 1), ('duzagaš', 1), ('dzêsiens', 1), ('dzēse', 1)]...\n",
      "[2022-09-12 05:07:17,925] keeping 2000000 tokens which were in no less than 0 and no more than 1230000 (=100.0%) documents\n",
      "[2022-09-12 05:07:22,710] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:07:22,796] adding document #1230000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:07:44,432] discarding 31505 tokens: [('neorich', 1), ('bibidh', 1), ('eureditions', 1), ('krasaesin', 1), ('lesouac', 1), ('phenkun', 1), ('pseudospies', 1), ('sélénite', 1), ('érosphère', 1), ('étamines', 1)]...\n",
      "[2022-09-12 05:07:44,433] keeping 2000000 tokens which were in no less than 0 and no more than 1240000 (=100.0%) documents\n",
      "[2022-09-12 05:07:47,772] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:07:47,830] adding document #1240000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:08:10,726] discarding 33387 tokens: [('北江光唇魚', 1), ('qiǎng', 1), ('上字', 1), ('下字', 1), ('孫炎', 1), ('御製增訂清文鑑', 1), ('德紅反', 1), ('爾雅音義', 1), ('ajŋ', 1), ('cìqīng', 1)]...\n",
      "[2022-09-12 05:08:10,728] keeping 2000000 tokens which were in no less than 0 and no more than 1250000 (=100.0%) documents\n",
      "[2022-09-12 05:08:15,486] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:08:15,570] adding document #1250000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:08:38,144] discarding 34196 tokens: [('ristiisad', 1), ('selkynchek', 1), ('srecno', 1), ('sôkatsu', 1), ('tmen', 1), ('zralé', 1), ('havimo', 1), ('komigaz', 1), ('sindorskoye', 1), ('sysolskoye', 1)]...\n",
      "[2022-09-12 05:08:38,146] keeping 2000000 tokens which were in no less than 0 and no more than 1260000 (=100.0%) documents\n",
      "[2022-09-12 05:08:42,830] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:08:42,915] adding document #1260000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:09:05,546] discarding 37404 tokens: [('videoville', 1), ('vintergatans', 1), ('azriim', 1), ('azurax', 1), ('bazim', 1), ('eleura', 1), ('gorag', 1), ('gormeel', 1), ('mephit', 1), ('shadowgirls', 1)]...\n",
      "[2022-09-12 05:09:05,549] keeping 2000000 tokens which were in no less than 0 and no more than 1270000 (=100.0%) documents\n",
      "[2022-09-12 05:09:10,336] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:09:10,422] adding document #1270000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:09:32,526] discarding 33744 tokens: [('karuon', 1), ('kataseio', 1), ('nötkråka', 1), ('nøddekrige', 1), ('nøttekråke', 1), ('königslegende', 1), ('dorfzaun', 1), ('inhibitants', 1), ('leftecuador', 1), ('montecristis', 1)]...\n",
      "[2022-09-12 05:09:32,528] keeping 2000000 tokens which were in no less than 0 and no more than 1280000 (=100.0%) documents\n",
      "[2022-09-12 05:09:37,632] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:09:37,717] adding document #1280000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:09:59,351] discarding 40119 tokens: [('aghuania', 1), ('alindja', 1), ('andovk', 1), ('azerabaijan', 1), ('bagben', 1), ('boghakar', 1), ('brokored', 1), ('chahandoukht', 1), ('chahaponk', 1), ('chamkor', 1)]...\n",
      "[2022-09-12 05:09:59,353] keeping 2000000 tokens which were in no less than 0 and no more than 1290000 (=100.0%) documents\n",
      "[2022-09-12 05:10:04,061] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:10:04,146] adding document #1290000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:10:24,686] discarding 29691 tokens: [('伏見宮', 1), ('家重', 1), ('邦永親王', 1), ('camnus', 1), ('ccatsu', 1), ('womcam', 1), ('genyū', 1), ('houjuin', 1), ('jokkoin', 1), ('zumnyoin', 1)]...\n",
      "[2022-09-12 05:10:24,688] keeping 2000000 tokens which were in no less than 0 and no more than 1300000 (=100.0%) documents\n",
      "[2022-09-12 05:10:28,353] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:10:28,410] adding document #1300000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:10:48,995] discarding 32997 tokens: [('urushido', 1), ('ibrother', 1), ('imother', 1), ('αλευράς', 1), ('armap', 1), ('devresource', 1), ('symdef', 1), ('fiimeiru', 1), ('under_the_open_sky', 1), ('yakeru', 1)]...\n",
      "[2022-09-12 05:10:48,997] keeping 2000000 tokens which were in no less than 0 and no more than 1310000 (=100.0%) documents\n",
      "[2022-09-12 05:10:52,342] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:10:52,399] adding document #1310000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:11:14,523] discarding 34844 tokens: [('fastavn', 1), ('fastx', 1), ('ieny', 1), ('mfcompress', 1), ('moltype', 1), ('nosine', 1), ('ovax_chick', 1), ('preallocation', 1), ('seqid', 1), ('sequence_', 1)]...\n",
      "[2022-09-12 05:11:14,525] keeping 2000000 tokens which were in no less than 0 and no more than 1320000 (=100.0%) documents\n",
      "[2022-09-12 05:11:19,307] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:11:19,394] adding document #1320000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:11:40,582] discarding 31728 tokens: [('mainschool', 1), ('preparadija', 1), ('preparandium', 1), ('realschool', 1), ('shīyuàn', 1), ('师大', 1), ('師院', 1), ('教育大學', 1), ('manhammer', 1), ('technogeeks', 1)]...\n",
      "[2022-09-12 05:11:40,585] keeping 2000000 tokens which were in no less than 0 and no more than 1330000 (=100.0%) documents\n",
      "[2022-09-12 05:11:45,368] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:11:45,453] adding document #1330000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:12:07,233] discarding 35841 tokens: [('elphnstone', 1), ('dulio', 1), ('gauto', 1), ('ibérique', 1), ('iscovich', 1), ('lebonian', 1), ('lestingi', 1), ('lomianto', 1), ('pontrémoli', 1), ('rolandelli', 1)]...\n",
      "[2022-09-12 05:12:07,235] keeping 2000000 tokens which were in no less than 0 and no more than 1340000 (=100.0%) documents\n",
      "[2022-09-12 05:12:11,918] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:12:12,000] adding document #1340000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:12:34,078] discarding 33082 tokens: [('cortesina', 1), ('amaruka', 1), ('amarukaśataka', 1), ('amarusataka', 1), ('amarusatakam', 1), ('amarushataka', 1), ('amaruśataka', 1), ('devadhar', 1), ('ravichandra', 1), ('अमर', 1)]...\n",
      "[2022-09-12 05:12:34,080] keeping 2000000 tokens which were in no less than 0 and no more than 1350000 (=100.0%) documents\n",
      "[2022-09-12 05:12:38,854] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:12:38,935] adding document #1350000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:13:00,752] discarding 35485 tokens: [('czeydner', 1), ('feketehalmy', 1), ('fioravanzo', 1), ('gorondy', 1), ('jány', 1), ('kisbarnak', 1), ('nemzetvezető', 1), ('reichmarshall', 1), ('szügyi', 1), ('vrkljan', 1)]...\n",
      "[2022-09-12 05:13:00,754] keeping 2000000 tokens which were in no less than 0 and no more than 1360000 (=100.0%) documents\n",
      "[2022-09-12 05:13:04,081] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:13:04,137] adding document #1360000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:13:26,299] discarding 39131 tokens: [('apmamman', 1), ('wisco', 1), ('crossmahon', 1), ('mluver', 1), ('resilire', 1), ('unddr', 1), ('wcdr', 1), ('decimazione', 1), ('kallstroem', 1), ('nuidheacht', 1)]...\n",
      "[2022-09-12 05:13:26,301] keeping 2000000 tokens which were in no less than 0 and no more than 1370000 (=100.0%) documents\n",
      "[2022-09-12 05:13:29,668] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:13:29,725] adding document #1370000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:13:51,132] discarding 33219 tokens: [('cantalonia', 1), ('milldred', 1), ('ourdigitalworld', 1), ('geitsætri', 1), ('leirdalen', 1), ('storgjuvtinden', 1), ('storjuvbrean', 1), ('storjuvet', 1), ('storjuvtinden', 1), ('transcddj', 1)]...\n",
      "[2022-09-12 05:13:51,134] keeping 2000000 tokens which were in no less than 0 and no more than 1380000 (=100.0%) documents\n",
      "[2022-09-12 05:13:55,871] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:13:55,954] adding document #1380000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:14:17,185] discarding 34887 tokens: [('senthamil', 1), ('thillainathan', 1), ('amerex', 1), ('frostline', 1), ('hotohara', 1), ('kesshitai', 1), ('kyaeen', 1), ('rodoplphe', 1), ('skav', 1), ('calgaryarea', 1)]...\n",
      "[2022-09-12 05:14:17,187] keeping 2000000 tokens which were in no less than 0 and no more than 1390000 (=100.0%) documents\n",
      "[2022-09-12 05:14:20,556] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:14:20,613] adding document #1390000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:14:42,548] discarding 33485 tokens: [('downarowicz', 1), ('kiernik', 1), ('smólski', 1), ('bródáin', 1), ('dancity', 1), ('deachunter', 1), ('faggot#2', 1), ('rhasaan', 1), ('slaughterhaus', 1), ('bezzasadne', 1)]...\n",
      "[2022-09-12 05:14:42,549] keeping 2000000 tokens which were in no less than 0 and no more than 1400000 (=100.0%) documents\n",
      "[2022-09-12 05:14:47,290] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:14:47,374] adding document #1400000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:15:09,762] discarding 32595 tokens: [('bruwier', 1), ('egīls', 1), ('kinyor', 1), ('kocuvan', 1), ('kucej', 1), ('tēbelis', 1), ('zadoynov', 1), ('lignière', 1), ('asaeqw', 1), ('axskb', 1)]...\n",
      "[2022-09-12 05:15:09,764] keeping 2000000 tokens which were in no less than 0 and no more than 1410000 (=100.0%) documents\n",
      "[2022-09-12 05:15:13,169] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:15:13,226] adding document #1410000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:15:35,429] discarding 30391 tokens: [('hirshel', 1), ('elmsn', 1), ('blasenschnecken', 1), ('gemisto', 1), ('lithoglyphus', 1), ('physcella', 1), ('phytobentos', 1), ('sidthimunki', 1), ('uktag', 1), ('naturalhistoryroom', 1)]...\n",
      "[2022-09-12 05:15:35,431] keeping 2000000 tokens which were in no less than 0 and no more than 1420000 (=100.0%) documents\n",
      "[2022-09-12 05:15:40,142] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:15:40,223] adding document #1420000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:16:02,663] discarding 32854 tokens: [('hocktideh', 1), ('boutviseth', 1), ('kirisute', 1), ('mackfall', 1), ('usacti', 1), ('usharddigi', 1), ('bandmastership', 1), ('lachryis', 1), ('willowbye', 1), ('jesset', 1)]...\n",
      "[2022-09-12 05:16:02,665] keeping 2000000 tokens which were in no less than 0 and no more than 1430000 (=100.0%) documents\n",
      "[2022-09-12 05:16:07,456] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:16:07,538] adding document #1430000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:16:30,746] discarding 33260 tokens: [('arnautz', 1), ('ataüt', 1), ('corbatàs', 1), ('cucunhan', 1), ('maucòr', 1), ('mètge', 1), ('nuòch', 1), ('pietat', 1), ('secrèt', 1), ('servici', 1)]...\n",
      "[2022-09-12 05:16:30,748] keeping 2000000 tokens which were in no less than 0 and no more than 1440000 (=100.0%) documents\n",
      "[2022-09-12 05:16:35,601] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:16:35,686] adding document #1440000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:16:57,318] discarding 30633 tokens: [('uhtrex', 1), ('gcvt', 1), ('subtilise', 1), ('exog', 1), ('maturases', 1), ('josyln', 1), ('borsodban', 1), ('cserehát', 1), ('dörgicse', 1), ('emlékek', 1)]...\n",
      "[2022-09-12 05:16:57,320] keeping 2000000 tokens which were in no less than 0 and no more than 1450000 (=100.0%) documents\n",
      "[2022-09-12 05:17:00,695] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:17:00,751] adding document #1450000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:17:21,927] discarding 28986 tokens: [('bfnc', 1), ('cebron', 1), ('gourgéennes', 1), ('gourgéens', 1), ('heptadeca', 1), ('mycocentrosporina', 1), ('stafon', 1), ('acrocylindrium', 1), ('caerulens', 1), ('cerulein', 1)]...\n",
      "[2022-09-12 05:17:21,929] keeping 2000000 tokens which were in no less than 0 and no more than 1460000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:17:26,695] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:17:26,777] adding document #1460000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:17:49,960] discarding 35805 tokens: [('braincogs', 1), ('tobbs', 1), ('cietra', 1), ('makkole', 1), ('wehmann', 1), ('mansoorian', 1), ('nonnested', 1), ('ajungilak', 1), ('conzzeta', 1), ('dintikon', 1)]...\n",
      "[2022-09-12 05:17:49,961] keeping 2000000 tokens which were in no less than 0 and no more than 1470000 (=100.0%) documents\n",
      "[2022-09-12 05:17:53,288] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:17:53,345] adding document #1470000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:18:16,770] discarding 29899 tokens: [('etablisements', 1), ('polynesie', 1), ('établisements', 1), ('liberpluscarden', 1), ('macgreagor', 1), ('ophirville', 1), ('bronchorst', 1), ('catalijntje', 1), ('muzikaal', 1), ('agersborg', 1)]...\n",
      "[2022-09-12 05:18:16,772] keeping 2000000 tokens which were in no less than 0 and no more than 1480000 (=100.0%) documents\n",
      "[2022-09-12 05:18:21,899] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:18:21,983] adding document #1480000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:18:42,852] discarding 30259 tokens: [('boysag', 1), ('cheyava', 1), ('nankwoeap', 1), ('tuckup', 1), ('carcone', 1), ('giannoccaro', 1), ('machitski', 1), ('sarafree', 1), ('stepec', 1), ('waaijenberg', 1)]...\n",
      "[2022-09-12 05:18:42,854] keeping 2000000 tokens which were in no less than 0 and no more than 1490000 (=100.0%) documents\n",
      "[2022-09-12 05:18:46,582] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:18:46,664] adding document #1490000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:19:08,552] discarding 34603 tokens: [('hemispericle', 1), ('setoff', 1), ('slipsheets', 1), ('birkenwaldfinnland', 1), ('grädel', 1), ('hertzmann', 1), ('kolaitis', 1), ('libkin', 1), ('hurdla', 1), ('ispirati', 1)]...\n",
      "[2022-09-12 05:19:08,553] keeping 2000000 tokens which were in no less than 0 and no more than 1500000 (=100.0%) documents\n",
      "[2022-09-12 05:19:13,449] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:19:13,536] adding document #1500000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:19:35,766] discarding 31790 tokens: [('abandonedwithin', 1), ('abhidhammic', 1), ('anappanasati', 1), ('assāsa', 1), ('cittapassaddhi', 1), ('energeticness', 1), ('jhanic', 1), ('kayāpassaddhi', 1), ('khemavaggo', 1), ('kāyo', 1)]...\n",
      "[2022-09-12 05:19:35,767] keeping 2000000 tokens which were in no less than 0 and no more than 1510000 (=100.0%) documents\n",
      "[2022-09-12 05:19:39,096] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:19:39,152] adding document #1510000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:19:58,757] discarding 28365 tokens: [('precoloniale', 1), ('educatated', 1), ('cubasil', 1), ('geboundolf', 1), ('gimili', 1), ('halkeginia', 1), ('harkeginia', 1), ('illococo', 1), ('illococoo', 1), ('luctiana', 1)]...\n",
      "[2022-09-12 05:19:58,759] keeping 2000000 tokens which were in no less than 0 and no more than 1520000 (=100.0%) documents\n",
      "[2022-09-12 05:20:03,543] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:20:03,628] adding document #1520000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:20:22,836] discarding 23480 tokens: [('celab', 1), ('laboratiores', 1), ('multicampus', 1), ('ruffilli', 1), ('sitlec', 1), ('sslmit', 1), ('ineternet', 1), ('reflectus', 1), ('stipling', 1), ('ahrr', 1)]...\n",
      "[2022-09-12 05:20:22,838] keeping 2000000 tokens which were in no less than 0 and no more than 1530000 (=100.0%) documents\n",
      "[2022-09-12 05:20:26,187] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:20:26,244] adding document #1530000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:20:47,397] discarding 30779 tokens: [('giordanni', 1), ('returen', 1), ('sabbides', 1), ('wnje', 1), ('gorffenol', 1), ('verulanium', 1), ('bittlesman', 1), ('moviealso', 1), ('roleminiseries', 1), ('snowcoming', 1)]...\n",
      "[2022-09-12 05:20:47,399] keeping 2000000 tokens which were in no less than 0 and no more than 1540000 (=100.0%) documents\n",
      "[2022-09-12 05:20:51,805] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:20:51,890] adding document #1540000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:21:13,707] discarding 35057 tokens: [('radolphus', 1), ('whittleswick', 1), ('wolvernote', 1), ('macelfreshi', 1), ('pacificbio', 1), ('siciliensis', 1), ('compuling', 1), ('mistbelt', 1), ('salsafied', 1), ('texmas', 1)]...\n",
      "[2022-09-12 05:21:13,709] keeping 2000000 tokens which were in no less than 0 and no more than 1550000 (=100.0%) documents\n",
      "[2022-09-12 05:21:18,435] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:21:18,519] adding document #1550000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:21:40,306] discarding 36209 tokens: [('gaeji', 1), ('watbangnomkho', 1), ('bysarna', 1), ('lofqvist', 1), ('vargarna', 1), ('agerwala', 1), ('akyildiz', 1), ('sabnani', 1), ('egilof', 1), ('garbenhof', 1)]...\n",
      "[2022-09-12 05:21:40,309] keeping 2000000 tokens which were in no less than 0 and no more than 1560000 (=100.0%) documents\n",
      "[2022-09-12 05:21:45,118] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:21:45,204] adding document #1560000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:22:07,565] discarding 37084 tokens: [('cártamo', 1), ('kettukaazcha', 1), ('kettukazhcha', 1), ('kuthiras', 1), ('therus', 1), ('thrikkunnappuzha', 1), ('valiyakulangara', 1), ('scelle', 1), ('amerstrand', 1), ('anuawar', 1)]...\n",
      "[2022-09-12 05:22:07,567] keeping 2000000 tokens which were in no less than 0 and no more than 1570000 (=100.0%) documents\n",
      "[2022-09-12 05:22:12,322] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:22:12,406] adding document #1570000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:22:36,236] discarding 30479 tokens: [('seelard', 1), ('derlwyn', 1), ('berça', 1), ('muscelul', 1), ('capieau', 1), ('ettyket', 1), ('pooey', 1), ('ecueil', 1), ('escoueuille', 1), ('escueles', 1)]...\n",
      "[2022-09-12 05:22:36,239] keeping 2000000 tokens which were in no less than 0 and no more than 1580000 (=100.0%) documents\n",
      "[2022-09-12 05:22:41,163] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:22:41,248] adding document #1580000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:23:04,052] discarding 32871 tokens: [('hannedouche', 1), ('boszacky', 1), ('metalochimic', 1), ('turcuș', 1), ('cortiguera', 1), ('gredilla', 1), ('nidáguila', 1), ('nocedo', 1), ('quintanaloma', 1), ('paraspheres', 1)]...\n",
      "[2022-09-12 05:23:04,054] keeping 2000000 tokens which were in no less than 0 and no more than 1590000 (=100.0%) documents\n",
      "[2022-09-12 05:23:08,855] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:23:08,941] adding document #1590000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:23:29,796] discarding 28462 tokens: [('bapx', 1), ('eureptiles', 1), ('morganucodont', 1), ('morganucodontidae', 1), ('theromorphs', 1), ('ursich', 1), ('passionettes', 1), ('bivariant', 1), ('kasilyo', 1), ('kesilyo', 1)]...\n",
      "[2022-09-12 05:23:29,798] keeping 2000000 tokens which were in no less than 0 and no more than 1600000 (=100.0%) documents\n",
      "[2022-09-12 05:23:33,175] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:23:33,233] adding document #1600000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:23:56,338] discarding 32489 tokens: [('bondway', 1), ('viadux', 1), ('lbnr', 1), ('wpyc', 1), ('deasonville', 1), ('rolingson', 1), ('colwitz', 1), ('aljomaa', 1), ('annapoornamma', 1), ('chukkalu', 1)]...\n",
      "[2022-09-12 05:23:56,340] keeping 2000000 tokens which were in no less than 0 and no more than 1610000 (=100.0%) documents\n",
      "[2022-09-12 05:23:59,693] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:23:59,750] adding document #1610000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:24:21,611] discarding 30538 tokens: [('incased', 1), ('lysengen', 1), ('rapest', 1), ('abhyudayam', 1), ('mamayya', 1), ('mokkapati', 1), ('mrokkubadi', 1), ('parvateesam', 1), ('pativratyam', 1), ('samavesamu', 1)]...\n",
      "[2022-09-12 05:24:21,614] keeping 2000000 tokens which were in no less than 0 and no more than 1620000 (=100.0%) documents\n",
      "[2022-09-12 05:24:26,372] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:24:26,456] adding document #1620000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:24:47,976] discarding 38103 tokens: [('dinostrophinx', 1), ('diphyphyllum', 1), ('diplochone', 1), ('diplogyra', 1), ('diploria', 1), ('diplothecangia', 1), ('dipterophyllum', 1), ('disaraea', 1), ('discocoenia', 1), ('discocoeniopsis', 1)]...\n",
      "[2022-09-12 05:24:47,978] keeping 2000000 tokens which were in no less than 0 and no more than 1630000 (=100.0%) documents\n",
      "[2022-09-12 05:24:52,672] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:24:52,758] adding document #1630000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:25:14,309] discarding 40655 tokens: [('theodorvscard', 1), ('trivvltivsm', 1), ('xcardd', 1), ('museumjf', 1), ('rabioux', 1), ('vicechampion', 1), ('vilancicos', 1), ('adamala', 1), ('phosphorimidazolide', 1), ('tafds', 1)]...\n",
      "[2022-09-12 05:25:14,311] keeping 2000000 tokens which were in no less than 0 and no more than 1640000 (=100.0%) documents\n",
      "[2022-09-12 05:25:17,687] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:25:17,746] adding document #1640000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:25:38,213] discarding 28979 tokens: [('cuusoo', 1), ('しんかい', 1), ('atthow', 1), ('stevingstone', 1), ('cuaŋ', 1), ('cɯaŋ', 1), ('kwean', 1), ('qenaan', 1), ('wippercht', 1), ('dimnus', 1)]...\n",
      "[2022-09-12 05:25:38,215] keeping 2000000 tokens which were in no less than 0 and no more than 1650000 (=100.0%) documents\n",
      "[2022-09-12 05:25:41,612] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:25:41,670] adding document #1650000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:26:03,700] discarding 32710 tokens: [('salenski', 1), ('telfordi', 1), ('yamashinai', 1), ('yukonicus', 1), ('zimmermanni', 1), ('carultary', 1), ('comprovinciales', 1), ('elimarius', 1), ('kernévez', 1), ('leoniæ', 1)]...\n",
      "[2022-09-12 05:26:03,701] keeping 2000000 tokens which were in no less than 0 and no more than 1660000 (=100.0%) documents\n",
      "[2022-09-12 05:26:08,414] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:26:08,498] adding document #1660000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:26:34,702] discarding 32046 tokens: [('accarezza', 1), ('animallogic', 1), ('gransito', 1), ('kingdomfeature', 1), ('ozpetek', 1), ('riconoscere', 1), ('scenografie', 1), ('voltapagine', 1), ('bolai', 1), ('borgadars', 1)]...\n",
      "[2022-09-12 05:26:34,704] keeping 2000000 tokens which were in no less than 0 and no more than 1670000 (=100.0%) documents\n",
      "[2022-09-12 05:26:39,468] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:26:39,553] adding document #1670000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:27:00,911] discarding 35072 tokens: [('utseende', 1), ('plaster#6', 1), ('saleman', 1), ('radiobrás', 1), ('ardbennie', 1), ('eibocs', 1), ('muparutsa', 1), ('nhamhondera', 1), ('prosound', 1), ('rusike', 1)]...\n",
      "[2022-09-12 05:27:00,913] keeping 2000000 tokens which were in no less than 0 and no more than 1680000 (=100.0%) documents\n",
      "[2022-09-12 05:27:05,644] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:27:05,729] adding document #1680000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:27:26,212] discarding 29330 tokens: [('simbli', 1), ('rgct', 1), ('sherifs', 1), ('biochemstry', 1), ('chrystabel', 1), ('metbio', 1), ('ephiriel', 1), ('qwote', 1), ('hamaōtsu', 1), ('linehool', 1)]...\n",
      "[2022-09-12 05:27:26,214] keeping 2000000 tokens which were in no less than 0 and no more than 1690000 (=100.0%) documents\n",
      "[2022-09-12 05:27:31,421] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:27:31,507] adding document #1690000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:27:53,210] discarding 37559 tokens: [('pasmem', 1), ('gcsp', 1), ('levyna', 1), ('smtpe', 1), ('adonyev', 1), ('baturintseva', 1), ('boerie', 1), ('endoursed', 1), ('mcrice', 1), ('hydraulika', 1)]...\n",
      "[2022-09-12 05:27:53,211] keeping 2000000 tokens which were in no less than 0 and no more than 1700000 (=100.0%) documents\n",
      "[2022-09-12 05:27:57,985] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:27:58,072] adding document #1700000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:28:22,581] discarding 43906 tokens: [('czareanah', 1), ('getalla', 1), ('tablate', 1), ('bocmacauandgrandlisboa', 1), ('wienies', 1), ('hariharalay', 1), ('tckl', 1), ('xiaolüren', 1), ('colligiana', 1), ('bitparts', 1)]...\n",
      "[2022-09-12 05:28:22,583] keeping 2000000 tokens which were in no less than 0 and no more than 1710000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:28:26,954] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:28:27,013] adding document #1710000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:28:49,011] discarding 37604 tokens: [('radwaniec', 1), ('wieruszew', 1), ('białogród', 1), ('modrzerzewo', 1), ('spławiecka', 1), ('słaboludź', 1), ('zberzynek', 1), ('osowce', 1), ('podłężna', 1), ('pąchów', 1)]...\n",
      "[2022-09-12 05:28:49,013] keeping 2000000 tokens which were in no less than 0 and no more than 1720000 (=100.0%) documents\n",
      "[2022-09-12 05:28:53,795] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:28:53,883] adding document #1720000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:29:15,933] discarding 32362 tokens: [('olipporu', 1), ('parasparam', 1), ('pushparagam', 1), ('sairabanu', 1), ('satbeer', 1), ('thrissivaperoor', 1), ('ummukulsu', 1), ('virrudh', 1), ('yahaaan', 1), ('gainline', 1)]...\n",
      "[2022-09-12 05:29:15,935] keeping 2000000 tokens which were in no less than 0 and no more than 1730000 (=100.0%) documents\n",
      "[2022-09-12 05:29:20,765] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:29:20,852] adding document #1730000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:29:44,785] discarding 32326 tokens: [('gerenda', 1), ('aškrabić', 1), ('thenthal', 1), ('vtpip', 1), ('apiden', 1), ('hawaiischen', 1), ('oberösterr', 1), ('rüthersdorf', 1), ('synonymie', 1), ('wildbienenforschung', 1)]...\n",
      "[2022-09-12 05:29:44,787] keeping 2000000 tokens which were in no less than 0 and no more than 1740000 (=100.0%) documents\n",
      "[2022-09-12 05:29:49,865] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:29:49,952] adding document #1740000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:30:13,479] discarding 30617 tokens: [('ruzatullah', 1), ('idahostatearchives', 1), ('kfau', 1), ('libraryhost', 1), ('owyee', 1), ('eldonian', 1), ('silvestrian', 1), ('broadlines', 1), ('aenk', 1), ('supersymmetic', 1)]...\n",
      "[2022-09-12 05:30:13,481] keeping 2000000 tokens which were in no less than 0 and no more than 1750000 (=100.0%) documents\n",
      "[2022-09-12 05:30:18,177] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:30:18,263] adding document #1750000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:30:42,926] discarding 28915 tokens: [('localhikes', 1), ('shiveringly', 1), ('ghazrael', 1), ('vowchurch', 1), ('adventrurer', 1), ('hawkston', 1), ('ruweila', 1), ('garnati', 1), ('hajduporta', 1), ('hysmaelita', 1)]...\n",
      "[2022-09-12 05:30:42,928] keeping 2000000 tokens which were in no less than 0 and no more than 1760000 (=100.0%) documents\n",
      "[2022-09-12 05:30:46,298] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:30:46,357] adding document #1760000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:31:09,679] discarding 31336 tokens: [('crossbook', 1), ('havresham', 1), ('nprd', 1), ('aucrug', 1), ('haddens', 1), ('hoquerug', 1), ('moineddin', 1), ('sagaku', 1), ('huggeleg', 1), ('akac', 1)]...\n",
      "[2022-09-12 05:31:09,681] keeping 2000000 tokens which were in no less than 0 and no more than 1770000 (=100.0%) documents\n",
      "[2022-09-12 05:31:13,797] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:31:13,856] adding document #1770000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:31:38,574] discarding 33145 tokens: [('kymatik', 1), ('bearlakeg', 1), ('broadwaterg', 1), ('eaglelakeg', 1), ('eaglelakeq', 1), ('igraphics', 1), ('mtxc', 1), ('slaew', 1), ('slaex', 1), ('slafd', 1)]...\n",
      "[2022-09-12 05:31:38,576] keeping 2000000 tokens which were in no less than 0 and no more than 1780000 (=100.0%) documents\n",
      "[2022-09-12 05:31:43,355] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:31:43,441] adding document #1780000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:32:11,046] discarding 28645 tokens: [('epsiplus', 1), ('kōnishi', 1), ('wallycar', 1), ('laserbase', 1), ('yoshizi', 1), ('八據點潟湖', 1), ('北波海山', 1), ('北衛灘', 1), ('南衛灘', 1), ('大王庙', 1)]...\n",
      "[2022-09-12 05:32:11,048] keeping 2000000 tokens which were in no less than 0 and no more than 1790000 (=100.0%) documents\n",
      "[2022-09-12 05:32:15,811] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:32:15,897] adding document #1790000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:32:46,748] discarding 26338 tokens: [('北京故事', 1), ('北京故事续篇', 1), ('蓝宇续集', 1), ('abudo', 1), ('muhate', 1), ('opello', 1), ('relationshowever', 1), ('sharfudine', 1), ('dispositionion', 1), ('montye', 1)]...\n",
      "[2022-09-12 05:32:46,750] keeping 2000000 tokens which were in no less than 0 and no more than 1800000 (=100.0%) documents\n",
      "[2022-09-12 05:32:50,157] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:32:50,215] adding document #1800000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:33:12,085] discarding 29133 tokens: [('secretarian', 1), ('secularmainly', 1), ('seculartraditionally', 1), ('shaghila', 1), ('siryani', 1), ('tajamo', 1), ('takattol', 1), ('taliyeh', 1), ('tashnagtsutiun', 1), ('tawhidiyya', 1)]...\n",
      "[2022-09-12 05:33:12,087] keeping 2000000 tokens which were in no less than 0 and no more than 1810000 (=100.0%) documents\n",
      "[2022-09-12 05:33:15,414] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:33:15,472] adding document #1810000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:33:38,557] discarding 30081 tokens: [('neokonservativna', 1), ('novoto', 1), ('obedineni', 1), ('patrioticnite', 1), ('rabotnicheska', 1), ('republikanči', 1), ('sbdr', 1), ('shtati', 1), ('sotsialdemokraticheska', 1), ('spsz', 1)]...\n",
      "[2022-09-12 05:33:38,559] keeping 2000000 tokens which were in no less than 0 and no more than 1820000 (=100.0%) documents\n",
      "[2022-09-12 05:33:41,923] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:33:41,982] adding document #1820000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:34:02,976] discarding 33468 tokens: [('пнзпнс', 1), ('польскае', 1), ('польшчы', 1), ('прагрэсупартия', 1), ('працыбелорусская', 1), ('праўдуговори', 1), ('псп', 1), ('півапартия', 1), ('пўазпвес', 1), ('рабочы', 1)]...\n",
      "[2022-09-12 05:34:02,978] keeping 2000000 tokens which were in no less than 0 and no more than 1830000 (=100.0%) documents\n",
      "[2022-09-12 05:34:06,320] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:34:06,379] adding document #1830000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:34:27,587] discarding 28914 tokens: [('belonoperca', 1), ('diplectanidae', 1), ('diploprionini', 1), ('epiphelinae', 1), ('gonioplectrus', 1), ('grammistini', 1), ('grammistops', 1), ('jeboehlkia', 1), ('liopropomini', 1), ('niphonini', 1)]...\n",
      "[2022-09-12 05:34:27,589] keeping 2000000 tokens which were in no less than 0 and no more than 1840000 (=100.0%) documents\n",
      "[2022-09-12 05:34:32,739] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:34:32,826] adding document #1840000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:34:54,725] discarding 27828 tokens: [('kudumbiyas', 1), ('kulavardhanasya', 1), ('kurichyan', 1), ('kutumbiya', 1), ('kâri', 1), ('lalithappa', 1), ('lalithappanu', 1), ('madisi', 1), ('manikyapuri', 1), ('mukkanna', 1)]...\n",
      "[2022-09-12 05:34:54,727] keeping 2000000 tokens which were in no less than 0 and no more than 1850000 (=100.0%) documents\n",
      "[2022-09-12 05:34:58,543] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:34:58,629] adding document #1850000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:35:19,508] discarding 29202 tokens: [('denste', 1), ('drêf', 1), ('drêven', 1), ('dudiggerode', 1), ('düringerode', 1), ('eastlings', 1), ('ebischope', 1), ('ederdu', 1), ('eggere', 1), ('elbostfälisch', 1)]...\n",
      "[2022-09-12 05:35:19,510] keeping 2000000 tokens which were in no less than 0 and no more than 1860000 (=100.0%) documents\n",
      "[2022-09-12 05:35:24,143] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:35:24,229] adding document #1860000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:35:45,904] discarding 30315 tokens: [('qesarinë', 1), ('rical', 1), ('tsarica', 1), ('tsaryna', 1), ('tsarytsa', 1), ('tsisareva', 1), ('èsi', 1), ('ķeizariene', 1), ('царина', 1), ('царыца', 1)]...\n",
      "[2022-09-12 05:35:45,906] keeping 2000000 tokens which were in no less than 0 and no more than 1870000 (=100.0%) documents\n",
      "[2022-09-12 05:35:49,146] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:35:49,205] adding document #1870000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:36:11,921] discarding 29431 tokens: [('gateherst', 1), ('ahakia', 1), ('curvatural', 1), ('epikeratome', 1), ('hypermetropes', 1), ('centrumpárt', 1), ('hmdk', 1), ('mmnök', 1), ('interfemoris', 1), ('διαμηρίζειν', 1)]...\n",
      "[2022-09-12 05:36:11,924] keeping 2000000 tokens which were in no less than 0 and no more than 1880000 (=100.0%) documents\n",
      "[2022-09-12 05:36:16,616] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:36:16,702] adding document #1880000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:36:39,641] discarding 30914 tokens: [('mokomboso', 1), ('cotachessett', 1), ('dottridge', 1), ('odence', 1), ('paupmunnuck', 1), ('shellfishermen', 1), ('aftershaving', 1), ('boycotthindustanunilever', 1), ('gskch', 1), ('huln', 1)]...\n",
      "[2022-09-12 05:36:39,643] keeping 2000000 tokens which were in no less than 0 and no more than 1890000 (=100.0%) documents\n",
      "[2022-09-12 05:36:44,357] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:36:44,445] adding document #1890000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:37:07,234] discarding 30952 tokens: [('narratologic', 1), ('neuroracer', 1), ('enrobes', 1), ('inbriginge', 1), ('sweare', 1), ('alaskatel', 1), ('buckeyetel', 1), ('buckye', 1), ('closecall', 1), ('crexendo', 1)]...\n",
      "[2022-09-12 05:37:07,236] keeping 2000000 tokens which were in no less than 0 and no more than 1900000 (=100.0%) documents\n",
      "[2022-09-12 05:37:11,870] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:37:11,957] adding document #1900000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:37:34,449] discarding 29234 tokens: [('matheia', 1), ('pequliar', 1), ('ivansmelov', 1), ('solodkov', 1), ('tsiselsky', 1), ('yanskii', 1), ('zachariev', 1), ('jebaburba', 1), ('levitant', 1), ('meekbots', 1)]...\n",
      "[2022-09-12 05:37:34,452] keeping 2000000 tokens which were in no less than 0 and no more than 1910000 (=100.0%) documents\n",
      "[2022-09-12 05:37:39,097] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:37:39,184] adding document #1910000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:38:00,178] discarding 30879 tokens: [('cuteler', 1), ('forgiatori', 1), ('forkmaker', 1), ('fadaeian', 1), ('alekseevskii', 1), ('groundspace', 1), ('sadrolhefazi', 1), ('cosmpolitan', 1), ('häuer', 1), ('khrushchevist', 1)]...\n",
      "[2022-09-12 05:38:00,179] keeping 2000000 tokens which were in no less than 0 and no more than 1920000 (=100.0%) documents\n",
      "[2022-09-12 05:38:03,483] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:38:03,543] adding document #1920000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:38:25,210] discarding 45999 tokens: [('effecti', 1), ('thymbris', 1), ('iskandahar', 1), ('gopvernment', 1), ('whitehouselawyers', 1), ('atial', 1), ('inchivila', 1), ('navoroji', 1), ('subtads', 1), ('paori', 1)]...\n",
      "[2022-09-12 05:38:25,212] keeping 2000000 tokens which were in no less than 0 and no more than 1930000 (=100.0%) documents\n",
      "[2022-09-12 05:38:30,160] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:38:30,250] adding document #1930000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:38:51,367] discarding 25766 tokens: [('nordhausenwaisenstr', 1), ('nordhausenwiki', 1), ('predigerstraße', 1), ('rautenstraße', 1), ('rodishain', 1), ('stempeda', 1), ('südharzroute', 1), ('und_gedenkstätte_dora', 1), ('waisenstraße', 1), ('walkenrieder', 1)]...\n",
      "[2022-09-12 05:38:51,369] keeping 2000000 tokens which were in no less than 0 and no more than 1940000 (=100.0%) documents\n",
      "[2022-09-12 05:38:54,658] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:38:54,716] adding document #1940000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:39:19,719] discarding 32132 tokens: [('rawbah', 1), ('tashhez', 1), ('bortbyting', 1), ('byerholm', 1), ('coisricim', 1), ('gairim', 1), ('lheiy', 1), ('odmieńce', 1), ('roggenmuhme', 1), ('roggenmutter', 1)]...\n",
      "[2022-09-12 05:39:19,721] keeping 2000000 tokens which were in no less than 0 and no more than 1950000 (=100.0%) documents\n",
      "[2022-09-12 05:39:24,445] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:39:24,533] adding document #1950000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:39:49,524] discarding 31869 tokens: [('kilwynet', 1), ('netherhill', 1), ('stangrant', 1), ('yewcrest', 1), ('belsuites', 1), ('gudaibya', 1), ('memmar', 1), ('tijaria', 1), ('okoua', 1), ('glenore', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:39:49,527] keeping 2000000 tokens which were in no less than 0 and no more than 1960000 (=100.0%) documents\n",
      "[2022-09-12 05:39:54,434] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:39:54,521] adding document #1960000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:40:17,342] discarding 31439 tokens: [('arubamu', 1), ('hajimatta', 1), ('nareru', 1), ('tsukuritakute', 1), ('yuiから', 1), ('あれば', 1), ('いつだって', 1), ('このアルバムを', 1), ('その物語は', 1), ('となりで', 1)]...\n",
      "[2022-09-12 05:40:17,344] keeping 2000000 tokens which were in no less than 0 and no more than 1970000 (=100.0%) documents\n",
      "[2022-09-12 05:40:20,609] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:40:20,668] adding document #1970000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:40:43,244] discarding 37770 tokens: [('brinë', 1), ('brundusina', 1), ('ciccioricciobrindisi', 1), ('criscituni', 1), ('edipower', 1), ('federchimica', 1), ('negaroamaro', 1), ('ostunensis', 1), ('pampasciuli', 1), ('pampasciuni', 1)]...\n",
      "[2022-09-12 05:40:43,246] keeping 2000000 tokens which were in no less than 0 and no more than 1980000 (=100.0%) documents\n",
      "[2022-09-12 05:40:47,944] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:40:48,032] adding document #1980000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:41:12,847] discarding 33729 tokens: [('我们将会更加了解东巴文字', 1), ('玛里玛莎文', 1), ('納西', 1), ('阮坷文', 1), ('麽些', 1), ('friseesalat', 1), ('flowerpaste', 1), ('esculenta_mating_', 1), ('esculenta_swimming_', 1), ('herpfrance', 1)]...\n",
      "[2022-09-12 05:41:12,849] keeping 2000000 tokens which were in no less than 0 and no more than 1990000 (=100.0%) documents\n",
      "[2022-09-12 05:41:17,667] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:41:17,757] adding document #1990000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:41:43,790] discarding 33880 tokens: [('drdny', 1), ('grbts', 1), ('khadesh', 1), ('kmyţ', 1), ('krkmš', 1), ('krkš', 1), ('kškš', 1), ('mushanet', 1), ('mwšꜣnt', 1), ('nearin', 1)]...\n",
      "[2022-09-12 05:41:43,792] keeping 2000000 tokens which were in no less than 0 and no more than 2000000 (=100.0%) documents\n",
      "[2022-09-12 05:41:48,521] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:41:48,610] adding document #2000000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:42:12,639] discarding 32304 tokens: [('calciumphosphate', 1), ('litinetsky', 1), ('litinetzky', 1), ('filmcan', 1), ('kerlo', 1), ('mirva', 1), ('poubennec', 1), ('cystidium', 1), ('lachnocladium', 1), ('lembosia', 1)]...\n",
      "[2022-09-12 05:42:12,641] keeping 2000000 tokens which were in no less than 0 and no more than 2010000 (=100.0%) documents\n",
      "[2022-09-12 05:42:17,319] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:42:17,408] adding document #2010000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:42:38,810] discarding 28296 tokens: [('abwegen', 1), ('dollheubel', 1), ('duffon', 1), ('filmdirektor', 1), ('finanzminister', 1), ('gefängnisdirektor', 1), ('hollberg', 1), ('konservator', 1), ('marczek', 1), ('scharwitz', 1)]...\n",
      "[2022-09-12 05:42:38,811] keeping 2000000 tokens which were in no less than 0 and no more than 2020000 (=100.0%) documents\n",
      "[2022-09-12 05:42:42,091] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:42:42,151] adding document #2020000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:43:04,359] discarding 32541 tokens: [('dvorje', 1), ('plinthes', 1), ('primeurs', 1), ('iraundegui', 1), ('iraurgui', 1), ('ognedal', 1), ('abadeer', 1), ('adramahlihk', 1), ('agrith', 1), ('algaliarept', 1)]...\n",
      "[2022-09-12 05:43:04,361] keeping 2000000 tokens which were in no less than 0 and no more than 2030000 (=100.0%) documents\n",
      "[2022-09-12 05:43:07,653] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:43:07,714] adding document #2030000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:43:28,914] discarding 32053 tokens: [('cyclodorippoidea', 1), ('podetremata', 1), ('podotremata', 1), ('raninoidea', 1), ('netlinc', 1), ('balyawi', 1), ('bhishm', 1), ('mayukh', 1), ('najish', 1), ('orrisa', 1)]...\n",
      "[2022-09-12 05:43:28,916] keeping 2000000 tokens which were in no less than 0 and no more than 2040000 (=100.0%) documents\n",
      "[2022-09-12 05:43:33,624] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:43:33,712] adding document #2040000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:43:55,946] discarding 32738 tokens: [('bibliobot', 1), ('pbagalleries', 1), ('wolffer', 1), ('fiskavaig', 1), ('harport', 1), ('portnalong', 1), ('apisak', 1), ('berguelang', 1), ('bodee', 1), ('boonak', 1)]...\n",
      "[2022-09-12 05:43:55,948] keeping 2000000 tokens which were in no less than 0 and no more than 2050000 (=100.0%) documents\n",
      "[2022-09-12 05:43:59,307] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:43:59,367] adding document #2050000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:44:22,050] discarding 34784 tokens: [('linwels', 1), ('manosil', 1), ('matolon', 1), ('maxeran', 1), ('maxolone', 1), ('meclam', 1), ('meclid', 1), ('meclomid', 1), ('meclopstad', 1), ('meniperan', 1)]...\n",
      "[2022-09-12 05:44:22,052] keeping 2000000 tokens which were in no less than 0 and no more than 2060000 (=100.0%) documents\n",
      "[2022-09-12 05:44:26,757] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:44:26,845] adding document #2060000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:44:49,744] discarding 31764 tokens: [('gncv', 1), ('shalvata', 1), ('konstapel', 1), ('maskinist', 1), ('squadleaders', 1), ('genapol', 1), ('attingalaru', 1), ('chemunjimotta', 1), ('kollampuzha', 1), ('kollampuzhayaru', 1)]...\n",
      "[2022-09-12 05:44:49,746] keeping 2000000 tokens which were in no less than 0 and no more than 2070000 (=100.0%) documents\n",
      "[2022-09-12 05:44:54,398] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:44:54,487] adding document #2070000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:45:20,700] discarding 31428 tokens: [('brodesky', 1), ('anticarsia', 1), ('coffeeweeds', 1), ('gemmatalis', 1), ('velvetbean', 1), ('bernhardson', 1), ('olness', 1), ('manfteuffel', 1), ('öberrhein', 1), ('borrebach', 1)]...\n",
      "[2022-09-12 05:45:20,703] keeping 2000000 tokens which were in no less than 0 and no more than 2080000 (=100.0%) documents\n",
      "[2022-09-12 05:45:25,363] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:45:25,452] adding document #2080000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:45:48,355] discarding 32179 tokens: [('catarractae', 1), ('kareao', 1), ('pūtea', 1), ('endorfinas', 1), ('selangorfc', 1), ('zaiza', 1), ('arsova', 1), ('assenova', 1), ('atanassova', 1), ('barutchijska', 1)]...\n",
      "[2022-09-12 05:45:48,358] keeping 2000000 tokens which were in no less than 0 and no more than 2090000 (=100.0%) documents\n",
      "[2022-09-12 05:45:53,436] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:45:53,527] adding document #2090000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:46:16,598] discarding 37379 tokens: [('postaviti', 1), ('postawi', 1), ('pośpieszychą', 1), ('siedziesze', 1), ('sěděaše', 1), ('sěděti', 1), ('viděti', 1), ('viděxъ', 1), ('widziech', 1), ('balagonj', 1)]...\n",
      "[2022-09-12 05:46:16,600] keeping 2000000 tokens which were in no less than 0 and no more than 2100000 (=100.0%) documents\n",
      "[2022-09-12 05:46:21,685] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:46:21,775] adding document #2100000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:46:44,457] discarding 32862 tokens: [('paragastric', 1), ('brekekekèx', 1), ('koàx', 1), ('evolussamba', 1), ('tamarear', 1), ('unencounter', 1), ('asoiwayama', 1), ('chikabumidai', 1), ('comusindopacom', 1), ('deragawa', 1)]...\n",
      "[2022-09-12 05:46:44,459] keeping 2000000 tokens which were in no less than 0 and no more than 2110000 (=100.0%) documents\n",
      "[2022-09-12 05:46:48,680] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:46:48,771] adding document #2110000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:47:11,255] discarding 35060 tokens: [('南海話', 1), ('南海话', 1), ('南番順方言', 1), ('南通話', 1), ('南通话', 1), ('南阳话', 1), ('南陽話', 1), ('厦门话', 1), ('双峰话', 1), ('古田話', 1)]...\n",
      "[2022-09-12 05:47:11,257] keeping 2000000 tokens which were in no less than 0 and no more than 2120000 (=100.0%) documents\n",
      "[2022-09-12 05:47:14,851] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:47:14,941] adding document #2120000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:47:35,875] discarding 25496 tokens: [('cinnans', 1), ('dictatores', 1), ('spinning#8', 1), ('suffecti', 1), ('aruscity', 1), ('condició', 1), ('finalista', 1), ('flaixtv', 1), ('gosadera', 1), ('gossadera', 1)]...\n",
      "[2022-09-12 05:47:35,877] keeping 2000000 tokens which were in no less than 0 and no more than 2130000 (=100.0%) documents\n",
      "[2022-09-12 05:47:40,604] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:47:40,692] adding document #2130000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:48:02,225] discarding 33240 tokens: [('halfkey', 1), ('howsells', 1), ('readitional', 1), ('textlocal', 1), ('lightbub', 1), ('galatarasaray', 1), ('imaginarme', 1), ('pilito', 1), ('afœddæ', 1), ('bleadum', 1)]...\n",
      "[2022-09-12 05:48:02,227] keeping 2000000 tokens which were in no less than 0 and no more than 2140000 (=100.0%) documents\n",
      "[2022-09-12 05:48:06,628] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:48:06,690] adding document #2140000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:48:28,682] discarding 49644 tokens: [('ταμίας', 1), ('bīnqiáo', 1), ('bǎipéng', 1), ('chéngguǎn', 1), ('chéngxiāng', 1), ('dàzhài', 1), ('fēnghuáng', 1), ('gēhán', 1), ('héhéng', 1), ('hòuyì', 1)]...\n",
      "[2022-09-12 05:48:28,684] keeping 2000000 tokens which were in no less than 0 and no more than 2150000 (=100.0%) documents\n",
      "[2022-09-12 05:48:33,466] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:48:33,559] adding document #2150000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:48:57,861] discarding 32182 tokens: [('aaboubou', 1), ('desriveaux', 1), ('primé', 1), ('energetika', 1), ('udmz', 1), ('alón', 1), ('astorganos', 1), ('astórica', 1), ('berciano', 1), ('bergidum', 1)]...\n",
      "[2022-09-12 05:48:57,863] keeping 2000000 tokens which were in no less than 0 and no more than 2160000 (=100.0%) documents\n",
      "[2022-09-12 05:49:02,555] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:49:02,644] adding document #2160000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:49:25,053] discarding 33220 tokens: [('ερμιονίδας', 1), ('ερυθρών', 1), ('ευξεινούπολης', 1), ('ευπαλίου', 1), ('ζακυνθιακός', 1), ('ζαχάρως', 1), ('ζευγολατιού', 1), ('ζεφυρίου', 1), ('ζωνιανών', 1), ('ζωφριάς', 1)]...\n",
      "[2022-09-12 05:49:25,055] keeping 2000000 tokens which were in no less than 0 and no more than 2170000 (=100.0%) documents\n",
      "[2022-09-12 05:49:28,413] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:49:28,475] adding document #2170000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:49:51,229] discarding 35224 tokens: [('långsjöån', 1), ('långsån', 1), ('långträskälven', 1), ('långträskån', 1), ('långvattsbäcken', 1), ('långvattsån', 1), ('långån', 1), ('låsån', 1), ('lödran', 1), ('löftaån', 1)]...\n",
      "[2022-09-12 05:49:51,231] keeping 2000000 tokens which were in no less than 0 and no more than 2180000 (=100.0%) documents\n",
      "[2022-09-12 05:49:55,913] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:49:56,004] adding document #2180000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:50:17,029] discarding 28018 tokens: [('ammed', 1), ('catletsburg', 1), ('destillières', 1), ('fournière', 1), ('lormois', 1), ('rainulphe', 1), ('roncé', 1), ('casblancakids', 1), ('openened', 1), ('hydroworld', 1)]...\n",
      "[2022-09-12 05:50:17,031] keeping 2000000 tokens which were in no less than 0 and no more than 2190000 (=100.0%) documents\n",
      "[2022-09-12 05:50:20,268] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:50:20,328] adding document #2190000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:50:41,517] discarding 29948 tokens: [('altsteinzeitlichen', 1), ('besiedlungsgeschichte', 1), ('fundmaterial', 1), ('galgenberges', 1), ('galgenleithen', 1), ('neolithische', 1), ('prasads', 1), ('shpanin', 1), ('campiere', 1), ('inotsume', 1)]...\n",
      "[2022-09-12 05:50:41,518] keeping 2000000 tokens which were in no less than 0 and no more than 2200000 (=100.0%) documents\n",
      "[2022-09-12 05:50:45,693] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:50:45,782] adding document #2200000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:51:08,253] discarding 28795 tokens: [('allog', 1), ('shawgo', 1), ('loginovo', 1), ('uptegrove', 1), ('anisothecium', 1), ('aongstroemia', 1), ('aongstroemiopsis', 1), ('braunfelsia', 1), ('brotherobryum', 1), ('bryotestua', 1)]...\n",
      "[2022-09-12 05:51:08,255] keeping 2000000 tokens which were in no less than 0 and no more than 2210000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:51:12,951] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:51:13,040] adding document #2210000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:51:37,320] discarding 31916 tokens: [('hrangbana', 1), ('kamalanagar', 1), ('lengpui', 1), ('selesih', 1), ('thankima', 1), ('zawlnuam', 1), ('zirtiri', 1), ('darotha', 1), ('godutai', 1), ('karkhanis', 1)]...\n",
      "[2022-09-12 05:51:37,321] keeping 2000000 tokens which were in no less than 0 and no more than 2220000 (=100.0%) documents\n",
      "[2022-09-12 05:51:42,286] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:51:42,376] adding document #2220000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:52:12,865] discarding 32544 tokens: [('délina', 1), ('camptocormia', 1), ('festinating', 1), ('ratchety', 1), ('salpètrière', 1), ('stepcore', 1), ('dushalas', 1), ('भवश', 1), ('casteilla', 1), ('anadevidia', 1)]...\n",
      "[2022-09-12 05:52:12,867] keeping 2000000 tokens which were in no less than 0 and no more than 2230000 (=100.0%) documents\n",
      "[2022-09-12 05:52:18,014] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:52:18,105] adding document #2230000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:52:42,288] discarding 32152 tokens: [('boldra', 1), ('wassinger', 1), ('mcnae', 1), ('mctrusty', 1), ('cazian', 1), ('bequita', 1), ('delpérier', 1), ('kerkove', 1), ('mușoiu', 1), ('panaet', 1)]...\n",
      "[2022-09-12 05:52:42,290] keeping 2000000 tokens which were in no less than 0 and no more than 2240000 (=100.0%) documents\n",
      "[2022-09-12 05:52:46,245] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:52:46,336] adding document #2240000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:53:08,371] discarding 47355 tokens: [('strumpers', 1), ('wellibob', 1), ('cuilenross', 1), ('primivistic', 1), ('lubyianka', 1), ('darkmarch', 1), ('deathspeakers', 1), ('delphana', 1), ('golemkore', 1), ('karrudan', 1)]...\n",
      "[2022-09-12 05:53:08,373] keeping 2000000 tokens which were in no less than 0 and no more than 2250000 (=100.0%) documents\n",
      "[2022-09-12 05:53:13,099] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:53:13,192] adding document #2250000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:53:36,611] discarding 36722 tokens: [('smaiylov', 1), ('togjanov', 1), ('adventuresky', 1), ('alienssky', 1), ('blockbusterssky', 1), ('bookssky', 1), ('bournesky', 1), ('britssky', 1), ('caribbeansky', 1), ('christmassky', 1)]...\n",
      "[2022-09-12 05:53:36,613] keeping 2000000 tokens which were in no less than 0 and no more than 2260000 (=100.0%) documents\n",
      "[2022-09-12 05:53:39,909] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:53:39,971] adding document #2260000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:54:03,140] discarding 35927 tokens: [('pourtchi', 1), ('quaend', 1), ('rotchets', 1), ('réfuge', 1), ('sabllaon', 1), ('saeure', 1), ('saonge', 1), ('sàns', 1), ('tcheur', 1), ('terjours', 1)]...\n",
      "[2022-09-12 05:54:03,143] keeping 2000000 tokens which were in no less than 0 and no more than 2270000 (=100.0%) documents\n",
      "[2022-09-12 05:54:07,891] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:54:07,983] adding document #2270000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:54:30,662] discarding 35177 tokens: [('moczulskim', 1), ('nieosobiste', 1), ('przemówienia', 1), ('przyzwoitym', 1), ('reinholda', 1), ('sabotażu', 1), ('sajderowa', 1), ('steczowicz', 1), ('szwedowską', 1), ('untergrundpresse', 1)]...\n",
      "[2022-09-12 05:54:30,665] keeping 2000000 tokens which were in no less than 0 and no more than 2280000 (=100.0%) documents\n",
      "[2022-09-12 05:54:35,760] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:54:35,851] adding document #2280000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:54:58,641] discarding 32945 tokens: [('zbork', 1), ('zdrednie', 1), ('zeptémber', 1), ('zgniłych', 1), ('zgniłéch', 1), ('zgubić', 1), ('ziamiá', 1), ('zidżiał', 1), ('zidżiáł', 1), ('zielbark', 1)]...\n",
      "[2022-09-12 05:54:58,643] keeping 2000000 tokens which were in no less than 0 and no more than 2290000 (=100.0%) documents\n",
      "[2022-09-12 05:55:02,011] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:55:02,073] adding document #2290000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:55:26,126] discarding 34396 tokens: [('ḥara', 1), ('lelere', 1), ('soldatenhalle', 1), ('weiseborn', 1), ('dchd', 1), ('manichord', 1), ('querflügel', 1), ('spinnit', 1), ('reliquos', 1), ('descheynes', 1)]...\n",
      "[2022-09-12 05:55:26,128] keeping 2000000 tokens which were in no less than 0 and no more than 2300000 (=100.0%) documents\n",
      "[2022-09-12 05:55:31,215] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:55:31,307] adding document #2300000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:55:54,516] discarding 33867 tokens: [('bonifraters', 1), ('danielg', 1), ('jezusowy', 1), ('larischów', 1), ('przykopa', 1), ('smykowska', 1), ('adityakethu', 1), ('alolupa', 1), ('amapramaadhy', 1), ('anaadhrushya', 1)]...\n",
      "[2022-09-12 05:55:54,518] keeping 2000000 tokens which were in no less than 0 and no more than 2310000 (=100.0%) documents\n",
      "[2022-09-12 05:55:59,203] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:55:59,295] adding document #2310000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:56:21,589] discarding 39435 tokens: [('crosslem', 1), ('dagarius', 1), ('darnarde', 1), ('desconneu', 1), ('dodinel', 1), ('dodinet', 1), ('dryaun', 1), ('durnor', 1), ('dysconyus', 1), ('elianz', 1)]...\n",
      "[2022-09-12 05:56:21,591] keeping 2000000 tokens which were in no less than 0 and no more than 2320000 (=100.0%) documents\n",
      "[2022-09-12 05:56:26,322] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:56:26,414] adding document #2320000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:56:48,631] discarding 32892 tokens: [('宣傳政策', 1), ('移風易俗', 1), ('豐富生活', 1), ('aouru', 1), ('popoke', 1), ('puahaere', 1), ('rangiahoheraaotea', 1), ('tamamotu', 1), ('tūkāroto', 1), ('khasbaug', 1)]...\n",
      "[2022-09-12 05:56:48,633] keeping 2000000 tokens which were in no less than 0 and no more than 2330000 (=100.0%) documents\n",
      "[2022-09-12 05:56:53,213] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:56:53,304] adding document #2330000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:57:14,800] discarding 33230 tokens: [('herlaua', 1), ('leisurezone', 1), ('leizurezone', 1), ('onlinegolf', 1), ('paringdon', 1), ('tewingas', 1), ('weligun', 1), ('caegshoe', 1), ('cayshobury', 1), ('mullanys', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 05:57:14,801] keeping 2000000 tokens which were in no less than 0 and no more than 2340000 (=100.0%) documents\n",
      "[2022-09-12 05:57:18,171] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:57:18,234] adding document #2340000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:57:41,522] discarding 31983 tokens: [('джерелознавчого', 1), ('катедрата', 1), ('легендата', 1), ('перешчепине', 1), ('софийские', 1), ('столицата', 1), ('сходознавство', 1), ('трудове', 1), ('унногундурского', 1), ('шуменски', 1)]...\n",
      "[2022-09-12 05:57:41,524] keeping 2000000 tokens which were in no less than 0 and no more than 2350000 (=100.0%) documents\n",
      "[2022-09-12 05:57:46,276] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:57:46,368] adding document #2350000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:58:10,013] discarding 28624 tokens: [('cubitopia', 1), ('jachým', 1), ('jakovlevic', 1), ('užgorod', 1), ('vilemov', 1), ('charryse', 1), ('erkenntnisvermögen', 1), ('gracov', 1), ('soyockina', 1), ('αισθητα', 1)]...\n",
      "[2022-09-12 05:58:10,015] keeping 2000000 tokens which were in no less than 0 and no more than 2360000 (=100.0%) documents\n",
      "[2022-09-12 05:58:13,333] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:58:13,395] adding document #2360000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:58:38,190] discarding 29121 tokens: [('ochreriades', 1), ('osmiini', 1), ('othinosmia', 1), ('pachyanthidium', 1), ('paradioxys', 1), ('paranthidium', 1), ('pararhophites', 1), ('pararhophitini', 1), ('plesianthidium', 1), ('prodioxys', 1)]...\n",
      "[2022-09-12 05:58:38,192] keeping 2000000 tokens which were in no less than 0 and no more than 2370000 (=100.0%) documents\n",
      "[2022-09-12 05:58:42,906] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:58:42,997] adding document #2370000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:59:07,582] discarding 31115 tokens: [('amrutur', 1), ('brissová', 1), ('immunoisolation', 1), ('jiéhóng', 1), ('lacík', 1), ('andersoniae', 1), ('freesiarefracta', 1), ('heuningrug', 1), ('kirstenboshbotgard', 1), ('wildformb', 1)]...\n",
      "[2022-09-12 05:59:07,584] keeping 2000000 tokens which were in no less than 0 and no more than 2380000 (=100.0%) documents\n",
      "[2022-09-12 05:59:10,881] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:59:10,943] adding document #2380000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:59:33,750] discarding 33539 tokens: [('cosmoptera', 1), ('csiip', 1), ('ecoexploratorium', 1), ('innoviator', 1), ('nebp', 1), ('sciencealive', 1), ('sciport', 1), ('stratostar', 1), ('blue_mosque', 1), ('mezquita_azul_', 1)]...\n",
      "[2022-09-12 05:59:33,752] keeping 2000000 tokens which were in no less than 0 and no more than 2390000 (=100.0%) documents\n",
      "[2022-09-12 05:59:37,108] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:59:37,170] adding document #2390000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 05:59:58,145] discarding 34686 tokens: [('동몽선습', 1), ('만원권', 1), ('묵포도도', 1), ('반발', 1), ('북평촌', 1), ('사친', 1), ('산수도', 1), ('신사임당의', 1), ('신사임당이다', 1), ('신인선', 1)]...\n",
      "[2022-09-12 05:59:58,147] keeping 2000000 tokens which were in no less than 0 and no more than 2400000 (=100.0%) documents\n",
      "[2022-09-12 06:00:02,975] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:00:03,068] adding document #2400000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:00:25,779] discarding 34174 tokens: [('junchiguk', 1), ('mǎoyuè', 1), ('olbyeosinmi', 1), ('seoktanbyeong', 1), ('shíyuè', 1), ('shēnyuè', 1), ('ssukdanja', 1), ('ssuktang', 1), ('sānyuè', 1), ('wèiyuè', 1)]...\n",
      "[2022-09-12 06:00:25,782] keeping 2000000 tokens which were in no less than 0 and no more than 2410000 (=100.0%) documents\n",
      "[2022-09-12 06:00:29,213] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:00:29,276] adding document #2410000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:00:50,577] discarding 32674 tokens: [('radeerick', 1), ('raderick', 1), ('smallocke', 1), ('sooping', 1), ('stercutio', 1), ('sutes', 1), ('cimberididae', 1), ('gonipterinae', 1), ('mesophyletidae', 1), ('nanophyini', 1)]...\n",
      "[2022-09-12 06:00:50,579] keeping 2000000 tokens which were in no less than 0 and no more than 2420000 (=100.0%) documents\n",
      "[2022-09-12 06:00:54,587] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:00:54,680] adding document #2420000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:01:18,453] discarding 34825 tokens: [('burgerspace', 1), ('kakahai', 1), ('miscastings', 1), ('aabama', 1), ('bostonsing', 1), ('schoolfunding', 1), ('carapas', 1), ('totsk', 1), ('guardará', 1), ('姬晉', 1)]...\n",
      "[2022-09-12 06:01:18,455] keeping 2000000 tokens which were in no less than 0 and no more than 2430000 (=100.0%) documents\n",
      "[2022-09-12 06:01:23,157] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:01:23,249] adding document #2430000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:01:45,125] discarding 34825 tokens: [('tekuani', 1), ('tepēc', 1), ('tlaminques', 1), ('tēcuani', 1), ('coequalize', 1), ('tanbāku', 1), ('تنباکو', 1), ('نهضت', 1), ('dfbw', 1), ('tgals', 1)]...\n",
      "[2022-09-12 06:01:45,127] keeping 2000000 tokens which were in no less than 0 and no more than 2440000 (=100.0%) documents\n",
      "[2022-09-12 06:01:50,112] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:01:50,204] adding document #2440000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:02:10,814] discarding 27809 tokens: [('立神', 1), ('翔鶴', 1), ('艦船', 1), ('若草', 1), ('親潮', 1), ('迅鯨', 1), ('速吸', 1), ('鈴谷', 1), ('隼鷹', 1), ('雑役船の公称番号及船種変更の件', 1)]...\n",
      "[2022-09-12 06:02:10,816] keeping 2000000 tokens which were in no less than 0 and no more than 2450000 (=100.0%) documents\n",
      "[2022-09-12 06:02:14,121] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:02:14,183] adding document #2450000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:02:35,318] discarding 28408 tokens: [('minamishinozakimachi', 1), ('ninoechō', 1), ('nishiichinoe', 1), ('nishikoiwa', 1), ('nishikomatsugawachō', 1), ('nishimizue', 1), ('nīhori', 1), ('okinomiyachō', 1), ('shimoshinozakimachi', 1), ('shinozakimachi', 1)]...\n",
      "[2022-09-12 06:02:35,320] keeping 2000000 tokens which were in no less than 0 and no more than 2460000 (=100.0%) documents\n",
      "[2022-09-12 06:02:38,617] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:02:38,679] adding document #2460000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:03:02,668] discarding 31871 tokens: [('cityhallnews', 1), ('full_story', 1), ('queensledger', 1), ('underdog_', 1), ('boroștean', 1), ('bourceanu', 1), ('burdujan', 1), ('calcan', 1), ('chipirliu', 1), ('cojocărel', 1)]...\n",
      "[2022-09-12 06:03:02,670] keeping 2000000 tokens which were in no less than 0 and no more than 2470000 (=100.0%) documents\n",
      "[2022-09-12 06:03:07,127] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:03:07,219] adding document #2470000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:03:29,563] discarding 29830 tokens: [('arzika', 1), ('babgana', 1), ('jichuan', 1), ('nabungudu', 1), ('nagarta', 1), ('ojeba', 1), ('conscientization', 1), ('makagon', 1), ('passerson', 1), ('rurual', 1)]...\n",
      "[2022-09-12 06:03:29,565] keeping 2000000 tokens which were in no less than 0 and no more than 2480000 (=100.0%) documents\n",
      "[2022-09-12 06:03:33,211] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:03:33,273] adding document #2480000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:03:56,494] discarding 63536 tokens: [('andamane', 1), ('mijango', 1), ('sebarga', 1), ('tabbora', 1), ('tabunia', 1), ('nérita', 1), ('peloronta', 1), ('cinquiéme', 1), ('lépidoptéres', 1), ('andaloro', 1)]...\n",
      "[2022-09-12 06:03:56,496] keeping 2000000 tokens which were in no less than 0 and no more than 2490000 (=100.0%) documents\n",
      "[2022-09-12 06:03:59,835] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:03:59,901] adding document #2490000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:04:25,161] discarding 39715 tokens: [('goreny', 1), ('kéménd', 1), ('cråzznicz', 1), ('grafnitz', 1), ('krasnicz', 1), ('kresnice_', 1), ('kräznicz', 1), ('zapodje', 1), ('vučipolje', 1), ('taunk', 1)]...\n",
      "[2022-09-12 06:04:25,163] keeping 2000000 tokens which were in no less than 0 and no more than 2500000 (=100.0%) documents\n",
      "[2022-09-12 06:04:29,962] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:04:30,062] adding document #2500000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:04:52,877] discarding 33503 tokens: [('clarinetitis', 1), ('embioticidae', 1), ('ulreyi', 1), ('ipoola', 1), ('yakoyo', 1), ('marcelene', 1), ('condart', 1), ('fredor', 1), ('vaerkstaeder', 1), ('maclevy', 1)]...\n",
      "[2022-09-12 06:04:52,879] keeping 2000000 tokens which were in no less than 0 and no more than 2510000 (=100.0%) documents\n",
      "[2022-09-12 06:04:56,265] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:04:56,334] adding document #2510000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:05:19,650] discarding 33018 tokens: [('kudres', 1), ('kudreš', 1), ('collicut', 1), ('falkridge', 1), ('newzones', 1), ('nilex', 1), ('sakastew', 1), ('ehrengranath', 1), ('förtrogna', 1), ('patjas', 1)]...\n",
      "[2022-09-12 06:05:19,652] keeping 2000000 tokens which were in no less than 0 and no more than 2520000 (=100.0%) documents\n",
      "[2022-09-12 06:05:24,677] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:05:24,779] adding document #2520000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:05:48,019] discarding 34161 tokens: [('carliner', 1), ('coolmain', 1), ('chornohus', 1), ('lutzak', 1), ('michalcheon', 1), ('mihalcheon', 1), ('mihălcean', 1), ('dehado', 1), ('freewall', 1), ('magmahal', 1)]...\n",
      "[2022-09-12 06:05:48,021] keeping 2000000 tokens which were in no less than 0 and no more than 2530000 (=100.0%) documents\n",
      "[2022-09-12 06:05:52,749] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:05:52,843] adding document #2530000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:06:13,634] discarding 30311 tokens: [('skewville', 1), ('uncomissioned', 1), ('bonatstraße', 1), ('helmboldstraße', 1), ('nordelbien', 1), ('bimoto', 1), ('caravanner', 1), ('einride', 1), ('essesse', 1), ('finishline', 1)]...\n",
      "[2022-09-12 06:06:13,635] keeping 2000000 tokens which were in no less than 0 and no more than 2540000 (=100.0%) documents\n",
      "[2022-09-12 06:06:16,997] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:06:17,060] adding document #2540000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:06:38,438] discarding 30562 tokens: [('tamlaghoskutt', 1), ('tammaskey', 1), ('tamneosker', 1), ('tamneyaskey', 1), ('tateneosker', 1), ('tauniagher', 1), ('tawnaosker', 1), ('tawneosker', 1), ('tomnasker', 1), ('townasker', 1)]...\n",
      "[2022-09-12 06:06:38,440] keeping 2000000 tokens which were in no less than 0 and no more than 2550000 (=100.0%) documents\n",
      "[2022-09-12 06:06:43,277] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:06:43,370] adding document #2550000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:07:09,858] discarding 31568 tokens: [('sidneywells', 1), ('voltigern', 1), ('aranyfüst', 1), ('aranyifjú', 1), ('asszonyok', 1), ('bakaruhában', 1), ('cikkgyűjtemény', 1), ('cseresznye', 1), ('diadalmas', 1), ('elbeszélései', 1)]...\n",
      "[2022-09-12 06:07:09,859] keeping 2000000 tokens which were in no less than 0 and no more than 2560000 (=100.0%) documents\n",
      "[2022-09-12 06:07:13,221] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:07:13,284] adding document #2560000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:07:39,873] discarding 34613 tokens: [('relite', 1), ('hadjarat', 1), ('bahaipedia', 1), ('islandheart', 1), ('woodlen', 1), ('fakiris', 1), ('klassiki', 1), ('periptosi', 1), ('vlavis', 1), ('zirinis', 1)]...\n",
      "[2022-09-12 06:07:39,874] keeping 2000000 tokens which were in no less than 0 and no more than 2570000 (=100.0%) documents\n",
      "[2022-09-12 06:07:43,194] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:07:43,257] adding document #2570000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:08:08,308] discarding 45852 tokens: [('bauernstand', 1), ('bettbrunn', 1), ('bischofes', 1), ('bogenberge', 1), ('boscos', 1), ('erziehungshäuser', 1), ('eucharistische', 1), ('festfeier', 1), ('fürbitter', 1), ('gedächtnisse', 1)]...\n",
      "[2022-09-12 06:08:08,310] keeping 2000000 tokens which were in no less than 0 and no more than 2580000 (=100.0%) documents\n",
      "[2022-09-12 06:08:12,991] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:08:13,088] adding document #2580000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:08:35,971] discarding 32851 tokens: [('visschedyk', 1), ('dubravski', 1), ('križevački', 1), ('trojstveni', 1), ('čelinac', 1), ('gmurray', 1), ('personneldirectory', 1), ('chhegu', 1), ('chhimi', 1), ('dzàdi', 1)]...\n",
      "[2022-09-12 06:08:35,972] keeping 2000000 tokens which were in no less than 0 and no more than 2590000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:08:39,374] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:08:39,438] adding document #2590000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:09:01,729] discarding 31750 tokens: [('aliubasignum', 1), ('bacteriopa', 1), ('baliolus', 1), ('chosokeialis', 1), ('cleronoma', 1), ('dallastai', 1), ('ebbei', 1), ('elaeopus', 1), ('erratus', 1), ('furcatalis', 1)]...\n",
      "[2022-09-12 06:09:01,731] keeping 2000000 tokens which were in no less than 0 and no more than 2600000 (=100.0%) documents\n",
      "[2022-09-12 06:09:05,072] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:09:05,135] adding document #2600000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:09:25,838] discarding 27841 tokens: [('brobury', 1), ('patalino', 1), ('icmla', 1), ('elisenbergløkkens', 1), ('elisenbergveien', 1), ('husflidskole', 1), ('kristinelundveien', 1), ('prodder', 1), ('karamella', 1), ('paradoxicide', 1)]...\n",
      "[2022-09-12 06:09:25,840] keeping 2000000 tokens which were in no less than 0 and no more than 2610000 (=100.0%) documents\n",
      "[2022-09-12 06:09:30,344] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:09:30,437] adding document #2610000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:09:51,677] discarding 27098 tokens: [('ненадовић', 1), ('belingeham', 1), ('frithe', 1), ('hambroughe', 1), ('pauncey', 1), ('pauncy', 1), ('surity', 1), ('natahniel', 1), ('prudlowe', 1), ('roward', 1)]...\n",
      "[2022-09-12 06:09:51,679] keeping 2000000 tokens which were in no less than 0 and no more than 2620000 (=100.0%) documents\n",
      "[2022-09-12 06:09:56,450] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:09:56,543] adding document #2620000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:10:21,978] discarding 28730 tokens: [('huckpac', 1), ('ldad', 1), ('virasin', 1), ('békaa', 1), ('ceuzb', 1), ('زحلة', 1), ('zunker', 1), ('vieiera', 1), ('manatour', 1), ('tymm', 1)]...\n",
      "[2022-09-12 06:10:21,980] keeping 2000000 tokens which were in no less than 0 and no more than 2630000 (=100.0%) documents\n",
      "[2022-09-12 06:10:25,289] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:10:25,352] adding document #2630000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:10:59,345] discarding 35517 tokens: [('heteronemertean', 1), ('homaxinella', 1), ('bassaens', 1), ('naibo', 1), ('paradifferential', 1), ('kanjū', 1), ('masuhide', 1), ('suzukiri', 1), ('ōshiba', 1), ('bigtone', 1)]...\n",
      "[2022-09-12 06:10:59,347] keeping 2000000 tokens which were in no less than 0 and no more than 2640000 (=100.0%) documents\n",
      "[2022-09-12 06:11:02,718] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:11:02,782] adding document #2640000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:11:25,764] discarding 35610 tokens: [('louisvale', 1), ('yoster', 1), ('dimenzija', 1), ('izvorot', 1), ('nadvor', 1), ('organizam', 1), ('slatkaristika', 1), ('superzen', 1), ('zamaraj', 1), ('зен', 1)]...\n",
      "[2022-09-12 06:11:25,765] keeping 2000000 tokens which were in no less than 0 and no more than 2650000 (=100.0%) documents\n",
      "[2022-09-12 06:11:29,161] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:11:29,227] adding document #2650000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:11:53,730] discarding 31853 tokens: [('icebucks', 1), ('arlequinats', 1), ('kazemaini', 1), ('reclotting', 1), ('lotnotes', 1), ('jerhal', 1), ('rakeim', 1), ('brandkogel', 1), ('grasbergen', 1), ('hintersteiner', 1)]...\n",
      "[2022-09-12 06:11:53,732] keeping 2000000 tokens which were in no less than 0 and no more than 2660000 (=100.0%) documents\n",
      "[2022-09-12 06:11:58,832] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:11:58,932] adding document #2660000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:12:21,646] discarding 34201 tokens: [('lautard', 1), ('mitrovitsa', 1), ('redjep', 1), ('pamami', 1), ('grieslegg', 1), ('tannkogel', 1), ('weißleiten', 1), ('gravayat', 1), ('vitaux', 1), ('flytspackel', 1)]...\n",
      "[2022-09-12 06:12:21,648] keeping 2000000 tokens which were in no less than 0 and no more than 2670000 (=100.0%) documents\n",
      "[2022-09-12 06:12:26,419] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:12:26,519] adding document #2670000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:12:50,336] discarding 34264 tokens: [('gothamchess', 1), ('almmo', 1), ('cascadability', 1), ('efunn', 1), ('esensors', 1), ('fuzzily', 1), ('lughofer', 1), ('nightcare', 1), ('delusioned', 1), ('kulwanth', 1)]...\n",
      "[2022-09-12 06:12:50,338] keeping 2000000 tokens which were in no less than 0 and no more than 2680000 (=100.0%) documents\n",
      "[2022-09-12 06:12:55,064] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:12:55,158] adding document #2680000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:13:20,555] discarding 40761 tokens: [('assinika', 1), ('theband', 1), ('spalteholz', 1), ('hardingjohn', 1), ('hämnäs', 1), ('jensenjette', 1), ('johnsonkelly', 1), ('jungyun', 1), ('keefekelly', 1), ('laxlasse', 1)]...\n",
      "[2022-09-12 06:13:20,557] keeping 2000000 tokens which were in no less than 0 and no more than 2690000 (=100.0%) documents\n",
      "[2022-09-12 06:13:25,340] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:13:25,435] adding document #2690000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:13:48,824] discarding 34208 tokens: [('金永穆', 1), ('이상열', 1), ('nontowered', 1), ('woolwashing', 1), ('medawela', 1), ('semiderivative', 1), ('dbiyat', 1), ('monchambert', 1), ('précéramique', 1), ('mausim', 1)]...\n",
      "[2022-09-12 06:13:48,827] keeping 2000000 tokens which were in no less than 0 and no more than 2700000 (=100.0%) documents\n",
      "[2022-09-12 06:13:53,599] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:13:53,694] adding document #2700000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:14:16,392] discarding 41317 tokens: [('ดดาว', 1), ('ดยางงาม', 1), ('ดเกต', 1), ('ดเกาะ', 1), ('ทก', 1), ('นยาว', 1), ('พง', 1), ('ยงพางคำ', 1), ('ยงห', 1), ('ยใต', 1)]...\n",
      "[2022-09-12 06:14:16,394] keeping 2000000 tokens which were in no less than 0 and no more than 2710000 (=100.0%) documents\n",
      "[2022-09-12 06:14:21,183] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:14:21,278] adding document #2710000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:14:45,632] discarding 45368 tokens: [('توست', 1), ('شمعم', 1), ('عذار', 1), ('غالب', 1), ('فراق', 1), ('کشتزار', 1), ('fundamentalkritik', 1), ('olgerdis', 1), ('gydel', 1), ('cornbred', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:14:45,634] keeping 2000000 tokens which were in no less than 0 and no more than 2720000 (=100.0%) documents\n",
      "[2022-09-12 06:14:50,762] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:14:50,860] adding document #2720000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:15:14,427] discarding 37741 tokens: [('ಶತಮ', 1), ('kinkomaahospital', 1), ('muuramekirkko', 1), ('saarenlahti', 1), ('vuorenlahti', 1), ('macdaragh', 1), ('nbtsc', 1), ('elazragh', 1), ('megrief', 1), ('boerdekreis', 1)]...\n",
      "[2022-09-12 06:15:14,429] keeping 2000000 tokens which were in no less than 0 and no more than 2730000 (=100.0%) documents\n",
      "[2022-09-12 06:15:19,190] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:15:19,286] adding document #2730000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:15:42,332] discarding 34323 tokens: [('ايڠاود', 1), ('ايڠايد', 1), ('كوتاواتو', 1), ('上海虹桥国际机场', 1), ('上海虹桥站', 1), ('红桥乡', 1), ('红桥区', 1), ('红桥镇', 1), ('虹桥乡', 1), ('虹桥路站', 1)]...\n",
      "[2022-09-12 06:15:42,333] keeping 2000000 tokens which were in no less than 0 and no more than 2740000 (=100.0%) documents\n",
      "[2022-09-12 06:15:45,683] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:15:45,746] adding document #2740000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:16:09,622] discarding 35633 tokens: [('hovendan', 1), ('illigitmate', 1), ('dyczko', 1), ('ahunahopgol', 1), ('bangsatap', 1), ('bangsataps', 1), ('chamkkot', 1), ('chamsagwan', 1), ('dangcheomuldonggul', 1), ('donggimnyeong', 1)]...\n",
      "[2022-09-12 06:16:09,624] keeping 2000000 tokens which were in no less than 0 and no more than 2750000 (=100.0%) documents\n",
      "[2022-09-12 06:16:14,404] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:16:14,500] adding document #2750000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:16:38,274] discarding 33465 tokens: [('econochoix', 1), ('arnundsen', 1), ('syurispeiloff', 1), ('chenkai', 1), ('deminet', 1), ('goutine', 1), ('handuo', 1), ('idpho', 1), ('malureanu', 1), ('nourgaliev', 1)]...\n",
      "[2022-09-12 06:16:38,275] keeping 2000000 tokens which were in no less than 0 and no more than 2760000 (=100.0%) documents\n",
      "[2022-09-12 06:16:41,648] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:16:41,712] adding document #2760000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:17:05,523] discarding 35001 tokens: [('lauroyloxy', 1), ('loroxanthin', 1), ('luteoxanthin', 1), ('lycopenal', 1), ('lycopenoate', 1), ('lycosome', 1), ('lycoxanthin', 1), ('mecdp', 1), ('nonaprenoxanthin', 1), ('oscillaxanthin', 1)]...\n",
      "[2022-09-12 06:17:05,525] keeping 2000000 tokens which were in no less than 0 and no more than 2770000 (=100.0%) documents\n",
      "[2022-09-12 06:17:10,279] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:17:10,376] adding document #2770000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:17:33,645] discarding 34989 tokens: [('mpekweni', 1), ('seasideness', 1), ('silwerstroom', 1), ('cfomputing', 1), ('eztalks', 1), ('roomware', 1), ('warrooms', 1), ('diajar', 1), ('gamelandegung', 1), ('kolènang', 1)]...\n",
      "[2022-09-12 06:17:33,646] keeping 2000000 tokens which were in no less than 0 and no more than 2780000 (=100.0%) documents\n",
      "[2022-09-12 06:17:36,983] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:17:37,049] adding document #2780000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:18:00,188] discarding 36042 tokens: [('keleşek', 1), ('kielı', 1), ('kʰijɘˈlɘ', 1), ('meiırban', 1), ('mʲɘjɘrˈbɑn', 1), ('namystan', 1), ('nɑməˈstɑn', 1), ('qaharman', 1), ('qalyppyz', 1), ('qazaqtyñ', 1)]...\n",
      "[2022-09-12 06:18:00,190] keeping 2000000 tokens which were in no less than 0 and no more than 2790000 (=100.0%) documents\n",
      "[2022-09-12 06:18:03,511] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:18:03,575] adding document #2790000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:18:27,603] discarding 39826 tokens: [('shimālī', 1), ('sämhar', 1), ('sänhet', 1), ('särayé', 1), ('sätit', 1), ('ĭkel', 1), ('borysevicz', 1), ('deidrea', 1), ('desktopaero', 1), ('foilsim', 1)]...\n",
      "[2022-09-12 06:18:27,605] keeping 2000000 tokens which were in no less than 0 and no more than 2800000 (=100.0%) documents\n",
      "[2022-09-12 06:18:32,384] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:18:32,480] adding document #2800000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:18:56,429] discarding 34270 tokens: [('rogetis', 1), ('sapresti', 1), ('suspendió', 1), ('veniamus', 1), ('veniatis', 1), ('vinieras', 1), ('voorwaardelijke', 1), ('zegene', 1), ('zegent', 1), ('šāʾ', 1)]...\n",
      "[2022-09-12 06:18:56,431] keeping 2000000 tokens which were in no less than 0 and no more than 2810000 (=100.0%) documents\n",
      "[2022-09-12 06:19:01,142] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:19:01,237] adding document #2810000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:19:24,123] discarding 31475 tokens: [('jasztrebie', 1), ('zofiowka', 1), ('syrkov', 1), ('zverin', 1), ('zverinets', 1), ('groenwegen', 1), ('sieveri', 1), ('dasonomia', 1), ('departemento', 1), ('ecologicos', 1)]...\n",
      "[2022-09-12 06:19:24,125] keeping 2000000 tokens which were in no less than 0 and no more than 2820000 (=100.0%) documents\n",
      "[2022-09-12 06:19:27,550] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:19:27,614] adding document #2820000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:19:51,989] discarding 34548 tokens: [('amascos', 1), ('carbonfibres', 1), ('dicsősége', 1), ('hunyadiak', 1), ('személyiség', 1), ('merechevschina', 1), ('brahmcharis', 1), ('duladeo', 1), ('duladeva', 1), ('jajahuti', 1)]...\n",
      "[2022-09-12 06:19:51,992] keeping 2000000 tokens which were in no less than 0 and no more than 2830000 (=100.0%) documents\n",
      "[2022-09-12 06:19:56,725] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:19:56,820] adding document #2830000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:20:20,757] discarding 35829 tokens: [('phyllomanganates', 1), ('vaitiekunas', 1), ('alchorani', 1), ('alkorani', 1), ('amedistarum', 1), ('arithmetricis', 1), ('beryllo', 1), ('concludens', 1), ('coniecturis', 1), ('cusana', 1)]...\n",
      "[2022-09-12 06:20:20,758] keeping 2000000 tokens which were in no less than 0 and no more than 2840000 (=100.0%) documents\n",
      "[2022-09-12 06:20:24,192] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:20:24,258] adding document #2840000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:20:46,908] discarding 35290 tokens: [('quzixi', 1), ('saixi', 1), ('sanjiaoxi', 1), ('shaoju', 1), ('siguxian', 1), ('sizhouxi', 1), ('tongzixi', 1), ('wuyinxi', 1), ('xiaoquxi', 1), ('xingganxi', 1)]...\n",
      "[2022-09-12 06:20:46,910] keeping 2000000 tokens which were in no less than 0 and no more than 2850000 (=100.0%) documents\n",
      "[2022-09-12 06:20:51,573] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:20:51,668] adding document #2850000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:21:16,609] discarding 36311 tokens: [('beautyboutiques', 1), ('medisystem', 1), ('wellwise', 1), ('grouchkateer', 1), ('afropulse', 1), ('chevychasemar', 1), ('sproj', 1), ('hfradio', 1), ('bingdian', 1), ('bīngdiǎn', 1)]...\n",
      "[2022-09-12 06:21:16,611] keeping 2000000 tokens which were in no less than 0 and no more than 2860000 (=100.0%) documents\n",
      "[2022-09-12 06:21:20,027] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:21:20,093] adding document #2860000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:21:42,988] discarding 32658 tokens: [('celetus', 1), ('draculo', 1), ('pogognathus', 1), ('ganzulia', 1), ('alungata', 1), ('atrisaxi', 1), ('binhdinhensis', 1), ('caudana', 1), ('charneri', 1), ('desmiti', 1)]...\n",
      "[2022-09-12 06:21:42,990] keeping 2000000 tokens which were in no less than 0 and no more than 2870000 (=100.0%) documents\n",
      "[2022-09-12 06:21:47,740] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:21:47,836] adding document #2870000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:22:11,129] discarding 36475 tokens: [('frotamerica', 1), ('frotamericashipwreck', 1), ('shipwreckmahenofraserisland', 1), ('wreccum', 1), ('dysmerski', 1), ('lotysch', 1), ('mcdonacle', 1), ('slegt', 1), ('syntje', 1), ('multiderivative', 1)]...\n",
      "[2022-09-12 06:22:11,130] keeping 2000000 tokens which were in no less than 0 and no more than 2880000 (=100.0%) documents\n",
      "[2022-09-12 06:22:14,525] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:22:14,589] adding document #2880000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:22:39,117] discarding 32767 tokens: [('keepthewebopen', 1), ('arielia', 1), ('citharopsis', 1), ('clinomitra', 1), ('cymakra', 1), ('diptychomitra', 1), ('helenella', 1), ('maorimorpha', 1), ('mitrellatoma', 1), ('mitriform', 1)]...\n",
      "[2022-09-12 06:22:39,119] keeping 2000000 tokens which were in no less than 0 and no more than 2890000 (=100.0%) documents\n",
      "[2022-09-12 06:22:43,866] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:22:43,962] adding document #2890000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:23:08,283] discarding 33828 tokens: [('charitarian', 1), ('cháonán', 1), ('chénghǎi', 1), ('háojiāng', 1), ('kakchio', 1), ('kialat', 1), ('shashanping', 1), ('dǒumén', 1), ('gaolanggangport', 1), ('hǎiyáng', 1)]...\n",
      "[2022-09-12 06:23:08,285] keeping 2000000 tokens which were in no less than 0 and no more than 2900000 (=100.0%) documents\n",
      "[2022-09-12 06:23:13,086] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:23:13,183] adding document #2900000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:23:36,513] discarding 31528 tokens: [('faceplace', 1), ('garnsviken', 1), ('kyrkroin', 1), ('sydväst', 1), ('chōichirō', 1), ('aberglasslyn', 1), ('allynbrook', 1), ('americymru', 1), ('bagwyllydiart', 1), ('brynderwyn', 1)]...\n",
      "[2022-09-12 06:23:36,515] keeping 2000000 tokens which were in no less than 0 and no more than 2910000 (=100.0%) documents\n",
      "[2022-09-12 06:23:41,327] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:23:41,423] adding document #2910000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:24:06,957] discarding 34650 tokens: [('furnariinae', 1), ('furnariini', 1), ('graytails', 1), ('philydorini', 1), ('pygarrhichini', 1), ('sclerurinae', 1), ('serrrana', 1), ('synallaxini', 1), ('sbierski', 1), ('wishbells', 1)]...\n",
      "[2022-09-12 06:24:06,959] keeping 2000000 tokens which were in no less than 0 and no more than 2920000 (=100.0%) documents\n",
      "[2022-09-12 06:24:11,692] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:24:11,788] adding document #2920000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:24:37,341] discarding 33511 tokens: [('antigraceful', 1), ('beerts', 1), ('chromogonie', 1), ('danih', 1), ('headstone_umberto_boccioni_chievo', 1), ('passèist', 1), ('simultanvisionen', 1), ('spiralförmige', 1), ('umbertoboccioni', 1), ('maximumvelocity', 1)]...\n",
      "[2022-09-12 06:24:37,343] keeping 2000000 tokens which were in no less than 0 and no more than 2930000 (=100.0%) documents\n",
      "[2022-09-12 06:24:40,738] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:24:40,804] adding document #2930000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:25:04,380] discarding 32775 tokens: [('gigueuse', 1), ('grocerie', 1), ('mechetagouine', 1), ('monmarquette', 1), ('turluté', 1), ('turlutée', 1), ('leptosomatidae', 1), ('nemaslan', 1), ('keysorting', 1), ('béllas', 1)]...\n",
      "[2022-09-12 06:25:04,381] keeping 2000000 tokens which were in no less than 0 and no more than 2940000 (=100.0%) documents\n",
      "[2022-09-12 06:25:07,801] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:25:07,866] adding document #2940000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:25:33,668] discarding 36186 tokens: [('patchinsville', 1), ('sumpawam', 1), ('sumpawams', 1), ('sumpwams', 1), ('quaheagh', 1), ('merserau', 1), ('polishtown', 1), ('awamock', 1), ('manhansack', 1), ('poggatticut', 1)]...\n",
      "[2022-09-12 06:25:33,670] keeping 2000000 tokens which were in no less than 0 and no more than 2950000 (=100.0%) documents\n",
      "[2022-09-12 06:25:38,797] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:25:38,893] adding document #2950000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:26:02,887] discarding 33542 tokens: [('düşmez', 1), ('düşməz', 1), ('dərya', 1), ('eglenmez', 1), ('enər', 1), ('ertiriňiz', 1), ('erär', 1), ('eýesi', 1), ('gapyl', 1), ('garasy', 1)]...\n",
      "[2022-09-12 06:26:02,889] keeping 2000000 tokens which were in no less than 0 and no more than 2960000 (=100.0%) documents\n",
      "[2022-09-12 06:26:07,648] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:26:07,744] adding document #2960000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:26:32,317] discarding 38935 tokens: [('woerdemann', 1), ('febreuary', 1), ('glavnokomandovaniya', 1), ('aardbei', 1), ('allambique', 1), ('beerstyle', 1), ('bezomerd', 1), ('lambicq', 1), ('lambikland', 1), ('meertsbier', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:26:32,319] keeping 2000000 tokens which were in no less than 0 and no more than 2970000 (=100.0%) documents\n",
      "[2022-09-12 06:26:37,276] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:26:37,374] adding document #2970000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:27:01,602] discarding 34658 tokens: [('bierboxer', 1), ('boxl', 1), ('buchsen', 1), ('buxn', 1), ('hystiocytic', 1), ('schecken', 1), ('thegang', 1), ('toneissen', 1), ('chernokamen', 1), ('cholesteroids', 1)]...\n",
      "[2022-09-12 06:27:01,604] keeping 2000000 tokens which were in no less than 0 and no more than 2980000 (=100.0%) documents\n",
      "[2022-09-12 06:27:06,396] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:27:06,493] adding document #2980000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:27:31,036] discarding 29414 tokens: [('dairmid', 1), ('hallidon', 1), ('heringman', 1), ('mathematicised', 1), ('sinnone', 1), ('tarrot', 1), ('anisurum', 1), ('hankinsoni', 1), ('neogaeus', 1), ('ceunet', 1)]...\n",
      "[2022-09-12 06:27:31,038] keeping 2000000 tokens which were in no less than 0 and no more than 2990000 (=100.0%) documents\n",
      "[2022-09-12 06:27:35,870] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:27:35,967] adding document #2990000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:28:04,675] discarding 33151 tokens: [('papyral', 1), ('proficiscentis', 1), ('acfea', 1), ('honourscitation', 1), ('droep', 1), ('dâzh', 1), ('gyûme', 1), ('gäpo', 1), ('kyongwä', 1), ('sanggä', 1)]...\n",
      "[2022-09-12 06:28:04,677] keeping 2000000 tokens which were in no less than 0 and no more than 3000000 (=100.0%) documents\n",
      "[2022-09-12 06:28:09,205] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:28:09,302] adding document #3000000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:28:33,114] discarding 31327 tokens: [('extremerly', 1), ('farouque', 1), ('additionalnotes', 1), ('araszkiewicz', 1), ('asensky', 1), ('augustynka', 1), ('bogdanki', 1), ('budziłek', 1), ('colorit', 1), ('dolcan', 1)]...\n",
      "[2022-09-12 06:28:33,116] keeping 2000000 tokens which were in no less than 0 and no more than 3010000 (=100.0%) documents\n",
      "[2022-09-12 06:28:37,931] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:28:38,026] adding document #3010000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:29:02,489] discarding 35784 tokens: [('dagurashibanipal', 1), ('darkeater', 1), ('draigoch', 1), ('eldrax', 1), ('elvarg', 1), ('feyrbrand', 1), ('grandeeney', 1), ('grimleal', 1), ('inclding', 1), ('kakaranatharans', 1)]...\n",
      "[2022-09-12 06:29:02,491] keeping 2000000 tokens which were in no less than 0 and no more than 3020000 (=100.0%) documents\n",
      "[2022-09-12 06:29:07,332] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:29:07,430] adding document #3020000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:29:31,700] discarding 28917 tokens: [('hambuchen', 1), ('hindermann', 1), ('padurariu', 1), ('varinska', 1), ('ballegoijen', 1), ('battlejuice', 1), ('caylana', 1), ('musicload', 1), ('popakademie', 1), ('popmusicdesign', 1)]...\n",
      "[2022-09-12 06:29:31,701] keeping 2000000 tokens which were in no less than 0 and no more than 3030000 (=100.0%) documents\n",
      "[2022-09-12 06:29:36,464] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:29:36,559] adding document #3030000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:30:00,602] discarding 40183 tokens: [('coptidoideae', 1), ('coptoideae', 1), ('delphinium_elatum_hybride_', 1), ('dichocarpeae', 1), ('frekkels', 1), ('glaucidioideae', 1), ('helleboroideae', 1), ('hellebroreae', 1), ('hydrastidoideae', 1), ('isopyreae', 1)]...\n",
      "[2022-09-12 06:30:00,604] keeping 2000000 tokens which were in no less than 0 and no more than 3040000 (=100.0%) documents\n",
      "[2022-09-12 06:30:03,947] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:30:04,013] adding document #3040000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:30:28,174] discarding 33284 tokens: [('prosetice', 1), ('lomaca', 1), ('herondae', 1), ('ambonne', 1), ('echangistes', 1), ('hangoartpoort', 1), ('achaici', 1), ('aeolicis', 1), ('epitaphius', 1), ('pseudaeolicis', 1)]...\n",
      "[2022-09-12 06:30:28,176] keeping 2000000 tokens which were in no less than 0 and no more than 3050000 (=100.0%) documents\n",
      "[2022-09-12 06:30:33,379] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:30:33,476] adding document #3050000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:30:57,582] discarding 33123 tokens: [('geacps', 1), ('fotografsko', 1), ('puharotype', 1), ('puharotypes', 1), ('svetlopis', 1), ('joevon', 1), ('koihonua', 1), ('vonbaron', 1), ('licklieder', 1), ('antipsychem', 1)]...\n",
      "[2022-09-12 06:30:57,584] keeping 2000000 tokens which were in no less than 0 and no more than 3060000 (=100.0%) documents\n",
      "[2022-09-12 06:31:02,291] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:31:02,387] adding document #3060000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:31:27,917] discarding 32537 tokens: [('leutge', 1), ('deepabai', 1), ('derhadi', 1), ('devalgaon', 1), ('elur', 1), ('ghorpades', 1), ('hingni', 1), ('jadhavs', 1), ('jagpalrao', 1), ('jategau', 1)]...\n",
      "[2022-09-12 06:31:27,919] keeping 2000000 tokens which were in no less than 0 and no more than 3070000 (=100.0%) documents\n",
      "[2022-09-12 06:31:32,659] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:31:32,755] adding document #3070000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:31:58,925] discarding 32824 tokens: [('purzeczko', 1), ('pwsip', 1), ('śledziewskich', 1), ('śmiarowskich', 1), ('ataqah', 1), ('attaqa', 1), ('esmæˈʕiːl', 1), ('jæˈsiːn', 1), ('espáne', 1), ('partholom', 1)]...\n",
      "[2022-09-12 06:31:58,927] keeping 2000000 tokens which were in no less than 0 and no more than 3080000 (=100.0%) documents\n",
      "[2022-09-12 06:32:03,640] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:32:03,736] adding document #3080000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:32:28,067] discarding 32986 tokens: [('waldnatur', 1), ('waldwildnis', 1), ('waldökologischen', 1), ('weidhütte', 1), ('reganum', 1), ('reganus', 1), ('bascap', 1), ('chambertrust', 1), ('codescentre', 1), ('docdex', 1)]...\n",
      "[2022-09-12 06:32:28,068] keeping 2000000 tokens which were in no less than 0 and no more than 3090000 (=100.0%) documents\n",
      "[2022-09-12 06:32:31,466] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:32:31,532] adding document #3090000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:32:56,857] discarding 33548 tokens: [('providentiensis', 1), ('acucella', 1), ('bakchus', 1), ('canepazzo', 1), ('comissare', 1), ('cotium', 1), ('ditob', 1), ('forlinghetti', 1), ('gesulado', 1), ('grazza', 1)]...\n",
      "[2022-09-12 06:32:56,858] keeping 2000000 tokens which were in no less than 0 and no more than 3100000 (=100.0%) documents\n",
      "[2022-09-12 06:33:00,285] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:33:00,352] adding document #3100000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:33:27,834] discarding 35298 tokens: [('wolsbornart', 1), ('allomethyloside', 1), ('allosido', 1), ('arabinosido', 1), ('convallatoxon', 1), ('convallotoxoloside', 1), ('corglycon', 1), ('deoxyallose', 1), ('glovewort', 1), ('glucoconvalloside', 1)]...\n",
      "[2022-09-12 06:33:27,836] keeping 2000000 tokens which were in no less than 0 and no more than 3110000 (=100.0%) documents\n",
      "[2022-09-12 06:33:32,688] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:33:32,787] adding document #3110000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:33:57,842] discarding 35984 tokens: [('polukrvni', 1), ('poluplave', 1), ('poters', 1), ('poļakova', 1), ('presoa', 1), ('prizonierul', 1), ('puoliverinen', 1), ('qədəhi', 1), ('rasāyanakkallŭ', 1), ('relikui', 1)]...\n",
      "[2022-09-12 06:33:57,844] keeping 2000000 tokens which were in no less than 0 and no more than 3120000 (=100.0%) documents\n",
      "[2022-09-12 06:34:02,644] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:34:02,742] adding document #3120000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:34:27,062] discarding 31225 tokens: [('banknotes_and_coins', 1), ('danish_coins', 1), ('kroneskillinger', 1), ('skillinger', 1), ('tømmeby', 1), ('acitretine', 1), ('acroc', 1), ('acrocephalopolydactyly', 1), ('acrodysplasia', 1), ('acrok', 1)]...\n",
      "[2022-09-12 06:34:27,064] keeping 2000000 tokens which were in no less than 0 and no more than 3130000 (=100.0%) documents\n",
      "[2022-09-12 06:34:31,862] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:34:31,960] adding document #3130000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:34:59,044] discarding 33566 tokens: [('deutschseite', 1), ('formalgrammatically', 1), ('tribeschief', 1), ('wietusch', 1), ('caxatambo', 1), ('minestuffs', 1), ('matijevec', 1), ('spöhel', 1), ('wolfsput', 1), ('armstrongoak', 1)]...\n",
      "[2022-09-12 06:34:59,046] keeping 2000000 tokens which were in no less than 0 and no more than 3140000 (=100.0%) documents\n",
      "[2022-09-12 06:35:03,907] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:35:04,004] adding document #3140000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:35:31,806] discarding 31428 tokens: [('theoì', 1), ('therápeuson', 1), ('theôn', 1), ('tripoun', 1), ('trékhei', 1), ('tàs', 1), ('táde', 1), ('tákhiston', 1), ('táratte', 1), ('têide', 1)]...\n",
      "[2022-09-12 06:35:31,809] keeping 2000000 tokens which were in no less than 0 and no more than 3150000 (=100.0%) documents\n",
      "[2022-09-12 06:35:36,568] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:35:36,665] adding document #3150000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:36:00,705] discarding 30429 tokens: [('glasdon', 1), ('harrowside', 1), ('sandgrownians', 1), ('cocopeat', 1), ('defibering', 1), ('bidialectism', 1), ('criaron', 1), ('derasatik', 1), ('intersentential', 1), ('juilli', 1)]...\n",
      "[2022-09-12 06:36:00,706] keeping 2000000 tokens which were in no less than 0 and no more than 3160000 (=100.0%) documents\n",
      "[2022-09-12 06:36:04,184] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:36:04,250] adding document #3160000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:36:29,070] discarding 30498 tokens: [('xyphoides', 1), ('ξιφοειδές', 1), ('withwreathed', 1), ('hogmal', 1), ('schlachtreste', 1), ('southeray', 1), ('devotionsummer', 1), ('dna出錯', 1), ('kôtetsu', 1), ('lovewish', 1)]...\n",
      "[2022-09-12 06:36:29,071] keeping 2000000 tokens which were in no less than 0 and no more than 3170000 (=100.0%) documents\n",
      "[2022-09-12 06:36:32,530] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:36:32,596] adding document #3170000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:36:57,060] discarding 33066 tokens: [('mizhirgi', 1), ('mt_shkhara_as_seen_from_khalde_', 1), ('tikhtengen', 1), ('uilpata', 1), ('vittfarnegeorgien', 1), ('салтинское', 1), ('ჭაუხი', 1), ('godulfus', 1), ('hatchetations', 1), ('thaybyoo', 1)]...\n",
      "[2022-09-12 06:36:57,062] keeping 2000000 tokens which were in no less than 0 and no more than 3180000 (=100.0%) documents\n",
      "[2022-09-12 06:37:01,854] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:37:01,954] adding document #3180000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:37:27,268] discarding 29759 tokens: [('ålandsfärjanrederi', 1), ('blathaich', 1), ('sodherton', 1), ('csgraph', 1), ('floydwarshallwithpathreconstruction', 1), ('matlab_bgl', 1), ('quickgraphpcl', 1), ('hamburgerology', 1), ('agnetenwal', 1), ('trevianum', 1)]...\n",
      "[2022-09-12 06:37:27,270] keeping 2000000 tokens which were in no less than 0 and no more than 3190000 (=100.0%) documents\n",
      "[2022-09-12 06:37:31,125] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:37:31,191] adding document #3190000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:37:55,492] discarding 28170 tokens: [('pissboy', 1), ('centralbahnplatz', 1), ('jesenik', 1), ('moravskoslezsky', 1), ('mscb', 1), ('opavou', 1), ('valšov', 1), ('vmoravskoslezském', 1), ('würbenthal', 1), ('alpinizmu', 1)]...\n",
      "[2022-09-12 06:37:55,494] keeping 2000000 tokens which were in no less than 0 and no more than 3200000 (=100.0%) documents\n",
      "[2022-09-12 06:37:58,931] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:37:58,998] adding document #3200000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:38:27,023] discarding 32053 tokens: [('suspex', 1), ('bertelegni', 1), ('eduardito', 1), ('meliante', 1), ('toscanito', 1), ('braidinhill', 1), ('castleatted', 1), ('morishil', 1), ('willieyeards', 1), ('williezeards', 1)]...\n",
      "[2022-09-12 06:38:27,025] keeping 2000000 tokens which were in no less than 0 and no more than 3210000 (=100.0%) documents\n",
      "[2022-09-12 06:38:30,540] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:38:30,607] adding document #3210000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:38:55,868] discarding 30302 tokens: [('guénette', 1), ('akkent', 1), ('chamob', 1), ('gyanvihar', 1), ('panchmarhi', 1), ('fuerststift', 1), ('komádi', 1), ('nurmijarvi', 1), ('tallowchandlers', 1), ('mindmup', 1)]...\n",
      "[2022-09-12 06:38:55,870] keeping 2000000 tokens which were in no less than 0 and no more than 3220000 (=100.0%) documents\n",
      "[2022-09-12 06:38:59,306] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:38:59,372] adding document #3220000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:39:24,447] discarding 32322 tokens: [('superphone', 1), ('laroucheindependent', 1), ('serrettenew', 1), ('ahorake', 1), ('areyto', 1), ('trafico', 1), ('medjidia', 1), ('neulerchenfeld', 1), ('neulerchenfelder', 1), ('powrozniak', 1)]...\n",
      "[2022-09-12 06:39:24,448] keeping 2000000 tokens which were in no less than 0 and no more than 3230000 (=100.0%) documents\n",
      "[2022-09-12 06:39:29,228] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:39:29,325] adding document #3230000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:39:52,991] discarding 36167 tokens: [('aaoni', 1), ('apperences', 1), ('armaanon', 1), ('ekbir', 1), ('kullfi', 1), ('kumarr', 1), ('miilee', 1), ('adhye', 1), ('datye', 1), ('hasita', 1)]...\n",
      "[2022-09-12 06:39:52,992] keeping 2000000 tokens which were in no less than 0 and no more than 3240000 (=100.0%) documents\n",
      "[2022-09-12 06:39:57,802] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:39:57,901] adding document #3240000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:40:22,865] discarding 39005 tokens: [('tuamoto', 1), ('twistii', 1), ('beishui', 1), ('miaobei', 1), ('shaxia', 1), ('zhangchong', 1), ('zhoubei', 1), ('cangxia', 1), ('shanglian', 1), ('zhuling', 1)]...\n",
      "[2022-09-12 06:40:22,867] keeping 2000000 tokens which were in no less than 0 and no more than 3250000 (=100.0%) documents\n",
      "[2022-09-12 06:40:27,468] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:40:27,536] adding document #3250000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:40:51,328] discarding 33858 tokens: [('netikras', 1), ('papročiai', 1), ('pukyté', 1), ('zuikis', 1), ('civilisationens', 1), ('framväxten', 1), ('fredsarbete', 1), ('förberedelse', 1), ('förberedelsemetoder', 1), ('globala', 1)]...\n",
      "[2022-09-12 06:40:51,330] keeping 2000000 tokens which were in no less than 0 and no more than 3260000 (=100.0%) documents\n",
      "[2022-09-12 06:40:56,219] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:40:56,318] adding document #3260000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:41:20,843] discarding 34242 tokens: [('arukkani', 1), ('bhoothalingam', 1), ('oorengum', 1), ('pudaka', 1), ('pudhichalum', 1), ('pudichen', 1), ('purushanthan', 1), ('samanja', 1), ('vaadaikatru', 1), ('asianfuse', 1)]...\n",
      "[2022-09-12 06:41:20,845] keeping 2000000 tokens which were in no less than 0 and no more than 3270000 (=100.0%) documents\n",
      "[2022-09-12 06:41:25,664] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:41:25,763] adding document #3270000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:41:49,930] discarding 35438 tokens: [('magallanesjulio', 1), ('mattaconsuelo', 1), ('narvaezornella', 1), ('ocagustavo', 1), ('ochoawaleska', 1), ('oderling', 1), ('perezjesus', 1), ('piñangonatacha', 1), ('regaladorenzo', 1), ('requenaroselis', 1)]...\n",
      "[2022-09-12 06:41:49,932] keeping 2000000 tokens which were in no less than 0 and no more than 3280000 (=100.0%) documents\n",
      "[2022-09-12 06:41:54,702] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:41:54,801] adding document #3280000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:42:18,119] discarding 28503 tokens: [('finlaw', 1), ('baradit', 1), ('filsa', 1), ('kaifman', 1), ('cbsongs', 1), ('ciptak', 1), ('throckmortan', 1), ('wnis', 1), ('biomagresbank', 1), ('glycanbuilder', 1)]...\n",
      "[2022-09-12 06:42:18,121] keeping 2000000 tokens which were in no less than 0 and no more than 3290000 (=100.0%) documents\n",
      "[2022-09-12 06:42:21,531] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:42:21,596] adding document #3290000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:42:46,263] discarding 34465 tokens: [('catwalked', 1), ('cgdt', 1), ('haweya', 1), ('pambili', 1), ('kakouros', 1), ('ulyanovskaya', 1), ('tilstonebank', 1), ('albez', 1), ('alron', 1), ('dafana', 1)]...\n",
      "[2022-09-12 06:42:46,265] keeping 2000000 tokens which were in no less than 0 and no more than 3300000 (=100.0%) documents\n",
      "[2022-09-12 06:42:51,102] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:42:51,202] adding document #3300000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:43:16,264] discarding 35974 tokens: [('kadhalare', 1), ('madheswaran', 1), ('poonthendrale', 1), ('poovukkul', 1), ('puthaiyal', 1), ('velaiyadu', 1), ('kandathu', 1), ('mayyama', 1), ('parvathiy', 1), ('pathimoonam', 1)]...\n",
      "[2022-09-12 06:43:16,266] keeping 2000000 tokens which were in no less than 0 and no more than 3310000 (=100.0%) documents\n",
      "[2022-09-12 06:43:21,111] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:43:21,211] adding document #3310000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:43:44,232] discarding 33160 tokens: [('neifert', 1), ('nosto', 1), ('putrer', 1), ('calou', 1), ('surattee', 1), ('malinye', 1), ('ulgulan', 1), ('borgesen', 1), ('gloiocladia', 1), ('phyllophoraceae', 1)]...\n",
      "[2022-09-12 06:43:44,234] keeping 2000000 tokens which were in no less than 0 and no more than 3320000 (=100.0%) documents\n",
      "[2022-09-12 06:43:49,123] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:43:49,222] adding document #3320000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:44:14,176] discarding 29599 tokens: [('larixia', 1), ('abbotioannis', 1), ('agrimisargos', 1), ('balourdosnikolaos', 1), ('chaldeosioannis', 1), ('christeasdimitrios', 1), ('daispavlos', 1), ('dekavalasnikolaos', 1), ('diomatarasevangelos', 1), ('dolaspetros', 1)]...\n",
      "[2022-09-12 06:44:14,178] keeping 2000000 tokens which were in no less than 0 and no more than 3330000 (=100.0%) documents\n",
      "[2022-09-12 06:44:19,282] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:44:19,379] adding document #3330000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:44:42,842] discarding 35859 tokens: [('srivijayasrama', 1), ('qaedi', 1), ('ственное', 1), ('notriggers', 1), ('bashprogor', 1), ('gibatovich', 1), ('kallimullin', 1), ('technoroid', 1), ('baraatis', 1), ('begawans', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:44:42,844] keeping 2000000 tokens which were in no less than 0 and no more than 3340000 (=100.0%) documents\n",
      "[2022-09-12 06:44:48,019] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:44:48,118] adding document #3340000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:45:11,781] discarding 32099 tokens: [('onesynergy', 1), ('festejos', 1), ('iniversal', 1), ('pemamino', 1), ('bealum', 1), ('chemda', 1), ('draded', 1), ('gardenswartz', 1), ('hladkowicz', 1), ('syatt', 1)]...\n",
      "[2022-09-12 06:45:11,783] keeping 2000000 tokens which were in no less than 0 and no more than 3350000 (=100.0%) documents\n",
      "[2022-09-12 06:45:16,631] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:45:16,729] adding document #3350000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:45:41,330] discarding 30677 tokens: [('wonderloos', 1), ('kergumyah', 1), ('almelu', 1), ('yeshwantapur', 1), ('kabeli', 1), ('herbář', 1), ('lacebug', 1), ('costanero', 1), ('rumei', 1), ('wanniang', 1)]...\n",
      "[2022-09-12 06:45:41,332] keeping 2000000 tokens which were in no less than 0 and no more than 3360000 (=100.0%) documents\n",
      "[2022-09-12 06:45:46,224] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:45:46,322] adding document #3360000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:46:10,658] discarding 33117 tokens: [('béliard', 1), ('vármúzeum', 1), ('bancessi', 1), ('ihiekwe', 1), ('deveruex', 1), ('reydo', 1), ('dosdecatorce', 1), ('aaranyak', 1), ('ambooty', 1), ('anooraadha', 1)]...\n",
      "[2022-09-12 06:46:10,660] keeping 2000000 tokens which were in no less than 0 and no more than 3370000 (=100.0%) documents\n",
      "[2022-09-12 06:46:15,519] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:46:15,618] adding document #3370000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:46:44,811] discarding 33552 tokens: [('orekkannale', 1), ('apdj', 1), ('dalgaon', 1), ('shreeks', 1), ('wugg', 1), ('frankenwaldgruppe', 1), ('course_descriptions', 1), ('uwstout', 1), ('rachette', 1), ('vasaro', 1)]...\n",
      "[2022-09-12 06:46:44,813] keeping 2000000 tokens which were in no less than 0 and no more than 3380000 (=100.0%) documents\n",
      "[2022-09-12 06:46:49,579] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:46:49,678] adding document #3380000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:47:15,727] discarding 34541 tokens: [('fogótico', 1), ('gesethame', 1), ('hueyzacatlán', 1), ('mitziton', 1), ('yalmús', 1), ('yashitinin', 1), ('chacaljocom', 1), ('chiflon', 1), ('junchavin', 1), ('yalumá', 1)]...\n",
      "[2022-09-12 06:47:15,729] keeping 2000000 tokens which were in no less than 0 and no more than 3390000 (=100.0%) documents\n",
      "[2022-09-12 06:47:20,584] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:47:20,684] adding document #3390000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:47:43,856] discarding 38072 tokens: [('benagairde', 1), ('ffquig', 1), ('rikkyū', 1), ('倒幕', 1), ('内務卿', 1), ('gensuiden', 1), ('kankojo', 1), ('microfisch', 1), ('元帥公爵大山巌', 1), ('年譜', 1)]...\n",
      "[2022-09-12 06:47:43,857] keeping 2000000 tokens which were in no less than 0 and no more than 3400000 (=100.0%) documents\n",
      "[2022-09-12 06:47:48,651] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:47:48,750] adding document #3400000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:48:11,908] discarding 35811 tokens: [('fǎhào', 1), ('fǎmíng', 1), ('garahui', 1), ('gelenmaa', 1), ('gereltü', 1), ('getlekh', 1), ('gevsh', 1), ('gonghaengnyeo', 1), ('gubanda', 1), ('guānshì', 1)]...\n",
      "[2022-09-12 06:48:11,909] keeping 2000000 tokens which were in no less than 0 and no more than 3410000 (=100.0%) documents\n",
      "[2022-09-12 06:48:16,730] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:48:16,828] adding document #3410000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:48:40,723] discarding 32362 tokens: [('ребёнка', 1), ('გველის', 1), ('პერანგი', 1), ('geosits', 1), ('hitcho', 1), ('самоубийца', 1), ('arkartdam', 1), ('khǫng', 1), ('lakhǫn', 1), ('čhaolon', 1)]...\n",
      "[2022-09-12 06:48:40,724] keeping 2000000 tokens which were in no less than 0 and no more than 3420000 (=100.0%) documents\n",
      "[2022-09-12 06:48:44,178] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:48:44,244] adding document #3420000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:49:07,601] discarding 30624 tokens: [('mccarleys', 1), ('nayausheeng', 1), ('onyotaa', 1), ('renouche', 1), ('tansleyville', 1), ('leadimll', 1), ('marooth', 1), ('mawders', 1), ('rollnecked', 1), ('wanbabes', 1)]...\n",
      "[2022-09-12 06:49:07,603] keeping 2000000 tokens which were in no less than 0 and no more than 3430000 (=100.0%) documents\n",
      "[2022-09-12 06:49:12,492] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:49:12,591] adding document #3430000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:49:36,648] discarding 32267 tokens: [('samkooksagi', 1), ('seolssi', 1), ('seonggak', 1), ('silhye', 1), ('yeolgi', 1), ('yeonpyo', 1), ('丕寧子', 1), ('實兮', 1), ('我兵', 1), ('我國', 1)]...\n",
      "[2022-09-12 06:49:36,650] keeping 2000000 tokens which were in no less than 0 and no more than 3440000 (=100.0%) documents\n",
      "[2022-09-12 06:49:41,824] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:49:41,924] adding document #3440000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:50:09,619] discarding 34165 tokens: [('вайца', 1), ('вешан', 1), ('вешаниг', 1), ('вогіур', 1), ('говр', 1), ('говрал', 1), ('говран', 1), ('говрана', 1), ('говрах', 1), ('говраца', 1)]...\n",
      "[2022-09-12 06:50:09,620] keeping 2000000 tokens which were in no less than 0 and no more than 3450000 (=100.0%) documents\n",
      "[2022-09-12 06:50:14,453] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:50:14,553] adding document #3450000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:50:36,397] discarding 31378 tokens: [('medvid', 1), ('mykhalyna', 1), ('mykhaylovich', 1), ('forfeitt', 1), ('kotape', 1), ('kwete', 1), ('magbondoline', 1), ('nyimi', 1), ('bonsink', 1), ('angenommenen', 1)]...\n",
      "[2022-09-12 06:50:36,398] keeping 2000000 tokens which were in no less than 0 and no more than 3460000 (=100.0%) documents\n",
      "[2022-09-12 06:50:39,836] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:50:39,904] adding document #3460000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:51:02,852] discarding 34315 tokens: [('angyria', 1), ('chrissí', 1), ('dragoulas', 1), ('gaiduronisi', 1), ('lychneus', 1), ('lygdos', 1), ('parikía', 1), ('tsipidos', 1), ('vounia', 1), ('xifara', 1)]...\n",
      "[2022-09-12 06:51:02,854] keeping 2000000 tokens which were in no less than 0 and no more than 3470000 (=100.0%) documents\n",
      "[2022-09-12 06:51:07,690] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:51:07,789] adding document #3470000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:51:30,713] discarding 33430 tokens: [('ไรพงศ', 1), ('ไลยวงศ', 1), ('androthi', 1), ('karrellian', 1), ('sarlac', 1), ('mœrr', 1), ('sayleperforming', 1), ('twelveperforming', 1), ('pmpnetwork', 1), ('gerho', 1)]...\n",
      "[2022-09-12 06:51:30,715] keeping 2000000 tokens which were in no less than 0 and no more than 3480000 (=100.0%) documents\n",
      "[2022-09-12 06:51:34,370] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:51:34,438] adding document #3480000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:51:59,521] discarding 44679 tokens: [('drouhot', 1), ('gaunace', 1), ('lorgnet', 1), ('tchimbembe', 1), ('skyreading', 1), ('tercerías', 1), ('dentelaire', 1), ('garence', 1), ('héliotrope', 1), ('macjonc', 1)]...\n",
      "[2022-09-12 06:51:59,523] keeping 2000000 tokens which were in no less than 0 and no more than 3490000 (=100.0%) documents\n",
      "[2022-09-12 06:52:04,387] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:52:04,489] adding document #3490000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:52:28,858] discarding 30736 tokens: [('zinoviyovych', 1), ('barrelworks', 1), ('fidence', 1), ('grull', 1), ('klaptraps', 1), ('koffmantimothy', 1), ('kutlass', 1), ('orangutango', 1), ('quered', 1), ('skurvy', 1)]...\n",
      "[2022-09-12 06:52:28,860] keeping 2000000 tokens which were in no less than 0 and no more than 3500000 (=100.0%) documents\n",
      "[2022-09-12 06:52:33,675] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:52:33,773] adding document #3500000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:52:57,011] discarding 30549 tokens: [('crandover', 1), ('lardendie', 1), ('larendie', 1), ('larondie', 1), ('skagg', 1), ('thruout', 1), ('eòghain', 1), ('sliochd', 1), ('frodesen', 1), ('gartnere', 1)]...\n",
      "[2022-09-12 06:52:57,012] keeping 2000000 tokens which were in no less than 0 and no more than 3510000 (=100.0%) documents\n",
      "[2022-09-12 06:53:01,856] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:53:01,956] adding document #3510000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:53:27,086] discarding 33948 tokens: [('ghadikolahi', 1), ('herijan', 1), ('heshtel', 1), ('javarom', 1), ('kalbadi', 1), ('kashpel', 1), ('kaykavoos', 1), ('khwarzamshah', 1), ('kowsan', 1), ('lavij', 1)]...\n",
      "[2022-09-12 06:53:27,088] keeping 2000000 tokens which were in no less than 0 and no more than 3520000 (=100.0%) documents\n",
      "[2022-09-12 06:53:31,964] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:53:32,064] adding document #3520000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:53:55,661] discarding 30705 tokens: [('ehtnographical', 1), ('polyvocal', 1), ('proccalimed', 1), ('confora', 1), ('courtrey', 1), ('wylackie', 1), ('albasham', 1), ('alkhazam', 1), ('alshath', 1), ('asulamy', 1)]...\n",
      "[2022-09-12 06:53:55,662] keeping 2000000 tokens which were in no less than 0 and no more than 3530000 (=100.0%) documents\n",
      "[2022-09-12 06:54:00,521] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:54:00,619] adding document #3530000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:54:22,974] discarding 31886 tokens: [('dekechilam', 1), ('kakhon', 1), ('kapileswari', 1), ('kedeche', 1), ('mafin', 1), ('nithur', 1), ('sandipta', 1), ('shankhachil', 1), ('tupur', 1), ('dipsalut', 1)]...\n",
      "[2022-09-12 06:54:22,976] keeping 2000000 tokens which were in no less than 0 and no more than 3540000 (=100.0%) documents\n",
      "[2022-09-12 06:54:26,428] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:54:26,495] adding document #3540000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:54:49,228] discarding 29702 tokens: [('siddiki', 1), ('yunkanjini', 1), ('raitel', 1), ('righttel', 1), ('marishal', 1), ('dailysportscar', 1), ('racesports', 1), ('tuscanr', 1), ('adefisayo', 1), ('akinbile', 1)]...\n",
      "[2022-09-12 06:54:49,230] keeping 2000000 tokens which were in no less than 0 and no more than 3550000 (=100.0%) documents\n",
      "[2022-09-12 06:54:54,029] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:54:54,127] adding document #3550000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:55:18,983] discarding 31982 tokens: [('eaglesport', 1), ('eurocellulari', 1), ('lilije', 1), ('orldrini', 1), ('palatrieste', 1), ('pavello', 1), ('snadeiro', 1), ('corevan', 1), ('francal', 1), ('pulvereitzer', 1)]...\n",
      "[2022-09-12 06:55:18,985] keeping 2000000 tokens which were in no less than 0 and no more than 3560000 (=100.0%) documents\n",
      "[2022-09-12 06:55:22,398] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:55:22,465] adding document #3560000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:55:47,582] discarding 32737 tokens: [('habitfruit', 1), ('謝德泰', 1), ('altawail', 1), ('sangsefidi', 1), ('youlley', 1), ('amoye', 1), ('archale', 1), ('banglabandh', 1), ('bhedetar', 1), ('chardham', 1)]...\n",
      "[2022-09-12 06:55:47,584] keeping 2000000 tokens which were in no less than 0 and no more than 3570000 (=100.0%) documents\n",
      "[2022-09-12 06:55:52,410] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:55:52,509] adding document #3570000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:56:16,327] discarding 27973 tokens: [('aguibou', 1), ('panathiniakos', 1), ('thriumphic', 1), ('aitebaar', 1), ('banjaara', 1), ('bazain', 1), ('chahatain', 1), ('chahey', 1), ('chalawa', 1), ('chatharay', 1)]...\n",
      "[2022-09-12 06:56:16,329] keeping 2000000 tokens which were in no less than 0 and no more than 3580000 (=100.0%) documents\n",
      "[2022-09-12 06:56:19,766] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:56:19,833] adding document #3580000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:56:45,503] discarding 29948 tokens: [('xingyong', 1), ('chisoro', 1), ('masvaure', 1), ('artschool', 1), ('barbiersbrug', 1), ('israelskade', 1), ('overhoeks', 1), ('terop', 1), ('catronio', 1), ('hindprints', 1)]...\n",
      "[2022-09-12 06:56:45,505] keeping 2000000 tokens which were in no less than 0 and no more than 3590000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 06:56:48,977] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:56:49,045] adding document #3590000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:57:12,597] discarding 30024 tokens: [('metabarcoding', 1), ('newphytologist', 1), ('subenvironments', 1), ('tanace', 1), ('atroolivaceus', 1), ('thiazonyl', 1), ('paulettejordan', 1), ('vandermaas', 1), ('linudden', 1), ('bötet', 1)]...\n",
      "[2022-09-12 06:57:12,599] keeping 2000000 tokens which were in no less than 0 and no more than 3600000 (=100.0%) documents\n",
      "[2022-09-12 06:57:16,073] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:57:16,141] adding document #3600000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:57:39,248] discarding 33930 tokens: [('underwilen', 1), ('denormalisation', 1), ('mcedar', 1), ('netbust', 1), ('northwoodpentium', 1), ('northwoodprescott', 1), ('northwoodprescottprescott', 1), ('preshot', 1), ('willamettenorthwoodprescott', 1), ('xeprescott', 1)]...\n",
      "[2022-09-12 06:57:39,250] keeping 2000000 tokens which were in no less than 0 and no more than 3610000 (=100.0%) documents\n",
      "[2022-09-12 06:57:44,145] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:57:44,245] adding document #3610000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:58:07,829] discarding 27674 tokens: [('fdba', 1), ('atınç', 1), ('bayrampaşaspor', 1), ('eslem', 1), ('fleetcorp', 1), ('karaal', 1), ('kurumuş', 1), ('küçükköylü', 1), ('majorworx', 1), ('mogaz', 1)]...\n",
      "[2022-09-12 06:58:07,831] keeping 2000000 tokens which were in no less than 0 and no more than 3620000 (=100.0%) documents\n",
      "[2022-09-12 06:58:11,295] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:58:11,361] adding document #3620000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:58:33,194] discarding 27551 tokens: [('moviegranadanominated', 1), ('plesiosynchronously', 1), ('gurdjeiff', 1), ('phrères', 1), ('rugafiori', 1), ('vosteen', 1), ('icelock', 1), ('natlock', 1), ('anthographers', 1), ('clarificators', 1)]...\n",
      "[2022-09-12 06:58:33,196] keeping 2000000 tokens which were in no less than 0 and no more than 3630000 (=100.0%) documents\n",
      "[2022-09-12 06:58:36,663] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:58:36,729] adding document #3630000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:58:59,385] discarding 29693 tokens: [('cupramonium', 1), ('delustering', 1), ('lycocell', 1), ('polynosic', 1), ('polynosics', 1), ('rayon_closeup_', 1), ('genderology', 1), ('khaniths', 1), ('transfemmebutch', 1), ('nusugbu', 1)]...\n",
      "[2022-09-12 06:58:59,386] keeping 2000000 tokens which were in no less than 0 and no more than 3640000 (=100.0%) documents\n",
      "[2022-09-12 06:59:04,268] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:59:04,369] adding document #3640000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:59:24,865] discarding 28063 tokens: [('wowfabgroovy', 1), ('maitaʻi', 1), ('asfrians', 1), ('mashyno', 1), ('robophilia', 1), ('technosexuals', 1), ('partnas', 1), ('timorembassy', 1), ('puloski', 1), ('willgues', 1)]...\n",
      "[2022-09-12 06:59:24,866] keeping 2000000 tokens which were in no less than 0 and no more than 3650000 (=100.0%) documents\n",
      "[2022-09-12 06:59:28,278] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:59:28,345] adding document #3650000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:59:53,250] discarding 35046 tokens: [('κλούσιον', 1), ('κονσίβιον', 1), ('κονσαιον', 1), ('κουριάτιον', 1), ('κυρινον', 1), ('οδαιον', 1), ('πατούλκιον', 1), ('πατρίκιον', 1), ('ποπάνων', 1), ('cassinix', 1)]...\n",
      "[2022-09-12 06:59:53,252] keeping 2000000 tokens which were in no less than 0 and no more than 3660000 (=100.0%) documents\n",
      "[2022-09-12 06:59:58,083] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 06:59:58,184] adding document #3660000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:00:22,209] discarding 30078 tokens: [('trzemeski', 1), ('żuławskiego', 1), ('гумплович', 1), ('البولندى', 1), ('الرياضيات', 1), ('بناخ', 1), ('ستيفان', 1), ('hoʻōpūloa', 1), ('mokuʻāweoweo', 1), ('ʻāweoweo', 1)]...\n",
      "[2022-09-12 07:00:22,211] keeping 2000000 tokens which were in no less than 0 and no more than 3670000 (=100.0%) documents\n",
      "[2022-09-12 07:00:25,710] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:00:25,778] adding document #3670000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:00:49,360] discarding 28246 tokens: [('fzpzc', 1), ('gedxx', 1), ('grbpfae', 1), ('haveb', 1), ('idges', 1), ('imtrq', 1), ('iomee', 1), ('izrrq', 1), ('izrvk', 1), ('jmsdz', 1)]...\n",
      "[2022-09-12 07:00:49,362] keeping 2000000 tokens which were in no less than 0 and no more than 3680000 (=100.0%) documents\n",
      "[2022-09-12 07:00:52,782] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:00:52,849] adding document #3680000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:01:15,074] discarding 30167 tokens: [('oldspelling', 1), ('rasional', 1), ('sprachreform', 1), ('blomfieldianus', 1), ('peristellein', 1), ('abroaded', 1), ('delannie', 1), ('guererri', 1), ('electroarteriography', 1), ('electroatriography', 1)]...\n",
      "[2022-09-12 07:01:15,076] keeping 2000000 tokens which were in no less than 0 and no more than 3690000 (=100.0%) documents\n",
      "[2022-09-12 07:01:18,554] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:01:18,622] adding document #3690000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:01:41,383] discarding 31031 tokens: [('centonized', 1), ('mollum', 1), ('psalmverses', 1), ('ecocultivation', 1), ('känan', 1), ('ardchieftain', 1), ('baylers', 1), ('eriú', 1), ('dönszelmann', 1), ('batzarov', 1)]...\n",
      "[2022-09-12 07:01:41,384] keeping 2000000 tokens which were in no less than 0 and no more than 3700000 (=100.0%) documents\n",
      "[2022-09-12 07:01:45,921] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:01:46,020] adding document #3700000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:02:10,438] discarding 31316 tokens: [('curitibabrasil', 1), ('dhelsinki', 1), ('operadearame', 1), ('operniy', 1), ('scalanotte', 1), ('wireoperahouse', 1), ('ffradsam', 1), ('ffradsham', 1), ('frandsham', 1), ('frodisham', 1)]...\n",
      "[2022-09-12 07:02:10,440] keeping 2000000 tokens which were in no less than 0 and no more than 3710000 (=100.0%) documents\n",
      "[2022-09-12 07:02:15,305] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:02:15,404] adding document #3710000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:02:41,361] discarding 32182 tokens: [('eolidia', 1), ('curriculams', 1), ('kuppelgrab', 1), ('tergast', 1), ('tutoress', 1), ('waitingmaid', 1), ('atterbrigg', 1), ('bevyll', 1), ('camelton', 1), ('carlyll', 1)]...\n",
      "[2022-09-12 07:02:41,363] keeping 2000000 tokens which were in no less than 0 and no more than 3720000 (=100.0%) documents\n",
      "[2022-09-12 07:02:44,830] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:02:44,898] adding document #3720000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:03:09,983] discarding 33409 tokens: [('bardowell', 1), ('bstars', 1), ('buille', 1), ('coayo', 1), ('deadpoetsociety', 1), ('deadpoetsoecity', 1), ('dennee', 1), ('dubhead', 1), ('dunyakan', 1), ('duraluxe', 1)]...\n",
      "[2022-09-12 07:03:09,984] keeping 2000000 tokens which were in no less than 0 and no more than 3730000 (=100.0%) documents\n",
      "[2022-09-12 07:03:14,829] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:03:14,928] adding document #3730000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:03:38,991] discarding 32462 tokens: [('beihuan', 1), ('深圳滨海大道', 1), ('滨海大道', 1), ('barbenson', 1), ('blackbeans', 1), ('breakfronts', 1), ('tadec', 1), ('toplights', 1), ('papillaire', 1), ('printanière', 1)]...\n",
      "[2022-09-12 07:03:38,993] keeping 2000000 tokens which were in no less than 0 and no more than 3740000 (=100.0%) documents\n",
      "[2022-09-12 07:03:42,447] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:03:42,515] adding document #3740000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:04:05,270] discarding 36238 tokens: [('roshak', 1), ('torsoed', 1), ('yongjae', 1), ('corthron', 1), ('schankman', 1), ('berthame', 1), ('kensy', 1), ('weeka', 1), ('wuiswell', 1), ('skagg', 1)]...\n",
      "[2022-09-12 07:04:05,272] keeping 2000000 tokens which were in no less than 0 and no more than 3750000 (=100.0%) documents\n",
      "[2022-09-12 07:04:08,806] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:04:08,876] adding document #3750000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:04:39,215] discarding 30452 tokens: [('chlodna', 1), ('prezerwatywy', 1), ('szum', 1), ('waggerman', 1), ('overpop', 1), ('hawlsey', 1), ('nydiver', 1), ('slissate', 1), ('tenaja', 1), ('thalehaha', 1)]...\n",
      "[2022-09-12 07:04:39,217] keeping 2000000 tokens which were in no less than 0 and no more than 3760000 (=100.0%) documents\n",
      "[2022-09-12 07:04:42,860] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:04:42,929] adding document #3760000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:05:05,927] discarding 29824 tokens: [('roantree', 1), ('ketrick', 1), ('zeimer', 1), ('dantreume', 1), ('yancich', 1), ('beniko', 1), ('recine', 1), ('abercych', 1), ('treseli', 1), ('artefactory', 1)]...\n",
      "[2022-09-12 07:05:05,929] keeping 2000000 tokens which were in no less than 0 and no more than 3770000 (=100.0%) documents\n",
      "[2022-09-12 07:05:09,439] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:05:09,507] adding document #3770000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:05:34,698] discarding 31424 tokens: [('garinati', 1), ('juard', 1), ('baviana', 1), ('cafreriana', 1), ('marksi', 1), ('htudies', 1), ('sakiestewa', 1), ('hazeleye', 1), ('lonidaw', 1), ('olondaw', 1)]...\n",
      "[2022-09-12 07:05:34,699] keeping 2000000 tokens which were in no less than 0 and no more than 3780000 (=100.0%) documents\n",
      "[2022-09-12 07:05:39,447] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:05:39,548] adding document #3780000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:06:00,210] discarding 27990 tokens: [('liscourt', 1), ('barleet', 1), ('amabele', 1), ('amagogotya', 1), ('amahleke', 1), ('amalinde', 1), ('amandlambe', 1), ('amandungwana', 1), ('amantinde', 1), ('amaqwathi', 1)]...\n",
      "[2022-09-12 07:06:00,212] keeping 2000000 tokens which were in no less than 0 and no more than 3790000 (=100.0%) documents\n",
      "[2022-09-12 07:06:03,696] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:06:03,783] adding document #3790000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:06:26,273] discarding 33908 tokens: [('hakaritai', 1), ('hiruobi', 1), ('hōshanō', 1), ('jōshiki', 1), ('kankintōbō', 1), ('midarana', 1), ('puchisuto', 1), ('shouin', 1), ('sodatete', 1), ('marquois', 1)]...\n",
      "[2022-09-12 07:06:26,276] keeping 2000000 tokens which were in no less than 0 and no more than 3800000 (=100.0%) documents\n",
      "[2022-09-12 07:06:31,489] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:06:31,590] adding document #3800000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:06:59,243] discarding 35611 tokens: [('delaplin', 1), ('gourbeyrienne', 1), ('pontrémy', 1), ('kochambalam', 1), ('tazhmon', 1), ('valiyambalam', 1), ('sherily', 1), ('arancioni', 1), ('carmelitana', 1), ('pettorine', 1)]...\n",
      "[2022-09-12 07:06:59,245] keeping 2000000 tokens which were in no less than 0 and no more than 3810000 (=100.0%) documents\n",
      "[2022-09-12 07:07:04,139] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:07:04,241] adding document #3810000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:07:26,768] discarding 33581 tokens: [('isifiya', 1), ('gisebo', 1), ('umbla', 1), ('ajaxi', 1), ('aleviku', 1), ('annelinna', 1), ('castovanni', 1), ('depoo', 1), ('emü', 1), ('ganvix', 1)]...\n",
      "[2022-09-12 07:07:26,769] keeping 2000000 tokens which were in no less than 0 and no more than 3820000 (=100.0%) documents\n",
      "[2022-09-12 07:07:30,494] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:07:30,563] adding document #3820000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:07:53,295] discarding 33924 tokens: [('sajeeva', 1), ('atoomwijk', 1), ('tandaura', 1), ('umrewal', 1), ('azhgaliyev', 1), ('snellink', 1), ('locarnese', 1), ('sikanianum', 1), ('sirenites', 1), ('rhodesfield', 1)]...\n",
      "[2022-09-12 07:07:53,296] keeping 2000000 tokens which were in no less than 0 and no more than 3830000 (=100.0%) documents\n",
      "[2022-09-12 07:07:58,161] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:07:58,262] adding document #3830000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:08:20,295] discarding 29639 tokens: [('ibcb', 1), ('dreamband', 1), ('dzjes', 1), ('jazzhattan', 1), ('noslen', 1), ('nostalia', 1), ('uppababy', 1), ('unnext', 1), ('kwoi', 1), ('seifallah', 1)]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:08:20,297] keeping 2000000 tokens which were in no less than 0 and no more than 3840000 (=100.0%) documents\n",
      "[2022-09-12 07:08:23,763] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:08:23,830] adding document #3840000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:08:48,538] discarding 31085 tokens: [('balbí', 1), ('botolotti', 1), ('hohwy', 1), ('abanyambo', 1), ('bacwezi', 1), ('banyambo', 1), ('njunaki', 1), ('nyambo', 1), ('rutara', 1), ('mathalene', 1)]...\n",
      "[2022-09-12 07:08:48,539] keeping 2000000 tokens which were in no less than 0 and no more than 3850000 (=100.0%) documents\n",
      "[2022-09-12 07:08:52,713] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:08:52,780] adding document #3850000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:09:16,417] discarding 31635 tokens: [('caschielawes', 1), ('crewallie', 1), ('pacock', 1), ('pinniewinkles', 1), ('schamefullie', 1), ('taillifeir', 1), ('thumbscrew#1', 1), ('unmercifullie', 1), ('yosie', 1), ('balețchi', 1)]...\n",
      "[2022-09-12 07:09:16,419] keeping 2000000 tokens which were in no less than 0 and no more than 3860000 (=100.0%) documents\n",
      "[2022-09-12 07:09:21,317] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:09:21,417] adding document #3860000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:09:45,760] discarding 30650 tokens: [('lucoque', 1), ('manahsiddhikari', 1), ('morajhari', 1), ('panchapura', 1), ('panjaur', 1), ('pethad', 1), ('suhavadevi', 1), ('altonia', 1), ('baguely', 1), ('kaphi', 1)]...\n",
      "[2022-09-12 07:09:45,762] keeping 2000000 tokens which were in no less than 0 and no more than 3870000 (=100.0%) documents\n",
      "[2022-09-12 07:09:49,219] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:09:49,287] adding document #3870000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:10:14,137] discarding 31889 tokens: [('thünefeld', 1), ('kwarantined', 1), ('nák', 1), ('usuns', 1), ('yuedzhi', 1), ('atbenaeum', 1), ('laidies', 1), ('orderd', 1), ('renderd', 1), ('gasore', 1)]...\n",
      "[2022-09-12 07:10:14,139] keeping 2000000 tokens which were in no less than 0 and no more than 3880000 (=100.0%) documents\n",
      "[2022-09-12 07:10:19,048] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:10:19,149] adding document #3880000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:10:44,530] discarding 33617 tokens: [('murcadh', 1), ('nebedza', 1), ('ngoupou', 1), ('nugandaširi', 1), ('odhe', 1), ('oluaso', 1), ('oquiztzin', 1), ('permuka', 1), ('qanšin', 1), ('ramathipatei', 1)]...\n",
      "[2022-09-12 07:10:44,531] keeping 2000000 tokens which were in no less than 0 and no more than 3890000 (=100.0%) documents\n",
      "[2022-09-12 07:10:47,999] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:10:48,067] adding document #3890000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:11:11,419] discarding 31452 tokens: [('ccvpd', 1), ('shaneea', 1), ('drumeldra', 1), ('drumeldry', 1), ('laxbenny', 1), ('clagettsville', 1), ('pisquerias', 1), ('areaalt', 1), ('buildingalt', 1), ('churchalt', 1)]...\n",
      "[2022-09-12 07:11:11,422] keeping 2000000 tokens which were in no less than 0 and no more than 3900000 (=100.0%) documents\n",
      "[2022-09-12 07:11:16,290] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:11:16,391] adding document #3900000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:11:41,363] discarding 32493 tokens: [('爱民好治', 1), ('状古述令', 1), ('献敏成行', 1), ('率义死国', 1), ('理顺习善', 1), ('由義而濟', 1), ('甲胄有劳', 1), ('疏远继位', 1), ('皜', 1), ('睦于兄弟', 1)]...\n",
      "[2022-09-12 07:11:41,364] keeping 2000000 tokens which were in no less than 0 and no more than 3910000 (=100.0%) documents\n",
      "[2022-09-12 07:11:46,721] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:11:46,824] adding document #3910000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:12:10,969] discarding 32903 tokens: [('ǂaʔhaǂ', 1), ('stanleychurch', 1), ('stanleymuse', 1), ('stanleypost', 1), ('ritewood', 1), ('emmettville', 1), ('idhsaa', 1), ('hagermanidaho', 1), ('rigginsidaho', 1), ('alenefrom', 1)]...\n",
      "[2022-09-12 07:12:10,971] keeping 2000000 tokens which were in no less than 0 and no more than 3920000 (=100.0%) documents\n",
      "[2022-09-12 07:12:15,881] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:12:15,982] adding document #3920000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:12:42,529] discarding 31477 tokens: [('sevianno', 1), ('covertsburg', 1), ('kingberry', 1), ('sadoian', 1), ('ducorschool', 1), ('svenhard', 1), ('rosaena', 1), ('yandanchei', 1), ('paratranit', 1), ('occupiedand', 1)]...\n",
      "[2022-09-12 07:12:42,530] keeping 2000000 tokens which were in no less than 0 and no more than 3930000 (=100.0%) documents\n",
      "[2022-09-12 07:12:47,439] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:12:47,539] adding document #3930000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:13:11,855] discarding 32000 tokens: [('zharon', 1), ('blancafor', 1), ('mantyjarvi', 1), ('panorame', 1), ('soliloquis', 1), ('battaler', 1), ('chlusowitz', 1), ('consecuence', 1), ('devekioed', 1), ('llaut', 1)]...\n",
      "[2022-09-12 07:13:11,857] keeping 2000000 tokens which were in no less than 0 and no more than 3940000 (=100.0%) documents\n",
      "[2022-09-12 07:13:16,720] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:13:16,821] adding document #3940000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:13:40,324] discarding 30004 tokens: [('zǎozhuāng', 1), ('五经', 1), ('曲阜孔廟大成殿', 1), ('bouillerce', 1), ('collègi', 1), ('juricon', 1), ('marancy', 1), ('paü', 1), ('pyreneeism', 1), ('groetzingen', 1)]...\n",
      "[2022-09-12 07:13:40,326] keeping 2000000 tokens which were in no less than 0 and no more than 3950000 (=100.0%) documents\n",
      "[2022-09-12 07:13:43,896] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:13:43,965] adding document #3950000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:14:09,308] discarding 34971 tokens: [('kirgut', 1), ('kirkur', 1), ('kirkün', 1), ('krêk', 1), ('qïrqïz', 1), ('qïrqïŕ', 1), ('ěrkèzī', 1), ('ɣuən', 1), ('tugarraf', 1), ('deejah', 1)]...\n",
      "[2022-09-12 07:14:09,311] keeping 2000000 tokens which were in no less than 0 and no more than 3960000 (=100.0%) documents\n",
      "[2022-09-12 07:14:14,147] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:14:14,249] adding document #3960000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:14:38,149] discarding 29084 tokens: [('kuiperspoort', 1), ('mzvc', 1), ('rhynsault', 1), ('varkenspoort', 1), ('walensingel', 1), ('jarltex', 1), ('kirkvoe', 1), ('kirkwaa', 1), ('aerling', 1), ('lengri', 1)]...\n",
      "[2022-09-12 07:14:38,151] keeping 2000000 tokens which were in no less than 0 and no more than 3970000 (=100.0%) documents\n",
      "[2022-09-12 07:14:41,664] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:14:41,732] adding document #3970000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:15:06,305] discarding 31364 tokens: [('teshet', 1), ('hovetonion', 1), ('carbjal', 1), ('gilzanean', 1), ('keisarinnankivihelsinginkauppatorilla', 1), ('trasportatione', 1), ('androblastomas', 1), ('aldebrandin', 1), ('gailde', 1), ('diuspurgau', 1)]...\n",
      "[2022-09-12 07:15:06,306] keeping 2000000 tokens which were in no less than 0 and no more than 3980000 (=100.0%) documents\n",
      "[2022-09-12 07:15:11,159] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:15:11,260] adding document #3980000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:15:37,451] discarding 31801 tokens: [('ubumbano', 1), ('cvmq', 1), ('igif', 1), ('llpq', 1), ('barrettproducer', 1), ('holyszproducer', 1), ('porterproducer', 1), ('sarnaformer', 1), ('shermanco', 1), ('angkarn', 1)]...\n",
      "[2022-09-12 07:15:37,453] keeping 2000000 tokens which were in no less than 0 and no more than 3990000 (=100.0%) documents\n",
      "[2022-09-12 07:15:42,322] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:15:42,423] adding document #3990000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:16:08,173] discarding 32337 tokens: [('jomfrukloster', 1), ('skørringe', 1), ('accounteer', 1), ('afrilearn', 1), ('agriple', 1), ('codebusafrica', 1), ('digilearns', 1), ('drinkup', 1), ('gleeworld', 1), ('inovo', 1)]...\n",
      "[2022-09-12 07:16:08,175] keeping 2000000 tokens which were in no less than 0 and no more than 4000000 (=100.0%) documents\n",
      "[2022-09-12 07:16:13,082] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:16:13,182] adding document #4000000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:16:37,948] discarding 32474 tokens: [('šidák', 1), ('louisvillemusic', 1), ('whysper', 1), ('ezhaigalin', 1), ('geethathile', 1), ('inimaiyaana', 1), ('kotvankar', 1), ('maattukkulle', 1), ('meikaadhalil', 1), ('povaayi', 1)]...\n",
      "[2022-09-12 07:16:37,950] keeping 2000000 tokens which were in no less than 0 and no more than 4010000 (=100.0%) documents\n",
      "[2022-09-12 07:16:42,827] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:16:42,928] adding document #4010000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:17:06,960] discarding 33389 tokens: [('goveexpanse', 1), ('knauston', 1), ('elliscountryside', 1), ('ellishomestead', 1), ('ellislimestone', 1), ('ellisroad', 1), ('ellissunset', 1), ('elliswindmill', 1), ('derusseau', 1), ('cockerall', 1)]...\n",
      "[2022-09-12 07:17:06,961] keeping 2000000 tokens which were in no less than 0 and no more than 4020000 (=100.0%) documents\n",
      "[2022-09-12 07:17:11,830] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:17:11,933] adding document #4020000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:17:37,194] discarding 35388 tokens: [('googobits', 1), ('eloquentist', 1), ('kwatkōṭ', 1), ('nauoroz', 1), ('vfiz', 1), ('vsky', 1), ('biosfix', 1), ('makesfx', 1), ('pkcfg', 1), ('pkunzjr', 1)]...\n",
      "[2022-09-12 07:17:37,195] keeping 2000000 tokens which were in no less than 0 and no more than 4030000 (=100.0%) documents\n",
      "[2022-09-12 07:17:40,730] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:17:40,798] adding document #4030000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:18:07,237] discarding 33683 tokens: [('suntsa', 1), ('suntsu', 1), ('suuts', 1), ('suutta', 1), ('suːm', 1), ('suːns', 1), ('suːts', 1), ('sínt', 1), ('sútsʰ', 1), ('súuntsi', 1)]...\n",
      "[2022-09-12 07:18:07,239] keeping 2000000 tokens which were in no less than 0 and no more than 4040000 (=100.0%) documents\n",
      "[2022-09-12 07:18:10,741] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:18:10,809] adding document #4040000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:18:41,055] discarding 50385 tokens: [('vespeira', 1), ('austiin', 1), ('cccte', 1), ('chilsholm', 1), ('burgamoo', 1), ('ozgo', 1), ('excelsius', 1), ('ineficaz', 1), ('taulangh', 1), ('lacin', 1)]...\n",
      "[2022-09-12 07:18:41,057] keeping 2000000 tokens which were in no less than 0 and no more than 4050000 (=100.0%) documents\n",
      "[2022-09-12 07:18:44,582] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:18:44,653] adding document #4050000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:19:11,569] discarding 31537 tokens: [('darkscorch', 1), ('satanial', 1), ('sonaura', 1), ('heeresarchiv', 1), ('priestdorff', 1), ('seyfart', 1), ('wülfingen', 1), ('devmurti', 1), ('kolasla', 1), ('ramkaran', 1)]...\n",
      "[2022-09-12 07:19:11,571] keeping 2000000 tokens which were in no less than 0 and no more than 4060000 (=100.0%) documents\n",
      "[2022-09-12 07:19:16,466] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:19:16,567] adding document #4060000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:19:39,774] discarding 30098 tokens: [('mauai', 1), ('傳聞', 1), ('報告總司令', 1), ('小白', 1), ('數碼超能量', 1), ('背后女人', 1), ('自由意志', 1), ('錯配', 1), ('usaw', 1), ('giannicolò', 1)]...\n",
      "[2022-09-12 07:19:39,776] keeping 2000000 tokens which were in no less than 0 and no more than 4070000 (=100.0%) documents\n",
      "[2022-09-12 07:19:44,569] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:19:44,671] adding document #4070000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:20:08,925] discarding 31599 tokens: [('barbhaya', 1), ('banchang', 1), ('gandarrapiddo', 1), ('summerspring', 1), ('pakgen', 1), ('sedransk', 1), ('slowakia', 1), ('namanadi', 1), ('karambola', 1), ('trillianes', 1)]...\n",
      "[2022-09-12 07:20:08,927] keeping 2000000 tokens which were in no less than 0 and no more than 4080000 (=100.0%) documents\n",
      "[2022-09-12 07:20:14,096] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:20:14,197] adding document #4080000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:20:40,556] discarding 39767 tokens: [('séchen', 1), ('taurique', 1), ('hartkernmunition', 1), ('cupfire', 1), ('depriming', 1), ('hintereis', 1), ('murtarol', 1), ('splugenpass', 1), ('boscani', 1), ('châtean', 1)]...\n",
      "[2022-09-12 07:20:40,560] keeping 2000000 tokens which were in no less than 0 and no more than 4090000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:20:44,470] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:20:44,539] adding document #4090000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:21:07,962] discarding 33321 tokens: [('gerymbas', 1), ('oarthus', 1), ('oryithus', 1), ('aellope', 1), ('aellō', 1), ('aellōpē', 1), ('nikothoē', 1), ('podargē', 1), ('podarkē', 1), ('αελλώπη', 1)]...\n",
      "[2022-09-12 07:21:07,964] keeping 2000000 tokens which were in no less than 0 and no more than 4100000 (=100.0%) documents\n",
      "[2022-09-12 07:21:11,778] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:21:11,879] adding document #4100000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:21:34,969] discarding 38442 tokens: [('ēzai', 1), ('ōdio', 1), ('ōtobakksu', 1), ('あおぞら銀行', 1), ('いすゞ自動車株式会社', 1), ('ぺんてる株式会社', 1), ('みずほ信託銀行株式会社', 1), ('みずほ情報総研株式会社', 1), ('アイコム株式会社', 1), ('アイシン精機株式会社', 1)]...\n",
      "[2022-09-12 07:21:34,971] keeping 2000000 tokens which were in no less than 0 and no more than 4110000 (=100.0%) documents\n",
      "[2022-09-12 07:21:39,858] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:21:39,960] adding document #4110000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:22:04,561] discarding 32281 tokens: [('hōkojōki', 1), ('imperfectable', 1), ('kenzeiki', 1), ('nyōjo', 1), ('níngbō', 1), ('shiryoken', 1), ('takushū', 1), ('tendōzan', 1), ('tiāntóngshān', 1), ('仏性伝東国師', 1)]...\n",
      "[2022-09-12 07:22:04,563] keeping 2000000 tokens which were in no less than 0 and no more than 4120000 (=100.0%) documents\n",
      "[2022-09-12 07:22:08,027] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:22:08,096] adding document #4120000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:22:31,758] discarding 37116 tokens: [('skiko', 1), ('temoatan', 1), ('temotan', 1), ('weapemeocs', 1), ('balaustine', 1), ('granatapfelblüte', 1), ('ikonostasi', 1), ('jnhm', 1), ('kollivozoumi', 1), ('pōmum', 1)]...\n",
      "[2022-09-12 07:22:31,760] keeping 2000000 tokens which were in no less than 0 and no more than 4130000 (=100.0%) documents\n",
      "[2022-09-12 07:22:36,689] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:22:36,791] adding document #4130000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:23:00,544] discarding 34922 tokens: [('houegbdaja', 1), ('wegbaja', 1), ('audati', 1), ('yevogan', 1), ('akingjogbin', 1), ('fraku', 1), ('wanjile', 1), ('bacqueryrisse', 1), ('adondozan', 1), ('agongolo', 1)]...\n",
      "[2022-09-12 07:23:00,545] keeping 2000000 tokens which were in no less than 0 and no more than 4140000 (=100.0%) documents\n",
      "[2022-09-12 07:23:05,405] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:23:05,507] adding document #4140000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:23:31,247] discarding 50225 tokens: [('redearsider', 1), ('mynewpalm', 1), ('nonperspectival', 1), ('pitech', 1), ('quarkdds', 1), ('quarked', 1), ('shapemaker', 1), ('brueckels', 1), ('dreamlly', 1), ('vpjk', 1)]...\n",
      "[2022-09-12 07:23:31,249] keeping 2000000 tokens which were in no less than 0 and no more than 4150000 (=100.0%) documents\n",
      "[2022-09-12 07:23:34,800] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:23:34,871] adding document #4150000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:23:59,829] discarding 41119 tokens: [('bashkatov', 1), ('bogatchova', 1), ('desilvia', 1), ('elzermann', 1), ('fswc', 1), ('goldenbook', 1), ('goldsmithg', 1), ('jähnichen', 1), ('kotriaga', 1), ('krisitn', 1)]...\n",
      "[2022-09-12 07:23:59,831] keeping 2000000 tokens which were in no less than 0 and no more than 4160000 (=100.0%) documents\n",
      "[2022-09-12 07:24:03,921] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:24:03,990] adding document #4160000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:24:31,729] discarding 32025 tokens: [('pekarksi', 1), ('pekarsi', 1), ('аlternativa', 1), ('haidamkas', 1), ('hervasiy', 1), ('hevrasiy', 1), ('kostysh', 1), ('lyntsevsky', 1), ('melkhisedek', 1), ('motronyn', 1)]...\n",
      "[2022-09-12 07:24:31,730] keeping 2000000 tokens which were in no less than 0 and no more than 4170000 (=100.0%) documents\n",
      "[2022-09-12 07:24:36,091] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:24:36,194] adding document #4170000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:25:03,082] discarding 32683 tokens: [('grimpoteuthidae', 1), ('cuplés', 1), ('microfónica', 1), ('alpatych', 1), ('nevinnye', 1), ('невинные', 1), ('скорая', 1), ('kanjila', 1), ('mpatu', 1), ('arellana', 1)]...\n",
      "[2022-09-12 07:25:03,084] keeping 2000000 tokens which were in no less than 0 and no more than 4180000 (=100.0%) documents\n",
      "[2022-09-12 07:25:08,056] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:25:08,158] adding document #4180000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:25:32,731] discarding 32310 tokens: [('ain_umm_sujoor_', 1), ('における特色ある教育活動', 1), ('サンホセ日本人学校', 1), ('サンホセ日本人学校における作文指導の工夫', 1), ('小規模校', 1), ('平塚市学校', 1), ('東京都調布市立富士見台小学校', 1), ('栄生', 1), ('私たちの学校では', 1), ('章教科指導', 1)]...\n",
      "[2022-09-12 07:25:32,732] keeping 2000000 tokens which were in no less than 0 and no more than 4190000 (=100.0%) documents\n",
      "[2022-09-12 07:25:36,206] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:25:36,274] adding document #4190000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:25:59,959] discarding 31414 tokens: [('hiyangthang', 1), ('lamkhai', 1), ('wabagai', 1), ('wrongspeak', 1), ('bjäresjö', 1), ('gierhi', 1), ('giardinelli', 1), ('mempo', 1), ('telecino', 1), ('juliandontcheff', 1)]...\n",
      "[2022-09-12 07:25:59,961] keeping 2000000 tokens which were in no less than 0 and no more than 4200000 (=100.0%) documents\n",
      "[2022-09-12 07:26:05,192] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:26:05,294] adding document #4200000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:26:29,148] discarding 32165 tokens: [('industrije', 1), ('informbirou', 1), ('ndustrija', 1), ('preseljenje', 1), ('ауто', 1), ('војне', 1), ('икаруса', 1), ('икарусу', 1), ('индустрије', 1), ('одумирање', 1)]...\n",
      "[2022-09-12 07:26:29,150] keeping 2000000 tokens which were in no less than 0 and no more than 4210000 (=100.0%) documents\n",
      "[2022-09-12 07:26:32,620] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:26:32,689] adding document #4210000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:26:58,375] discarding 34852 tokens: [('vinodsingh', 1), ('akademikas', 1), ('beliniche', 1), ('esbf', 1), ('mtsv', 1), ('neerbeek', 1), ('piotrkovia', 1), ('sarabon', 1), ('scbt', 1), ('tecnisa', 1)]...\n",
      "[2022-09-12 07:26:58,377] keeping 2000000 tokens which were in no less than 0 and no more than 4220000 (=100.0%) documents\n",
      "[2022-09-12 07:27:03,237] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:27:03,339] adding document #4220000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:27:31,545] discarding 26185 tokens: [('pūce', 1), ('atsgpmgwlpvfyrf', 1), ('conorfamide', 1), ('conorfamides', 1), ('farps', 1), ('fmrfa', 1), ('edelgasse', 1), ('jaskulsky', 1), ('witiska', 1), ('clericofascist', 1)]...\n",
      "[2022-09-12 07:27:31,546] keeping 2000000 tokens which were in no less than 0 and no more than 4230000 (=100.0%) documents\n",
      "[2022-09-12 07:27:35,145] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:27:35,214] adding document #4230000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:28:00,524] discarding 29817 tokens: [('abieca', 1), ('alalcol', 1), ('ekiega', 1), ('moowis', 1), ('muzzeniegun', 1), ('muzzinyegun', 1), ('cornbrooms', 1), ('civey', 1), ('fleisinger', 1), ('herrenbruch', 1)]...\n",
      "[2022-09-12 07:28:00,526] keeping 2000000 tokens which were in no less than 0 and no more than 4240000 (=100.0%) documents\n",
      "[2022-09-12 07:28:04,017] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:28:04,086] adding document #4240000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:28:36,761] discarding 31283 tokens: [('méndes', 1), ('achhedin', 1), ('bondas', 1), ('chitrakonda', 1), ('khuriguda', 1), ('ଡମ', 1), ('krasco', 1), ('bakaanu', 1), ('idealpolitik', 1), ('shiashie', 1)]...\n",
      "[2022-09-12 07:28:36,764] keeping 2000000 tokens which were in no less than 0 and no more than 4250000 (=100.0%) documents\n",
      "[2022-09-12 07:28:41,674] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:28:41,775] adding document #4250000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:29:11,440] discarding 32260 tokens: [('sertensis', 1), ('colecionador', 1), ('membher', 1), ('proession', 1), ('activizing', 1), ('дацькó', 1), ('vathsala', 1), ('dedective', 1), ('sulhi', 1), ('yolmen', 1)]...\n",
      "[2022-09-12 07:29:11,443] keeping 2000000 tokens which were in no less than 0 and no more than 4260000 (=100.0%) documents\n",
      "[2022-09-12 07:29:16,646] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:29:16,748] adding document #4260000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:29:42,076] discarding 28617 tokens: [('kalarathri', 1), ('keralaculture', 1), ('palakan', 1), ('shakteya', 1), ('udinur', 1), ('vairajathan', 1), ('aranguri', 1), ('berckemeyer', 1), ('berninzon', 1), ('caparó', 1)]...\n",
      "[2022-09-12 07:29:42,079] keeping 2000000 tokens which were in no less than 0 and no more than 4270000 (=100.0%) documents\n",
      "[2022-09-12 07:29:47,018] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:29:47,118] adding document #4270000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:30:17,632] discarding 29606 tokens: [('kdcy', 1), ('tilakwada', 1), ('acreditar', 1), ('atenção', 1), ('calcada', 1), ('delineamento', 1), ('eclesial', 1), ('existência', 1), ('fevreuary', 1), ('giselo', 1)]...\n",
      "[2022-09-12 07:30:17,634] keeping 2000000 tokens which were in no less than 0 and no more than 4280000 (=100.0%) documents\n",
      "[2022-09-12 07:30:22,382] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:30:22,484] adding document #4280000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:30:46,469] discarding 28182 tokens: [('obstructiontrolleybus', 1), ('quietnesstrolleybuses', 1), ('requireddrivers', 1), ('requiredtrolleybuses', 1), ('resistancerubber', 1), ('routewhen', 1), ('trainingthe', 1), ('trolleybusestrolleybuses', 1), ('waylanes', 1), ('chinesiske', 1)]...\n",
      "[2022-09-12 07:30:46,470] keeping 2000000 tokens which were in no less than 0 and no more than 4290000 (=100.0%) documents\n",
      "[2022-09-12 07:30:51,663] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:30:51,765] adding document #4290000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:31:16,381] discarding 27792 tokens: [('yḭ', 1), ('yẽna', 1), ('yẽnnɛ', 1), ('zelɛ', 1), ('zilemde', 1), ('zukukpo', 1), ('zyã', 1), ('zyẽ', 1), ('zɪlɪsɪ', 1), ('zɪlɪŋa', 1)]...\n",
      "[2022-09-12 07:31:16,382] keeping 2000000 tokens which were in no less than 0 and no more than 4300000 (=100.0%) documents\n",
      "[2022-09-12 07:31:19,898] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:31:19,966] adding document #4300000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:31:44,489] discarding 30807 tokens: [('plesioblattogryllus', 1), ('pritzkerprize', 1), ('retoria', 1), ('sangleem', 1), ('yurgeles', 1), ('atozoa', 1), ('pronymphs', 1), ('warriorfly', 1), ('chaterism', 1), ('aptenohyus', 1)]...\n",
      "[2022-09-12 07:31:44,490] keeping 2000000 tokens which were in no less than 0 and no more than 4310000 (=100.0%) documents\n",
      "[2022-09-12 07:31:48,049] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:31:48,119] adding document #4310000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:32:13,205] discarding 31833 tokens: [('snodrog', 1), ('videogems', 1), ('maneglier', 1), ('pernéty', 1), ('cenaine', 1), ('coppenole', 1), ('debaubien', 1), ('hoctement', 1), ('netascii', 1), ('antistrophos', 1)]...\n",
      "[2022-09-12 07:32:13,206] keeping 2000000 tokens which were in no less than 0 and no more than 4320000 (=100.0%) documents\n",
      "[2022-09-12 07:32:16,726] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:32:16,796] adding document #4320000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:32:42,215] discarding 31626 tokens: [('hyperellipse', 1), ('superelliptical', 1), ('ethicalness', 1), ('albeldensian', 1), ('asturibus', 1), ('nícer', 1), ('olalíes', 1), ('presbyterus', 1), ('rotensian', 1), ('sebastianensian', 1)]...\n",
      "[2022-09-12 07:32:42,217] keeping 2000000 tokens which were in no less than 0 and no more than 4330000 (=100.0%) documents\n",
      "[2022-09-12 07:32:47,131] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:32:47,233] adding document #4330000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:33:14,059] discarding 31051 tokens: [('neossoptiles', 1), ('pennibrachium', 1), ('teleoptiles', 1), ('majahilo', 1), ('mananonoka', 1), ('ngontsy', 1), ('roiniforme', 1), ('ankoala', 1), ('bejau', 1), ('cutmarked', 1)]...\n",
      "[2022-09-12 07:33:14,061] keeping 2000000 tokens which were in no less than 0 and no more than 4340000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:33:17,566] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:33:17,635] adding document #4340000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:33:42,653] discarding 31583 tokens: [('takkeja', 1), ('taloss', 1), ('tuemme', 1), ('ulkont', 1), ('voisitko', 1), ('voisitsä', 1), ('ähvä', 1), ('ähävä', 1), ('öylen', 1), ('adverbi', 1)]...\n",
      "[2022-09-12 07:33:42,655] keeping 2000000 tokens which were in no less than 0 and no more than 4350000 (=100.0%) documents\n",
      "[2022-09-12 07:33:47,580] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:33:47,684] adding document #4350000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:34:12,167] discarding 27685 tokens: [('yncm', 1), ('hayslett', 1), ('skyh', 1), ('bawskee', 1), ('errthang', 1), ('glockoma', 1), ('markeyvius', 1), ('bandplay', 1), ('countyii', 1), ('roadlesss', 1)]...\n",
      "[2022-09-12 07:34:12,168] keeping 2000000 tokens which were in no less than 0 and no more than 4360000 (=100.0%) documents\n",
      "[2022-09-12 07:34:17,085] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:34:17,187] adding document #4360000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:34:41,324] discarding 32003 tokens: [('arachchillage', 1), ('arrachchige', 1), ('arudchelvam', 1), ('cumaraswamy', 1), ('dorapege', 1), ('haputantrige', 1), ('harshadeva', 1), ('heeralu', 1), ('hemapriya', 1), ('illayapparachchi', 1)]...\n",
      "[2022-09-12 07:34:41,325] keeping 2000000 tokens which were in no less than 0 and no more than 4370000 (=100.0%) documents\n",
      "[2022-09-12 07:34:44,808] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:34:44,877] adding document #4370000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:35:12,288] discarding 32213 tokens: [('getulism', 1), ('getulismo', 1), ('janismo', 1), ('jânismo', 1), ('onselling', 1), ('zenviron', 1), ('flemmich', 1), ('werksleiter', 1), ('tauetia', 1), ('delgarde', 1)]...\n",
      "[2022-09-12 07:35:12,290] keeping 2000000 tokens which were in no less than 0 and no more than 4380000 (=100.0%) documents\n",
      "[2022-09-12 07:35:17,144] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:35:17,246] adding document #4380000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:35:44,058] discarding 31269 tokens: [('artingstall', 1), ('jucielen', 1), ('karriss', 1), ('keamogetse', 1), ('mestiaen', 1), ('mijgona', 1), ('mirzaeva', 1), ('yarisel', 1), ('yodgoroy', 1), ('aayansh', 1)]...\n",
      "[2022-09-12 07:35:44,060] keeping 2000000 tokens which were in no less than 0 and no more than 4390000 (=100.0%) documents\n",
      "[2022-09-12 07:35:47,579] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:35:47,648] adding document #4390000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:36:13,974] discarding 30652 tokens: [('knyáz', 1), ('rejtelmek', 1), ('ünnepnapok', 1), ('volansky', 1), ('zagaiev', 1), ('zgiev', 1), ('jingukyo', 1), ('jinshukyo', 1), ('kotaijin', 1), ('kaledienė', 1)]...\n",
      "[2022-09-12 07:36:13,976] keeping 2000000 tokens which were in no less than 0 and no more than 4400000 (=100.0%) documents\n",
      "[2022-09-12 07:36:18,889] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:36:18,990] adding document #4400000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:36:46,116] discarding 63657 tokens: [('umoukere', 1), ('ugbokor', 1), ('ukaeli', 1), ('ukotije', 1), ('ukpuru', 1), ('ulmuode', 1), ('umasa', 1), ('umechem', 1), ('umerim', 1), ('umoga', 1)]...\n",
      "[2022-09-12 07:36:46,118] keeping 2000000 tokens which were in no less than 0 and no more than 4410000 (=100.0%) documents\n",
      "[2022-09-12 07:36:51,145] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:36:51,253] adding document #4410000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:37:19,085] discarding 34256 tokens: [('embryogenetically', 1), ('readsorbing', 1), ('thyrogastric', 1), ('filmwilliams', 1), ('hagnazarian', 1), ('networkhbo', 1), ('squadwilliams', 1), ('zanyqa', 1), ('fanförare', 1), ('dosbas', 1)]...\n",
      "[2022-09-12 07:37:19,087] keeping 2000000 tokens which were in no less than 0 and no more than 4420000 (=100.0%) documents\n",
      "[2022-09-12 07:37:22,631] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:37:22,702] adding document #4420000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:37:49,037] discarding 31659 tokens: [('rishiking', 1), ('swachhta', 1), ('tosumbegovic', 1), ('chatzipavlou', 1), ('dimostheni', 1), ('efthyvoulos', 1), ('karemfylaki', 1), ('keoroghluzade', 1), ('róssos', 1), ('tourmousis', 1)]...\n",
      "[2022-09-12 07:37:49,039] keeping 2000000 tokens which were in no less than 0 and no more than 4430000 (=100.0%) documents\n",
      "[2022-09-12 07:37:53,910] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:37:54,011] adding document #4430000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:38:21,123] discarding 29426 tokens: [('maupigyrnun', 1), ('frejndlikh', 1), ('cellectra', 1), ('cqsa', 1), ('batnoam', 1), ('eglia', 1), ('elibaal', 1), ('urumilki', 1), ('yahimilik', 1), ('yapachemou', 1)]...\n",
      "[2022-09-12 07:38:21,125] keeping 2000000 tokens which were in no less than 0 and no more than 4440000 (=100.0%) documents\n",
      "[2022-09-12 07:38:24,696] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:38:24,765] adding document #4440000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:38:53,953] discarding 29237 tokens: [('quantizaton', 1), ('alhausen', 1), ('amezule', 1), ('andelach', 1), ('angreth', 1), ('badmenei', 1), ('balzweiler', 1), ('banvillars', 1), ('belmundt', 1), ('blitzbahn', 1)]...\n",
      "[2022-09-12 07:38:53,955] keeping 2000000 tokens which were in no less than 0 and no more than 4450000 (=100.0%) documents\n",
      "[2022-09-12 07:38:58,918] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:38:59,020] adding document #4450000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:39:24,587] discarding 28774 tokens: [('pahinter', 1), ('telegrafnue', 1), ('misheaded', 1), ('northernamerica', 1), ('mdlxiv', 1), ('oamce', 1), ('barrikadenszene', 1), ('ereignisblatt', 1), ('märztagen', 1), ('zeitgenössige', 1)]...\n",
      "[2022-09-12 07:39:24,589] keeping 2000000 tokens which were in no less than 0 and no more than 4460000 (=100.0%) documents\n",
      "[2022-09-12 07:39:28,128] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:39:28,197] adding document #4460000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:39:54,515] discarding 31226 tokens: [('daluk', 1), ('gadumba', 1), ('hvii', 1), ('lanuva', 1), ('perume', 1), ('rathugala', 1), ('stegeborn', 1), ('talagoya', 1), ('thissahamy', 1), ('tisahamy', 1)]...\n",
      "[2022-09-12 07:39:54,517] keeping 2000000 tokens which were in no less than 0 and no more than 4470000 (=100.0%) documents\n",
      "[2022-09-12 07:39:59,396] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:39:59,497] adding document #4470000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:40:23,315] discarding 29409 tokens: [('autonamiste', 1), ('a_stream_on_efate', 1), ('endehipa', 1), ('hamlinson', 1), ('südpazifik', 1), ('vunic', 1), ('forari', 1), ('mardiganyan', 1), ('pswps', 1), ('groterol', 1)]...\n",
      "[2022-09-12 07:40:23,316] keeping 2000000 tokens which were in no less than 0 and no more than 4480000 (=100.0%) documents\n",
      "[2022-09-12 07:40:26,849] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:40:26,918] adding document #4480000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:40:53,783] discarding 31412 tokens: [('arcottees', 1), ('balheems', 1), ('bangureeas', 1), ('baronee', 1), ('beraries', 1), ('bhyns', 1), ('bulheems', 1), ('bursot', 1), ('corisant', 1), ('daviga', 1)]...\n",
      "[2022-09-12 07:40:53,784] keeping 2000000 tokens which were in no less than 0 and no more than 4490000 (=100.0%) documents\n",
      "[2022-09-12 07:40:57,287] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:40:57,356] adding document #4490000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:41:23,783] discarding 31588 tokens: [('cotranscriptionally', 1), ('ctif', 1), ('curvispinosus', 1), ('mifd', 1), ('oglcnac', 1), ('petromyzom', 1), ('supervillin', 1), ('svil', 1), ('cereep', 1), ('ecotron', 1)]...\n",
      "[2022-09-12 07:41:23,785] keeping 2000000 tokens which were in no less than 0 and no more than 4500000 (=100.0%) documents\n",
      "[2022-09-12 07:41:29,004] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:41:29,105] adding document #4500000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:41:54,453] discarding 32930 tokens: [('kinderschutz', 1), ('arbaʽeen', 1), ('hasania', 1), ('khanqahe', 1), ('kichhauchha', 1), ('sasarami', 1), ('insurtec', 1), ('rightindem', 1), ('campillai', 1), ('funeral_mónica_echeverría_', 1)]...\n",
      "[2022-09-12 07:41:54,455] keeping 2000000 tokens which were in no less than 0 and no more than 4510000 (=100.0%) documents\n",
      "[2022-09-12 07:41:59,435] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:41:59,537] adding document #4510000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:42:23,916] discarding 29153 tokens: [('fmotq', 1), ('idnani', 1), ('anasis', 1), ('kmilsatcom', 1), ('twae', 1), ('yinwae', 1), ('pagenated', 1), ('chiofaro', 1), ('guidó', 1), ('scovanner', 1)]...\n",
      "[2022-09-12 07:42:23,918] keeping 2000000 tokens which were in no less than 0 and no more than 4520000 (=100.0%) documents\n",
      "[2022-09-12 07:42:28,825] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:42:28,926] adding document #4520000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:42:53,898] discarding 29315 tokens: [('anjulí', 1), ('jaquelín', 1), ('beesemyer', 1), ('bohnalite', 1), ('lenckite', 1), ('microlubricant', 1), ('rioco', 1), ('bereznikovskaya', 1), ('charkabozh', 1), ('charkabozhskaya', 1)]...\n",
      "[2022-09-12 07:42:53,901] keeping 2000000 tokens which were in no less than 0 and no more than 4530000 (=100.0%) documents\n",
      "[2022-09-12 07:42:58,919] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:42:59,022] adding document #4530000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:43:24,914] discarding 31485 tokens: [('ciolakowska', 1), ('petrop', 1), ('showwinner', 1), ('henszelman', 1), ('rūrangi', 1), ('thongkongtoon', 1), ('westler', 1), ('yongyoot', 1), ('féc', 1), ('gastiger', 1)]...\n",
      "[2022-09-12 07:43:24,917] keeping 2000000 tokens which were in no less than 0 and no more than 4540000 (=100.0%) documents\n",
      "[2022-09-12 07:43:29,881] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:43:29,984] adding document #4540000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:43:54,911] discarding 32574 tokens: [('janullatos', 1), ('rregullorja', 1), ('oulrc', 1), ('abujmarh', 1), ('korwas', 1), ('shubhranshu', 1), ('voicebook', 1), ('elsharaby', 1), ('faadel', 1), ('ghaghelestani', 1)]...\n",
      "[2022-09-12 07:43:54,913] keeping 2000000 tokens which were in no less than 0 and no more than 4550000 (=100.0%) documents\n",
      "[2022-09-12 07:43:59,828] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:43:59,930] adding document #4550000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:44:24,341] discarding 33833 tokens: [('hcup', 1), ('kozhimannil', 1), ('dulberg', 1), ('taesler', 1), ('ʻē', 1), ('aotizhongxin', 1), ('caixiajie', 1), ('tianchengjie', 1), ('wulihe', 1), ('yingpanjie', 1)]...\n",
      "[2022-09-12 07:44:24,342] keeping 2000000 tokens which were in no less than 0 and no more than 4560000 (=100.0%) documents\n",
      "[2022-09-12 07:44:27,875] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:44:27,945] adding document #4560000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:44:53,831] discarding 32608 tokens: [('ostromezk', 1), ('samostrzel', 1), ('assylium', 1), ('angusticoronatus', 1), ('chaemelio', 1), ('cricochalcis', 1), ('decosteri', 1), ('dolichomenta', 1), ('estéve', 1), ('fitsimmons', 1)]...\n",
      "[2022-09-12 07:44:53,832] keeping 2000000 tokens which were in no less than 0 and no more than 4570000 (=100.0%) documents\n",
      "[2022-09-12 07:44:58,794] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:44:58,896] adding document #4570000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:45:22,695] discarding 29473 tokens: [('nägemused', 1), ('teateid', 1), ('toiduretsepte', 1), ('õpilane', 1), ('janmavakasham', 1), ('kanichukulangara', 1), ('kombananayum', 1), ('velikkakath', 1), ('renditons', 1), ('goorkan', 1)]...\n",
      "[2022-09-12 07:45:22,696] keeping 2000000 tokens which were in no less than 0 and no more than 4580000 (=100.0%) documents\n",
      "[2022-09-12 07:45:27,153] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:45:27,255] adding document #4580000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:45:55,280] discarding 38570 tokens: [('asthale', 1), ('asuravamsham', 1), ('athramel', 1), ('azhagin', 1), ('baashpa', 1), ('bhaasuri', 1), ('chaanchaattam', 1), ('chaarutha', 1), ('chaaykkaan', 1), ('chamayana', 1)]...\n",
      "[2022-09-12 07:45:55,282] keeping 2000000 tokens which were in no less than 0 and no more than 4590000 (=100.0%) documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 07:46:00,182] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:46:00,286] adding document #4590000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:46:26,107] discarding 34129 tokens: [('nanotechnique', 1), ('minskytron', 1), ('riskbank', 1), ('omprises', 1), ('ashvattama', 1), ('dhṛtarāṣtra', 1), ('diparvan', 1), ('naimiśa', 1), ('pañcavimśa', 1), ('pāñcarātrin', 1)]...\n",
      "[2022-09-12 07:46:26,109] keeping 2000000 tokens which were in no less than 0 and no more than 4600000 (=100.0%) documents\n",
      "[2022-09-12 07:46:29,701] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:46:29,771] adding document #4600000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:46:56,817] discarding 32450 tokens: [('јá', 1), ('јад', 1), ('јунаку', 1), ('ќé', 1), ('ќи', 1), ('nonmetallicity', 1), ('agug', 1), ('agugbwvzc', 1), ('binarymime', 1), ('ciagpc', 1)]...\n",
      "[2022-09-12 07:46:56,819] keeping 2000000 tokens which were in no less than 0 and no more than 4610000 (=100.0%) documents\n",
      "[2022-09-12 07:47:02,104] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:47:02,206] adding document #4610000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:47:28,797] discarding 33699 tokens: [('nonarmigerous', 1), ('noninert', 1), ('obfirm', 1), ('obfirmation', 1), ('omnifocal', 1), ('peragration', 1), ('prefinite', 1), ('preformat', 1), ('premediaeval', 1), ('quadricipital', 1)]...\n",
      "[2022-09-12 07:47:28,799] keeping 2000000 tokens which were in no less than 0 and no more than 4620000 (=100.0%) documents\n",
      "[2022-09-12 07:47:34,080] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:47:34,184] adding document #4620000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:48:00,332] discarding 33620 tokens: [('toðne', 1), ('ttùrrí', 1), ('tulɛ', 1), ('tumali', 1), ('tuðní', 1), ('twaɽŋan', 1), ('tíɲən', 1), ('tɔgɔt', 1), ('tɔɔɽɔl', 1), ('tɛjɪɾ', 1)]...\n",
      "[2022-09-12 07:48:00,334] keeping 2000000 tokens which were in no less than 0 and no more than 4630000 (=100.0%) documents\n",
      "[2022-09-12 07:48:04,620] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:48:04,698] adding document #4630000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:48:29,970] discarding 33837 tokens: [('nakamyo', 1), ('sadomitsu', 1), ('かごいま', 1), ('かごっま', 1), ('かごひま', 1), ('かごんま', 1), ('berkajang', 1), ('mediviron', 1), ('ganß', 1), ('parsifil', 1)]...\n",
      "[2022-09-12 07:48:29,972] keeping 2000000 tokens which were in no less than 0 and no more than 4640000 (=100.0%) documents\n",
      "[2022-09-12 07:48:34,849] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:48:34,952] adding document #4640000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:48:58,354] discarding 30055 tokens: [('soebijanto', 1), ('estámos', 1), ('agnerus', 1), ('alíus', 1), ('aurlungatrausti', 1), ('baltram', 1), ('bauga', 1), ('bauggerd', 1), ('beadeca', 1), ('beadohild', 1)]...\n",
      "[2022-09-12 07:48:58,356] keeping 2000000 tokens which were in no less than 0 and no more than 4650000 (=100.0%) documents\n",
      "[2022-09-12 07:49:01,934] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:49:02,002] adding document #4650000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:51:01,279] discarding 31348 tokens: [('agohaze', 1), ('eugnathogobius', 1), ('بازركان', 1), ('kajeli', 1), ('abstersa', 1), ('cobabitatione', 1), ('coliustrandum', 1), ('decisionibus', 1), ('deuictos', 1), ('directione', 1)]...\n",
      "[2022-09-12 07:51:01,280] keeping 2000000 tokens which were in no less than 0 and no more than 4690000 (=100.0%) documents\n",
      "[2022-09-12 07:51:06,212] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:51:06,314] adding document #4690000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:51:29,152] discarding 33340 tokens: [('megatubes', 1), ('norfullerene', 1), ('rhonditic', 1), ('secofullerene', 1), ('trimetaspheres', 1), ('triumphene', 1), ('fredericstown', 1), ('marysvilleplacemarysvillenb', 1), ('publiccode', 1), ('adhore', 1)]...\n",
      "[2022-09-12 07:51:29,154] keeping 2000000 tokens which were in no less than 0 and no more than 4700000 (=100.0%) documents\n",
      "[2022-09-12 07:51:34,464] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:51:34,568] adding document #4700000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:51:58,922] discarding 34614 tokens: [('rōmaiōn', 1), ('egalitarianists', 1), ('galitarianism', 1), ('hierarchist', 1), ('hierarchists', 1), ('probativeness', 1), ('villova', 1), ('philochristus', 1), ('philomythus', 1), ('hezei', 1)]...\n",
      "[2022-09-12 07:51:58,924] keeping 2000000 tokens which were in no less than 0 and no more than 4710000 (=100.0%) documents\n",
      "[2022-09-12 07:52:02,517] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:52:02,588] adding document #4710000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:52:26,155] discarding 31892 tokens: [('abeya', 1), ('dabouk', 1), ('fokak', 1), ('hoarsening', 1), ('khafagy', 1), ('shahinaz', 1), ('tatakhalas', 1), ('tatawy', 1), ('tekha', 1), ('zawjatik', 1)]...\n",
      "[2022-09-12 07:52:26,158] keeping 2000000 tokens which were in no less than 0 and no more than 4720000 (=100.0%) documents\n",
      "[2022-09-12 07:52:31,074] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:52:31,176] adding document #4720000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:52:55,567] discarding 30166 tokens: [('brodding', 1), ('burncastle', 1), ('dolmoune', 1), ('excecution', 1), ('encurtidos', 1), ('unbottled', 1), ('hoglen', 1), ('honlulu', 1), ('pamanian', 1), ('reichhoff', 1)]...\n",
      "[2022-09-12 07:52:55,569] keeping 2000000 tokens which were in no less than 0 and no more than 4730000 (=100.0%) documents\n",
      "[2022-09-12 07:52:59,190] resulting dictionary: Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:52:59,260] adding document #4730000 to Dictionary(2000000 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...)\n",
      "[2022-09-12 07:53:01,768] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-12 07:53:02,128] built Dictionary(2001454 unique tokens: ['ability', 'abolition', 'abstentionism', 'academic', 'accept']...) from 4730463 documents (total 1888752251 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# loc = 'num'|'lr'|'ent'\n",
    "# pos = True|False\n",
    "# download latest wiki dump\n",
    "#w.download_wiki_dump('en', WIKIXML)\n",
    "\n",
    "# parse wiki dump\n",
    "wiki_sentences = w.WikiSentences(WIKIXML, 'en',lower=True) # Orignal\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='EM',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='DEP',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNS',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNSEM',lower=True,pos=False,loc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'political', 'philosophy', 'movement', 'sceptical', 'authority#4', 'reject', 'involuntary', 'coercive', 'form#11', 'hierarchy', 'anarchism', 'call#24', 'abolition', 'state', 'hold#15', 'unnecessary', 'undesirable', 'harmful', 'historically', 'left#7', 'wing#7', 'movement#3', 'placed', 'farthest', 'left', 'political', 'spectrum', 'usually', 'described', 'alongside', 'libertarian', 'marxism', 'libertarian', 'wing#7', 'libertarian', 'socialism', 'socialist', 'movement', 'ha', 'strong', 'historical', 'association', 'anti', 'capitalism', 'socialism', 'human', 'lived', 'society#1', 'without', 'formal', 'hierarchy#1', 'long', 'establishment#2', 'formal', 'state#3', 'realm', 'empire#1', 'rise#25', 'organised', 'hierarchical', 'body', 'scepticism', 'toward', 'authority#4', 'also', 'rose#18', 'th', 'century', 'self', 'conscious', 'political', 'movement#3', 'emerged', 'latter', 'half#1', 'th', 'first#11', 'decade', 'th', 'century', 'anarchist', 'movement', 'flourished', 'part#8', 'world#5', 'significant', 'role#1', 'worker', 'struggle', 'emancipation', 'various', 'anarchist', 'school#4', 'thought#2', 'formed', 'period', 'anarchist', 'taken', 'part#8']\n"
     ]
    }
   ],
   "source": [
    "for sent in wiki_sentences:\n",
    "    print(sent[:100])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:34.083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the data\n"
     ]
    }
   ],
   "source": [
    "#sv.save(wiki_sentences,\"wiki_sentences_pos_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_pos\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_loc\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences\") # orignal\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep2\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_uns\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_unsem\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_em\")\n",
    "sv.save(wiki_sentences,\"wiki_sentences_wnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phrase mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:50:46.815932Z",
     "start_time": "2021-05-29T18:50:46.810808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T22:24:31.858045Z",
     "start_time": "2021-05-29T18:51:21.445049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences, min_count=100, threshold=1)\n",
    "frozen_phrases = phrases.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:41:54.697897Z",
     "start_time": "2021-05-30T11:41:38.608475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(phrases,\"gensim_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:06:13.397797Z",
     "start_time": "2020-03-16T22:06:13.394080Z"
    }
   },
   "source": [
    "# Train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:49:02.663973Z",
     "start_time": "2022-03-13T05:49:01.181727Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentences = sv.load(\"wiki_sentences_no\")\n",
    "#temp_sens are cased!!\n",
    "#sentences = sv.load(\"temp_sens\")\n",
    " \n",
    "#sentences = sv.load(\"wiki_sentences\") #Normal sentences using wiki_old.py\n",
    "\n",
    "#Wiki_Sentences_SP are cased\n",
    "#sentences = sv.load(\"Wiki_Sentences_SP\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_loc\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_sp\") #New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_pos\") # not to be used\n",
    "#sentences = sv.load(\"Wiki_sentences_pos_sample\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent\") # New\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent_sample\") # New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_dep\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_dep2\") #New\n",
    "\n",
    "#wiki english sample Cased \n",
    "#sentences = sv.load(\"Wiki_sentences_sp_sample\")\n",
    "#sentences = sv.load(\"wiki_sentences_uns\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_unsem\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_em\") #New\n",
    "sentences = sv.load(\"wiki_sentences_wnet\") #New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T05:49:02.672912Z",
     "start_time": "2022-03-13T05:49:02.667329Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentences = wiki_sentences\n",
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:09.005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 09:57:00,595] Training model wordnet\n",
      "[2022-09-12 09:57:00,674] collecting all words and their counts\n",
      "[2022-09-12 09:57:12,969] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "[2022-09-12 09:59:49,872] PROGRESS: at sentence #40000, processed 56384702 words, keeping 856740 word types\n",
      "[2022-09-12 10:00:14,854] PROGRESS: at sentence #50000, processed 65366350 words, keeping 934779 word types\n",
      "[2022-09-12 10:00:30,295] PROGRESS: at sentence #60000, processed 70630249 words, keeping 955808 word types\n",
      "[2022-09-12 10:00:45,143] PROGRESS: at sentence #70000, processed 75423773 words, keeping 974931 word types\n",
      "[2022-09-12 10:00:58,905] PROGRESS: at sentence #80000, processed 79662425 words, keeping 990589 word types\n",
      "[2022-09-12 10:01:28,628] PROGRESS: at sentence #90000, processed 90797618 words, keeping 1082983 word types\n",
      "[2022-09-12 10:01:59,697] PROGRESS: at sentence #100000, processed 102457372 words, keeping 1190222 word types\n",
      "[2022-09-12 10:02:28,827] PROGRESS: at sentence #110000, processed 113044065 words, keeping 1279316 word types\n",
      "[2022-09-12 10:02:55,742] PROGRESS: at sentence #120000, processed 123280830 words, keeping 1367558 word types\n",
      "[2022-09-12 10:03:22,278] PROGRESS: at sentence #130000, processed 132841938 words, keeping 1437582 word types\n",
      "[2022-09-12 10:03:50,769] PROGRESS: at sentence #140000, processed 142957677 words, keeping 1523736 word types\n",
      "[2022-09-12 10:04:15,461] PROGRESS: at sentence #150000, processed 152164317 words, keeping 1608566 word types\n",
      "[2022-09-12 10:04:42,499] PROGRESS: at sentence #160000, processed 161579985 words, keeping 1682913 word types\n",
      "[2022-09-12 10:05:07,693] PROGRESS: at sentence #170000, processed 170405489 words, keeping 1749667 word types\n",
      "[2022-09-12 10:05:31,035] PROGRESS: at sentence #180000, processed 178921881 words, keeping 1807439 word types\n",
      "[2022-09-12 10:05:54,050] PROGRESS: at sentence #190000, processed 186790001 words, keeping 1864528 word types\n",
      "[2022-09-12 10:06:17,872] PROGRESS: at sentence #200000, processed 194871835 words, keeping 1924675 word types\n",
      "[2022-09-12 10:06:41,430] PROGRESS: at sentence #210000, processed 202919827 words, keeping 1977267 word types\n",
      "[2022-09-12 10:07:04,515] PROGRESS: at sentence #220000, processed 210772413 words, keeping 2031125 word types\n",
      "[2022-09-12 10:07:27,161] PROGRESS: at sentence #230000, processed 218489325 words, keeping 2084512 word types\n",
      "[2022-09-12 10:07:50,090] PROGRESS: at sentence #240000, processed 226175702 words, keeping 2131383 word types\n",
      "[2022-09-12 10:08:14,768] PROGRESS: at sentence #250000, processed 233925543 words, keeping 2179328 word types\n",
      "[2022-09-12 10:08:38,099] PROGRESS: at sentence #260000, processed 241204407 words, keeping 2223687 word types\n",
      "[2022-09-12 10:09:08,454] PROGRESS: at sentence #270000, processed 248346782 words, keeping 2272073 word types\n",
      "[2022-09-12 10:09:28,568] PROGRESS: at sentence #280000, processed 255307810 words, keeping 2317315 word types\n",
      "[2022-09-12 10:09:49,658] PROGRESS: at sentence #290000, processed 262373363 words, keeping 2366050 word types\n",
      "[2022-09-12 10:10:10,474] PROGRESS: at sentence #300000, processed 269053241 words, keeping 2414606 word types\n",
      "[2022-09-12 10:10:32,296] PROGRESS: at sentence #310000, processed 275834012 words, keeping 2461495 word types\n",
      "[2022-09-12 10:10:52,798] PROGRESS: at sentence #320000, processed 282542948 words, keeping 2503428 word types\n",
      "[2022-09-12 10:11:12,408] PROGRESS: at sentence #330000, processed 288930824 words, keeping 2540966 word types\n",
      "[2022-09-12 10:11:32,751] PROGRESS: at sentence #340000, processed 295545653 words, keeping 2576406 word types\n",
      "[2022-09-12 10:11:52,338] PROGRESS: at sentence #350000, processed 301923322 words, keeping 2610580 word types\n",
      "[2022-09-12 10:12:11,279] PROGRESS: at sentence #360000, processed 308106996 words, keeping 2643713 word types\n",
      "[2022-09-12 10:12:30,662] PROGRESS: at sentence #370000, processed 314378411 words, keeping 2682689 word types\n",
      "[2022-09-12 10:12:52,554] PROGRESS: at sentence #380000, processed 320651834 words, keeping 2716099 word types\n",
      "[2022-09-12 10:13:11,432] PROGRESS: at sentence #390000, processed 326725520 words, keeping 2751196 word types\n",
      "[2022-09-12 10:13:31,001] PROGRESS: at sentence #400000, processed 332850680 words, keeping 2788769 word types\n",
      "[2022-09-12 10:13:49,728] PROGRESS: at sentence #410000, processed 338892657 words, keeping 2826730 word types\n",
      "[2022-09-12 10:14:08,785] PROGRESS: at sentence #420000, processed 344897899 words, keeping 2859468 word types\n",
      "[2022-09-12 10:14:28,135] PROGRESS: at sentence #430000, processed 351041890 words, keeping 2895736 word types\n",
      "[2022-09-12 10:14:47,729] PROGRESS: at sentence #440000, processed 356927705 words, keeping 2930960 word types\n",
      "[2022-09-12 10:15:07,327] PROGRESS: at sentence #450000, processed 362954764 words, keeping 2967609 word types\n",
      "[2022-09-12 10:15:25,353] PROGRESS: at sentence #460000, processed 368532639 words, keeping 2996285 word types\n",
      "[2022-09-12 10:15:44,027] PROGRESS: at sentence #470000, processed 374192493 words, keeping 3031542 word types\n",
      "[2022-09-12 10:16:02,566] PROGRESS: at sentence #480000, processed 379975492 words, keeping 3061617 word types\n",
      "[2022-09-12 10:16:21,596] PROGRESS: at sentence #490000, processed 385790645 words, keeping 3094609 word types\n",
      "[2022-09-12 10:16:49,950] PROGRESS: at sentence #500000, processed 391564762 words, keeping 3126042 word types\n",
      "[2022-09-12 10:17:09,106] PROGRESS: at sentence #510000, processed 397229151 words, keeping 3158073 word types\n",
      "[2022-09-12 10:17:27,914] PROGRESS: at sentence #520000, processed 402851990 words, keeping 3189259 word types\n",
      "[2022-09-12 10:17:47,411] PROGRESS: at sentence #530000, processed 408315830 words, keeping 3222106 word types\n",
      "[2022-09-12 10:18:05,546] PROGRESS: at sentence #540000, processed 413797604 words, keeping 3250931 word types\n",
      "[2022-09-12 10:18:23,477] PROGRESS: at sentence #550000, processed 419197578 words, keeping 3280683 word types\n",
      "[2022-09-12 10:18:41,363] PROGRESS: at sentence #560000, processed 424555890 words, keeping 3307832 word types\n",
      "[2022-09-12 10:18:59,562] PROGRESS: at sentence #570000, processed 429675221 words, keeping 3333424 word types\n",
      "[2022-09-12 10:19:17,374] PROGRESS: at sentence #580000, processed 435149204 words, keeping 3363876 word types\n",
      "[2022-09-12 10:19:34,747] PROGRESS: at sentence #590000, processed 440315476 words, keeping 3392896 word types\n",
      "[2022-09-12 10:19:52,274] PROGRESS: at sentence #600000, processed 445473473 words, keeping 3420874 word types\n",
      "[2022-09-12 10:20:08,959] PROGRESS: at sentence #610000, processed 450380718 words, keeping 3447819 word types\n",
      "[2022-09-12 10:20:27,116] PROGRESS: at sentence #620000, processed 455440341 words, keeping 3475919 word types\n",
      "[2022-09-12 10:20:46,505] PROGRESS: at sentence #630000, processed 460390440 words, keeping 3503462 word types\n",
      "[2022-09-12 10:21:03,428] PROGRESS: at sentence #640000, processed 465276140 words, keeping 3528961 word types\n",
      "[2022-09-12 10:21:19,829] PROGRESS: at sentence #650000, processed 470019403 words, keeping 3560331 word types\n",
      "[2022-09-12 10:21:36,105] PROGRESS: at sentence #660000, processed 474839682 words, keeping 3586126 word types\n",
      "[2022-09-12 10:21:52,901] PROGRESS: at sentence #670000, processed 479647824 words, keeping 3609695 word types\n",
      "[2022-09-12 10:22:09,656] PROGRESS: at sentence #680000, processed 484424597 words, keeping 3634074 word types\n",
      "[2022-09-12 10:22:26,497] PROGRESS: at sentence #690000, processed 489291687 words, keeping 3676055 word types\n",
      "[2022-09-12 10:22:44,379] PROGRESS: at sentence #700000, processed 494083911 words, keeping 3701041 word types\n",
      "[2022-09-12 10:23:01,193] PROGRESS: at sentence #710000, processed 498856247 words, keeping 3722029 word types\n",
      "[2022-09-12 10:23:19,810] PROGRESS: at sentence #720000, processed 503765845 words, keeping 3746907 word types\n",
      "[2022-09-12 10:23:37,502] PROGRESS: at sentence #730000, processed 508498473 words, keeping 3772596 word types\n",
      "[2022-09-12 10:23:54,709] PROGRESS: at sentence #740000, processed 513200005 words, keeping 3795177 word types\n",
      "[2022-09-12 10:24:11,921] PROGRESS: at sentence #750000, processed 518008151 words, keeping 3821378 word types\n",
      "[2022-09-12 10:24:29,352] PROGRESS: at sentence #760000, processed 522571743 words, keeping 3844312 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 10:24:45,720] PROGRESS: at sentence #770000, processed 527113515 words, keeping 3867968 word types\n",
      "[2022-09-12 10:25:01,922] PROGRESS: at sentence #780000, processed 531734109 words, keeping 3894613 word types\n",
      "[2022-09-12 10:25:19,755] PROGRESS: at sentence #790000, processed 536364962 words, keeping 3918372 word types\n",
      "[2022-09-12 10:25:36,897] PROGRESS: at sentence #800000, processed 540844557 words, keeping 3941040 word types\n",
      "[2022-09-12 10:25:53,594] PROGRESS: at sentence #810000, processed 545399062 words, keeping 3965949 word types\n",
      "[2022-09-12 10:26:09,798] PROGRESS: at sentence #820000, processed 549883799 words, keeping 3989244 word types\n",
      "[2022-09-12 10:26:26,961] PROGRESS: at sentence #830000, processed 554487304 words, keeping 4013161 word types\n",
      "[2022-09-12 10:26:44,329] PROGRESS: at sentence #840000, processed 559004663 words, keeping 4036785 word types\n",
      "[2022-09-12 10:26:59,832] PROGRESS: at sentence #850000, processed 563448366 words, keeping 4059165 word types\n",
      "[2022-09-12 10:27:15,324] PROGRESS: at sentence #860000, processed 567887587 words, keeping 4080029 word types\n",
      "[2022-09-12 10:27:32,834] PROGRESS: at sentence #870000, processed 572390448 words, keeping 4105529 word types\n",
      "[2022-09-12 10:27:49,887] PROGRESS: at sentence #880000, processed 576887910 words, keeping 4126414 word types\n",
      "[2022-09-12 10:28:07,194] PROGRESS: at sentence #890000, processed 581326345 words, keeping 4148257 word types\n",
      "[2022-09-12 10:28:23,475] PROGRESS: at sentence #900000, processed 585676316 words, keeping 4169899 word types\n",
      "[2022-09-12 10:28:39,672] PROGRESS: at sentence #910000, processed 590006631 words, keeping 4190287 word types\n",
      "[2022-09-12 10:28:57,905] PROGRESS: at sentence #920000, processed 594483894 words, keeping 4210526 word types\n",
      "[2022-09-12 10:29:14,983] PROGRESS: at sentence #930000, processed 598983979 words, keeping 4235265 word types\n",
      "[2022-09-12 10:29:31,064] PROGRESS: at sentence #940000, processed 603152716 words, keeping 4258226 word types\n",
      "[2022-09-12 10:29:48,452] PROGRESS: at sentence #950000, processed 607405238 words, keeping 4286250 word types\n",
      "[2022-09-12 10:30:05,874] PROGRESS: at sentence #960000, processed 611650648 words, keeping 4311144 word types\n",
      "[2022-09-12 10:30:22,608] PROGRESS: at sentence #970000, processed 615958511 words, keeping 4337710 word types\n",
      "[2022-09-12 10:30:38,257] PROGRESS: at sentence #980000, processed 620138397 words, keeping 4359973 word types\n",
      "[2022-09-12 10:30:53,639] PROGRESS: at sentence #990000, processed 624254214 words, keeping 4383174 word types\n",
      "[2022-09-12 10:31:08,780] PROGRESS: at sentence #1000000, processed 628028550 words, keeping 4412284 word types\n",
      "[2022-09-12 10:31:25,421] PROGRESS: at sentence #1010000, processed 632267296 words, keeping 4437425 word types\n",
      "[2022-09-12 10:31:41,366] PROGRESS: at sentence #1020000, processed 636307512 words, keeping 4459114 word types\n",
      "[2022-09-12 10:31:57,264] PROGRESS: at sentence #1030000, processed 640330140 words, keeping 4478576 word types\n",
      "[2022-09-12 10:32:12,606] PROGRESS: at sentence #1040000, processed 644338502 words, keeping 4497704 word types\n",
      "[2022-09-12 10:32:28,215] PROGRESS: at sentence #1050000, processed 648255400 words, keeping 4515754 word types\n",
      "[2022-09-12 10:32:44,645] PROGRESS: at sentence #1060000, processed 652431573 words, keeping 4535759 word types\n",
      "[2022-09-12 10:32:59,529] PROGRESS: at sentence #1070000, processed 656148842 words, keeping 4554908 word types\n",
      "[2022-09-12 10:33:15,363] PROGRESS: at sentence #1080000, processed 660036477 words, keeping 4575187 word types\n",
      "[2022-09-12 10:33:30,406] PROGRESS: at sentence #1090000, processed 663903128 words, keeping 4594819 word types\n",
      "[2022-09-12 10:33:45,311] PROGRESS: at sentence #1100000, processed 667719469 words, keeping 4614586 word types\n",
      "[2022-09-12 10:34:01,982] PROGRESS: at sentence #1110000, processed 671873822 words, keeping 4633308 word types\n",
      "[2022-09-12 10:34:18,062] PROGRESS: at sentence #1120000, processed 675862676 words, keeping 4651261 word types\n",
      "[2022-09-12 10:34:35,240] PROGRESS: at sentence #1130000, processed 679755922 words, keeping 4670879 word types\n",
      "[2022-09-12 10:34:52,860] PROGRESS: at sentence #1140000, processed 683705663 words, keeping 4691104 word types\n",
      "[2022-09-12 10:35:08,404] PROGRESS: at sentence #1150000, processed 687539339 words, keeping 4709883 word types\n",
      "[2022-09-12 10:35:24,175] PROGRESS: at sentence #1160000, processed 691489988 words, keeping 4731674 word types\n",
      "[2022-09-12 10:35:39,840] PROGRESS: at sentence #1170000, processed 695377642 words, keeping 4749705 word types\n",
      "[2022-09-12 10:35:55,115] PROGRESS: at sentence #1180000, processed 699168870 words, keeping 4769110 word types\n",
      "[2022-09-12 10:36:10,719] PROGRESS: at sentence #1190000, processed 702873145 words, keeping 4792954 word types\n",
      "[2022-09-12 10:36:27,966] PROGRESS: at sentence #1200000, processed 706930446 words, keeping 4810337 word types\n",
      "[2022-09-12 10:36:43,930] PROGRESS: at sentence #1210000, processed 710768783 words, keeping 4829509 word types\n",
      "[2022-09-12 10:37:00,056] PROGRESS: at sentence #1220000, processed 714689160 words, keeping 4847682 word types\n",
      "[2022-09-12 10:37:16,526] PROGRESS: at sentence #1230000, processed 718579834 words, keeping 4864367 word types\n",
      "[2022-09-12 10:37:32,654] PROGRESS: at sentence #1240000, processed 722445834 words, keeping 4881971 word types\n",
      "[2022-09-12 10:37:49,041] PROGRESS: at sentence #1250000, processed 726335900 words, keeping 4900444 word types\n",
      "[2022-09-12 10:38:05,752] PROGRESS: at sentence #1260000, processed 730397303 words, keeping 4919525 word types\n",
      "[2022-09-12 10:38:21,733] PROGRESS: at sentence #1270000, processed 734328053 words, keeping 4940800 word types\n",
      "[2022-09-12 10:38:38,411] PROGRESS: at sentence #1280000, processed 738266926 words, keeping 4959274 word types\n",
      "[2022-09-12 10:38:54,348] PROGRESS: at sentence #1290000, processed 742090560 words, keeping 4982837 word types\n",
      "[2022-09-12 10:39:09,671] PROGRESS: at sentence #1300000, processed 745780220 words, keeping 4999165 word types\n",
      "[2022-09-12 10:39:25,584] PROGRESS: at sentence #1310000, processed 749592398 words, keeping 5016986 word types\n",
      "[2022-09-12 10:39:41,776] PROGRESS: at sentence #1320000, processed 753469293 words, keeping 5036651 word types\n",
      "[2022-09-12 10:39:58,496] PROGRESS: at sentence #1330000, processed 757156361 words, keeping 5053526 word types\n",
      "[2022-09-12 10:40:15,249] PROGRESS: at sentence #1340000, processed 761002551 words, keeping 5073991 word types\n",
      "[2022-09-12 10:40:32,551] PROGRESS: at sentence #1350000, processed 764782741 words, keeping 5092294 word types\n",
      "[2022-09-12 10:40:48,455] PROGRESS: at sentence #1360000, processed 768538265 words, keeping 5112595 word types\n",
      "[2022-09-12 10:41:05,007] PROGRESS: at sentence #1370000, processed 772278033 words, keeping 5135767 word types\n",
      "[2022-09-12 10:41:21,016] PROGRESS: at sentence #1380000, processed 775920575 words, keeping 5153722 word types\n",
      "[2022-09-12 10:41:36,633] PROGRESS: at sentence #1390000, processed 779768268 words, keeping 5171802 word types\n",
      "[2022-09-12 10:41:53,359] PROGRESS: at sentence #1400000, processed 783663942 words, keeping 5189152 word types\n",
      "[2022-09-12 10:42:10,565] PROGRESS: at sentence #1410000, processed 787941405 words, keeping 5206878 word types\n",
      "[2022-09-12 10:42:27,137] PROGRESS: at sentence #1420000, processed 791819437 words, keeping 5222538 word types\n",
      "[2022-09-12 10:42:43,079] PROGRESS: at sentence #1430000, processed 795568836 words, keeping 5239794 word types\n",
      "[2022-09-12 10:43:00,429] PROGRESS: at sentence #1440000, processed 799643369 words, keeping 5257118 word types\n",
      "[2022-09-12 10:43:16,173] PROGRESS: at sentence #1450000, processed 803170605 words, keeping 5273256 word types\n",
      "[2022-09-12 10:43:29,536] PROGRESS: at sentence #1460000, processed 806422225 words, keeping 5289245 word types\n",
      "[2022-09-12 10:43:47,146] PROGRESS: at sentence #1470000, processed 810228410 words, keeping 5309797 word types\n",
      "[2022-09-12 10:44:03,351] PROGRESS: at sentence #1480000, processed 813949475 words, keeping 5325451 word types\n",
      "[2022-09-12 10:44:18,983] PROGRESS: at sentence #1490000, processed 817583480 words, keeping 5341106 word types\n",
      "[2022-09-12 10:44:35,535] PROGRESS: at sentence #1500000, processed 821032975 words, keeping 5359436 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 10:44:52,658] PROGRESS: at sentence #1510000, processed 824594635 words, keeping 5375855 word types\n",
      "[2022-09-12 10:45:07,674] PROGRESS: at sentence #1520000, processed 827725201 words, keeping 5389953 word types\n",
      "[2022-09-12 10:45:22,052] PROGRESS: at sentence #1530000, processed 830784851 words, keeping 5402293 word types\n",
      "[2022-09-12 10:45:39,529] PROGRESS: at sentence #1540000, processed 834250882 words, keeping 5417749 word types\n",
      "[2022-09-12 10:45:55,707] PROGRESS: at sentence #1550000, processed 837745141 words, keeping 5436575 word types\n",
      "[2022-09-12 10:46:11,968] PROGRESS: at sentence #1560000, processed 841267617 words, keeping 5456356 word types\n",
      "[2022-09-12 10:46:29,095] PROGRESS: at sentence #1570000, processed 844942723 words, keeping 5476552 word types\n",
      "[2022-09-12 10:46:47,265] PROGRESS: at sentence #1580000, processed 848391547 words, keeping 5492185 word types\n",
      "[2022-09-12 10:47:04,831] PROGRESS: at sentence #1590000, processed 851929512 words, keeping 5509966 word types\n",
      "[2022-09-12 10:47:20,640] PROGRESS: at sentence #1600000, processed 855432769 words, keeping 5524692 word types\n",
      "[2022-09-12 10:47:38,013] PROGRESS: at sentence #1610000, processed 859089141 words, keeping 5541641 word types\n",
      "[2022-09-12 10:47:54,326] PROGRESS: at sentence #1620000, processed 862598506 words, keeping 5557376 word types\n",
      "[2022-09-12 10:48:10,069] PROGRESS: at sentence #1630000, processed 865993157 words, keeping 5579467 word types\n",
      "[2022-09-12 10:48:27,159] PROGRESS: at sentence #1640000, processed 869540478 words, keeping 5602511 word types\n",
      "[2022-09-12 10:48:43,181] PROGRESS: at sentence #1650000, processed 873143346 words, keeping 5616021 word types\n",
      "[2022-09-12 10:49:00,084] PROGRESS: at sentence #1660000, processed 876889770 words, keeping 5632379 word types\n",
      "[2022-09-12 10:49:19,382] PROGRESS: at sentence #1670000, processed 880601053 words, keeping 5648706 word types\n",
      "[2022-09-12 10:49:34,679] PROGRESS: at sentence #1680000, processed 884072973 words, keeping 5667591 word types\n",
      "[2022-09-12 10:49:50,535] PROGRESS: at sentence #1690000, processed 887378732 words, keeping 5681766 word types\n",
      "[2022-09-12 10:50:07,731] PROGRESS: at sentence #1700000, processed 890395950 words, keeping 5703472 word types\n",
      "[2022-09-12 10:50:26,546] PROGRESS: at sentence #1710000, processed 893633116 words, keeping 5729367 word types\n",
      "[2022-09-12 10:50:43,730] PROGRESS: at sentence #1720000, processed 897097427 words, keeping 5750078 word types\n",
      "[2022-09-12 10:51:00,114] PROGRESS: at sentence #1730000, processed 900526204 words, keeping 5766390 word types\n",
      "[2022-09-12 10:51:18,973] PROGRESS: at sentence #1740000, processed 903979964 words, keeping 5782453 word types\n",
      "[2022-09-12 10:51:36,405] PROGRESS: at sentence #1750000, processed 907449223 words, keeping 5797751 word types\n",
      "[2022-09-12 10:51:55,126] PROGRESS: at sentence #1760000, processed 910952516 words, keeping 5812293 word types\n",
      "[2022-09-12 10:52:13,102] PROGRESS: at sentence #1770000, processed 914478456 words, keeping 5828285 word types\n",
      "[2022-09-12 10:52:32,318] PROGRESS: at sentence #1780000, processed 918102843 words, keeping 5845643 word types\n",
      "[2022-09-12 10:52:54,500] PROGRESS: at sentence #1790000, processed 921659070 words, keeping 5860050 word types\n",
      "[2022-09-12 10:53:19,079] PROGRESS: at sentence #1800000, processed 925086691 words, keeping 5872660 word types\n",
      "[2022-09-12 10:53:35,349] PROGRESS: at sentence #1810000, processed 928590173 words, keeping 5887140 word types\n",
      "[2022-09-12 10:53:50,966] PROGRESS: at sentence #1820000, processed 931992914 words, keeping 5901495 word types\n",
      "[2022-09-12 10:54:07,551] PROGRESS: at sentence #1830000, processed 935581564 words, keeping 5918572 word types\n",
      "[2022-09-12 10:54:23,584] PROGRESS: at sentence #1840000, processed 939089603 words, keeping 5932242 word types\n",
      "[2022-09-12 10:54:40,254] PROGRESS: at sentence #1850000, processed 942627408 words, keeping 5945635 word types\n",
      "[2022-09-12 10:54:55,990] PROGRESS: at sentence #1860000, processed 945949073 words, keeping 5959886 word types\n",
      "[2022-09-12 10:55:13,129] PROGRESS: at sentence #1870000, processed 949486116 words, keeping 5974471 word types\n",
      "[2022-09-12 10:55:29,760] PROGRESS: at sentence #1880000, processed 952884727 words, keeping 5988488 word types\n",
      "[2022-09-12 10:55:46,068] PROGRESS: at sentence #1890000, processed 956268417 words, keeping 6003596 word types\n",
      "[2022-09-12 10:56:04,136] PROGRESS: at sentence #1900000, processed 959839463 words, keeping 6018686 word types\n",
      "[2022-09-12 10:56:21,656] PROGRESS: at sentence #1910000, processed 963188677 words, keeping 6033016 word types\n",
      "[2022-09-12 10:56:39,110] PROGRESS: at sentence #1920000, processed 966553617 words, keeping 6048071 word types\n",
      "[2022-09-12 10:56:56,092] PROGRESS: at sentence #1930000, processed 970008710 words, keeping 6076002 word types\n",
      "[2022-09-12 10:57:11,923] PROGRESS: at sentence #1940000, processed 973133830 words, keeping 6088569 word types\n",
      "[2022-09-12 10:57:32,187] PROGRESS: at sentence #1950000, processed 977030126 words, keeping 6104897 word types\n",
      "[2022-09-12 10:57:51,363] PROGRESS: at sentence #1960000, processed 980555022 words, keeping 6120059 word types\n",
      "[2022-09-12 10:58:09,344] PROGRESS: at sentence #1970000, processed 984278085 words, keeping 6135085 word types\n",
      "[2022-09-12 10:58:26,973] PROGRESS: at sentence #1980000, processed 987877106 words, keeping 6151337 word types\n",
      "[2022-09-12 10:58:46,177] PROGRESS: at sentence #1990000, processed 991353912 words, keeping 6168121 word types\n",
      "[2022-09-12 10:59:07,260] PROGRESS: at sentence #2000000, processed 994882190 words, keeping 6184929 word types\n",
      "[2022-09-12 10:59:25,157] PROGRESS: at sentence #2010000, processed 998261853 words, keeping 6200570 word types\n",
      "[2022-09-12 10:59:41,998] PROGRESS: at sentence #2020000, processed 1001604025 words, keeping 6213661 word types\n",
      "[2022-09-12 10:59:58,888] PROGRESS: at sentence #2030000, processed 1005105276 words, keeping 6229506 word types\n",
      "[2022-09-12 11:00:14,748] PROGRESS: at sentence #2040000, processed 1008565393 words, keeping 6245104 word types\n",
      "[2022-09-12 11:00:32,268] PROGRESS: at sentence #2050000, processed 1012042470 words, keeping 6260999 word types\n",
      "[2022-09-12 11:00:49,843] PROGRESS: at sentence #2060000, processed 1015541860 words, keeping 6278426 word types\n",
      "[2022-09-12 11:01:08,009] PROGRESS: at sentence #2070000, processed 1019098985 words, keeping 6293716 word types\n",
      "[2022-09-12 11:01:27,194] PROGRESS: at sentence #2080000, processed 1022695355 words, keeping 6308928 word types\n",
      "[2022-09-12 11:01:44,288] PROGRESS: at sentence #2090000, processed 1026163930 words, keeping 6324846 word types\n",
      "[2022-09-12 11:02:02,036] PROGRESS: at sentence #2100000, processed 1029729687 words, keeping 6343992 word types\n",
      "[2022-09-12 11:02:19,694] PROGRESS: at sentence #2110000, processed 1033241186 words, keeping 6359698 word types\n",
      "[2022-09-12 11:02:36,596] PROGRESS: at sentence #2120000, processed 1036550110 words, keeping 6377287 word types\n",
      "[2022-09-12 11:02:52,617] PROGRESS: at sentence #2130000, processed 1039610657 words, keeping 6389093 word types\n",
      "[2022-09-12 11:03:08,618] PROGRESS: at sentence #2140000, processed 1042883851 words, keeping 6405412 word types\n",
      "[2022-09-12 11:03:25,282] PROGRESS: at sentence #2150000, processed 1046133425 words, keeping 6433253 word types\n",
      "[2022-09-12 11:03:43,160] PROGRESS: at sentence #2160000, processed 1049620958 words, keeping 6448470 word types\n",
      "[2022-09-12 11:03:59,911] PROGRESS: at sentence #2170000, processed 1052874771 words, keeping 6464737 word types\n",
      "[2022-09-12 11:04:17,414] PROGRESS: at sentence #2180000, processed 1056306168 words, keeping 6481624 word types\n",
      "[2022-09-12 11:04:36,742] PROGRESS: at sentence #2190000, processed 1059792343 words, keeping 6494490 word types\n",
      "[2022-09-12 11:04:53,153] PROGRESS: at sentence #2200000, processed 1063181929 words, keeping 6509021 word types\n",
      "[2022-09-12 11:05:11,483] PROGRESS: at sentence #2210000, processed 1066711576 words, keeping 6522415 word types\n",
      "[2022-09-12 11:05:30,155] PROGRESS: at sentence #2220000, processed 1070087932 words, keeping 6537875 word types\n",
      "[2022-09-12 11:05:54,848] PROGRESS: at sentence #2230000, processed 1073729615 words, keeping 6553246 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 11:06:14,336] PROGRESS: at sentence #2240000, processed 1077121203 words, keeping 6568724 word types\n",
      "[2022-09-12 11:06:30,992] PROGRESS: at sentence #2250000, processed 1080364265 words, keeping 6592388 word types\n",
      "[2022-09-12 11:06:49,445] PROGRESS: at sentence #2260000, processed 1083848641 words, keeping 6610708 word types\n",
      "[2022-09-12 11:07:07,379] PROGRESS: at sentence #2270000, processed 1087233298 words, keeping 6628263 word types\n",
      "[2022-09-12 11:07:24,498] PROGRESS: at sentence #2280000, processed 1090619789 words, keeping 6644115 word types\n",
      "[2022-09-12 11:07:41,829] PROGRESS: at sentence #2290000, processed 1094074855 words, keeping 6659501 word types\n",
      "[2022-09-12 11:08:00,313] PROGRESS: at sentence #2300000, processed 1097684389 words, keeping 6675039 word types\n",
      "[2022-09-12 11:08:17,522] PROGRESS: at sentence #2310000, processed 1101218844 words, keeping 6690457 word types\n",
      "[2022-09-12 11:08:33,650] PROGRESS: at sentence #2320000, processed 1104561342 words, keeping 6709765 word types\n",
      "[2022-09-12 11:08:50,610] PROGRESS: at sentence #2330000, processed 1108094492 words, keeping 6725067 word types\n",
      "[2022-09-12 11:09:07,577] PROGRESS: at sentence #2340000, processed 1111401067 words, keeping 6740146 word types\n",
      "[2022-09-12 11:09:24,842] PROGRESS: at sentence #2350000, processed 1114845153 words, keeping 6754837 word types\n",
      "[2022-09-12 11:09:43,432] PROGRESS: at sentence #2360000, processed 1118475943 words, keeping 6767760 word types\n",
      "[2022-09-12 11:09:59,883] PROGRESS: at sentence #2370000, processed 1121839043 words, keeping 6780759 word types\n",
      "[2022-09-12 11:10:19,133] PROGRESS: at sentence #2380000, processed 1125270111 words, keeping 6795312 word types\n",
      "[2022-09-12 11:10:36,972] PROGRESS: at sentence #2390000, processed 1128874571 words, keeping 6810781 word types\n",
      "[2022-09-12 11:10:52,393] PROGRESS: at sentence #2400000, processed 1132096484 words, keeping 6827586 word types\n",
      "[2022-09-12 11:11:08,985] PROGRESS: at sentence #2410000, processed 1135517800 words, keeping 6843847 word types\n",
      "[2022-09-12 11:11:25,657] PROGRESS: at sentence #2420000, processed 1138840309 words, keeping 6858529 word types\n",
      "[2022-09-12 11:11:43,133] PROGRESS: at sentence #2430000, processed 1142384222 words, keeping 6874666 word types\n",
      "[2022-09-12 11:12:00,033] PROGRESS: at sentence #2440000, processed 1145810132 words, keeping 6889736 word types\n",
      "[2022-09-12 11:12:15,391] PROGRESS: at sentence #2450000, processed 1149052518 words, keeping 6902294 word types\n",
      "[2022-09-12 11:12:31,886] PROGRESS: at sentence #2460000, processed 1152202080 words, keeping 6914998 word types\n",
      "[2022-09-12 11:12:50,188] PROGRESS: at sentence #2470000, processed 1155345067 words, keeping 6928969 word types\n",
      "[2022-09-12 11:13:06,473] PROGRESS: at sentence #2480000, processed 1158612139 words, keeping 6942156 word types\n",
      "[2022-09-12 11:13:24,309] PROGRESS: at sentence #2490000, processed 1162179497 words, keeping 6987449 word types\n",
      "[2022-09-12 11:13:47,832] PROGRESS: at sentence #2500000, processed 1165693325 words, keeping 7004967 word types\n",
      "[2022-09-12 11:14:07,063] PROGRESS: at sentence #2510000, processed 1169086834 words, keeping 7021724 word types\n",
      "[2022-09-12 11:14:24,687] PROGRESS: at sentence #2520000, processed 1172341215 words, keeping 7037620 word types\n",
      "[2022-09-12 11:14:42,429] PROGRESS: at sentence #2530000, processed 1175751571 words, keeping 7053963 word types\n",
      "[2022-09-12 11:14:58,384] PROGRESS: at sentence #2540000, processed 1179043558 words, keeping 7067147 word types\n",
      "[2022-09-12 11:15:14,133] PROGRESS: at sentence #2550000, processed 1182344008 words, keeping 7081069 word types\n",
      "[2022-09-12 11:15:35,649] PROGRESS: at sentence #2560000, processed 1185550487 words, keeping 7094992 word types\n",
      "[2022-09-12 11:15:56,636] PROGRESS: at sentence #2570000, processed 1188935430 words, keeping 7111526 word types\n",
      "[2022-09-12 11:16:15,235] PROGRESS: at sentence #2580000, processed 1192398562 words, keeping 7134518 word types\n",
      "[2022-09-12 11:16:32,693] PROGRESS: at sentence #2590000, processed 1195732859 words, keeping 7149430 word types\n",
      "[2022-09-12 11:16:49,731] PROGRESS: at sentence #2600000, processed 1199021278 words, keeping 7163964 word types\n",
      "[2022-09-12 11:17:05,682] PROGRESS: at sentence #2610000, processed 1202115993 words, keeping 7176295 word types\n",
      "[2022-09-12 11:17:21,577] PROGRESS: at sentence #2620000, processed 1205065571 words, keeping 7188135 word types\n",
      "[2022-09-12 11:17:42,358] PROGRESS: at sentence #2630000, processed 1208271498 words, keeping 7200965 word types\n",
      "[2022-09-12 11:18:07,222] PROGRESS: at sentence #2640000, processed 1211760040 words, keeping 7217627 word types\n",
      "[2022-09-12 11:18:24,911] PROGRESS: at sentence #2650000, processed 1215093917 words, keeping 7234605 word types\n",
      "[2022-09-12 11:18:44,816] PROGRESS: at sentence #2660000, processed 1218581013 words, keeping 7249273 word types\n",
      "[2022-09-12 11:19:01,744] PROGRESS: at sentence #2670000, processed 1222039771 words, keeping 7264151 word types\n",
      "[2022-09-12 11:19:19,865] PROGRESS: at sentence #2680000, processed 1225419766 words, keeping 7279164 word types\n",
      "[2022-09-12 11:19:38,185] PROGRESS: at sentence #2690000, processed 1230171480 words, keeping 7297777 word types\n",
      "[2022-09-12 11:19:55,998] PROGRESS: at sentence #2700000, processed 1233649655 words, keeping 7313408 word types\n",
      "[2022-09-12 11:20:13,052] PROGRESS: at sentence #2710000, processed 1237065126 words, keeping 7330074 word types\n",
      "[2022-09-12 11:20:31,600] PROGRESS: at sentence #2720000, processed 1240556650 words, keeping 7347248 word types\n",
      "[2022-09-12 11:20:49,122] PROGRESS: at sentence #2730000, processed 1244179045 words, keeping 7364284 word types\n",
      "[2022-09-12 11:21:06,231] PROGRESS: at sentence #2740000, processed 1247739257 words, keeping 7379150 word types\n",
      "[2022-09-12 11:21:24,358] PROGRESS: at sentence #2750000, processed 1251296960 words, keeping 7394646 word types\n",
      "[2022-09-12 11:21:42,948] PROGRESS: at sentence #2760000, processed 1254803682 words, keeping 7409001 word types\n",
      "[2022-09-12 11:22:01,983] PROGRESS: at sentence #2770000, processed 1258275989 words, keeping 7424723 word types\n",
      "[2022-09-12 11:22:24,189] PROGRESS: at sentence #2780000, processed 1261628962 words, keeping 7440705 word types\n",
      "[2022-09-12 11:22:42,460] PROGRESS: at sentence #2790000, processed 1264973971 words, keeping 7456797 word types\n",
      "[2022-09-12 11:23:00,599] PROGRESS: at sentence #2800000, processed 1268246943 words, keeping 7474919 word types\n",
      "[2022-09-12 11:23:20,173] PROGRESS: at sentence #2810000, processed 1271548335 words, keeping 7489381 word types\n",
      "[2022-09-12 11:23:37,719] PROGRESS: at sentence #2820000, processed 1274641333 words, keeping 7503448 word types\n",
      "[2022-09-12 11:23:56,535] PROGRESS: at sentence #2830000, processed 1277997057 words, keeping 7519283 word types\n",
      "[2022-09-12 11:24:15,675] PROGRESS: at sentence #2840000, processed 1281369863 words, keeping 7535668 word types\n",
      "[2022-09-12 11:24:33,917] PROGRESS: at sentence #2850000, processed 1284768970 words, keeping 7551716 word types\n",
      "[2022-09-12 11:24:53,109] PROGRESS: at sentence #2860000, processed 1288177370 words, keeping 7567372 word types\n",
      "[2022-09-12 11:25:10,799] PROGRESS: at sentence #2870000, processed 1291506282 words, keeping 7581802 word types\n",
      "[2022-09-12 11:25:28,810] PROGRESS: at sentence #2880000, processed 1294904063 words, keeping 7597620 word types\n",
      "[2022-09-12 11:25:48,232] PROGRESS: at sentence #2890000, processed 1298422697 words, keeping 7611530 word types\n",
      "[2022-09-12 11:26:05,982] PROGRESS: at sentence #2900000, processed 1301851665 words, keeping 7626639 word types\n",
      "[2022-09-12 11:26:22,597] PROGRESS: at sentence #2910000, processed 1305170583 words, keeping 7640018 word types\n",
      "[2022-09-12 11:26:42,760] PROGRESS: at sentence #2920000, processed 1308664064 words, keeping 7654148 word types\n",
      "[2022-09-12 11:27:03,538] PROGRESS: at sentence #2930000, processed 1311981262 words, keeping 7668509 word types\n",
      "[2022-09-12 11:27:21,327] PROGRESS: at sentence #2940000, processed 1315478840 words, keeping 7682759 word types\n",
      "[2022-09-12 11:27:40,008] PROGRESS: at sentence #2950000, processed 1318882790 words, keeping 7699905 word types\n",
      "[2022-09-12 11:27:58,480] PROGRESS: at sentence #2960000, processed 1322288583 words, keeping 7714464 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 11:28:16,051] PROGRESS: at sentence #2970000, processed 1325712650 words, keeping 7731016 word types\n",
      "[2022-09-12 11:28:35,169] PROGRESS: at sentence #2980000, processed 1329122933 words, keeping 7745588 word types\n",
      "[2022-09-12 11:28:53,781] PROGRESS: at sentence #2990000, processed 1332525926 words, keeping 7757426 word types\n",
      "[2022-09-12 11:29:13,339] PROGRESS: at sentence #3000000, processed 1335866692 words, keeping 7771491 word types\n",
      "[2022-09-12 11:29:31,211] PROGRESS: at sentence #3010000, processed 1339011451 words, keeping 7785068 word types\n",
      "[2022-09-12 11:29:49,885] PROGRESS: at sentence #3020000, processed 1342313948 words, keeping 7800396 word types\n",
      "[2022-09-12 11:30:08,564] PROGRESS: at sentence #3030000, processed 1345488024 words, keeping 7812176 word types\n",
      "[2022-09-12 11:30:27,331] PROGRESS: at sentence #3040000, processed 1348852392 words, keeping 7830617 word types\n",
      "[2022-09-12 11:30:45,192] PROGRESS: at sentence #3050000, processed 1352041233 words, keeping 7845186 word types\n",
      "[2022-09-12 11:31:04,916] PROGRESS: at sentence #3060000, processed 1355385986 words, keeping 7858886 word types\n",
      "[2022-09-12 11:31:25,427] PROGRESS: at sentence #3070000, processed 1358649895 words, keeping 7873231 word types\n",
      "[2022-09-12 11:31:46,284] PROGRESS: at sentence #3080000, processed 1362168924 words, keeping 7887164 word types\n",
      "[2022-09-12 11:32:05,307] PROGRESS: at sentence #3090000, processed 1365658283 words, keeping 7901575 word types\n",
      "[2022-09-12 11:32:25,168] PROGRESS: at sentence #3100000, processed 1369101182 words, keeping 7915573 word types\n",
      "[2022-09-12 11:32:47,474] PROGRESS: at sentence #3110000, processed 1372600346 words, keeping 7929751 word types\n",
      "[2022-09-12 11:33:07,202] PROGRESS: at sentence #3120000, processed 1376149473 words, keeping 7944261 word types\n",
      "[2022-09-12 11:33:30,318] PROGRESS: at sentence #3130000, processed 1379467025 words, keeping 7956965 word types\n",
      "[2022-09-12 11:33:51,721] PROGRESS: at sentence #3140000, processed 1382934625 words, keeping 7970653 word types\n",
      "[2022-09-12 11:34:13,022] PROGRESS: at sentence #3150000, processed 1386420160 words, keeping 7983203 word types\n",
      "[2022-09-12 11:34:32,123] PROGRESS: at sentence #3160000, processed 1389701107 words, keeping 7996373 word types\n",
      "[2022-09-12 11:34:51,504] PROGRESS: at sentence #3170000, processed 1393125654 words, keeping 8009002 word types\n",
      "[2022-09-12 11:35:10,589] PROGRESS: at sentence #3180000, processed 1396634539 words, keeping 8022850 word types\n",
      "[2022-09-12 11:35:29,521] PROGRESS: at sentence #3190000, processed 1400137174 words, keeping 8034750 word types\n",
      "[2022-09-12 11:35:49,289] PROGRESS: at sentence #3200000, processed 1403549931 words, keeping 8046115 word types\n",
      "[2022-09-12 11:36:12,663] PROGRESS: at sentence #3210000, processed 1407132343 words, keeping 8059751 word types\n",
      "[2022-09-12 11:36:33,289] PROGRESS: at sentence #3220000, processed 1410674686 words, keeping 8072309 word types\n",
      "[2022-09-12 11:36:52,328] PROGRESS: at sentence #3230000, processed 1414103494 words, keeping 8086392 word types\n",
      "[2022-09-12 11:37:10,669] PROGRESS: at sentence #3240000, processed 1417373995 words, keeping 8100678 word types\n",
      "[2022-09-12 11:37:30,065] PROGRESS: at sentence #3250000, processed 1420699531 words, keeping 8118123 word types\n",
      "[2022-09-12 11:37:48,485] PROGRESS: at sentence #3260000, processed 1423979814 words, keeping 8132994 word types\n",
      "[2022-09-12 11:38:07,231] PROGRESS: at sentence #3270000, processed 1427415151 words, keeping 8146326 word types\n",
      "[2022-09-12 11:38:25,812] PROGRESS: at sentence #3280000, processed 1430714118 words, keeping 8160439 word types\n",
      "[2022-09-12 11:38:44,733] PROGRESS: at sentence #3290000, processed 1434027232 words, keeping 8171937 word types\n",
      "[2022-09-12 11:39:04,186] PROGRESS: at sentence #3300000, processed 1437433448 words, keeping 8186807 word types\n",
      "[2022-09-12 11:39:24,082] PROGRESS: at sentence #3310000, processed 1440926890 words, keeping 8202172 word types\n",
      "[2022-09-12 11:39:41,986] PROGRESS: at sentence #3320000, processed 1444206445 words, keeping 8214490 word types\n",
      "[2022-09-12 11:40:00,221] PROGRESS: at sentence #3330000, processed 1447433779 words, keeping 8225873 word types\n",
      "[2022-09-12 11:40:18,924] PROGRESS: at sentence #3340000, processed 1450827343 words, keeping 8240708 word types\n",
      "[2022-09-12 11:40:35,980] PROGRESS: at sentence #3350000, processed 1454062028 words, keeping 8253292 word types\n",
      "[2022-09-12 11:40:54,981] PROGRESS: at sentence #3360000, processed 1457431875 words, keeping 8265078 word types\n",
      "[2022-09-12 11:41:13,271] PROGRESS: at sentence #3370000, processed 1460808410 words, keeping 8278369 word types\n",
      "[2022-09-12 11:41:32,073] PROGRESS: at sentence #3380000, processed 1464253299 words, keeping 8292318 word types\n",
      "[2022-09-12 11:41:52,213] PROGRESS: at sentence #3390000, processed 1467652055 words, keeping 8306787 word types\n",
      "[2022-09-12 11:42:10,595] PROGRESS: at sentence #3400000, processed 1470965741 words, keeping 8322875 word types\n",
      "[2022-09-12 11:42:28,230] PROGRESS: at sentence #3410000, processed 1474082219 words, keeping 8337803 word types\n",
      "[2022-09-12 11:42:47,221] PROGRESS: at sentence #3420000, processed 1477192018 words, keeping 8350110 word types\n",
      "[2022-09-12 11:43:05,308] PROGRESS: at sentence #3430000, processed 1480379209 words, keeping 8362065 word types\n",
      "[2022-09-12 11:43:23,478] PROGRESS: at sentence #3440000, processed 1483585134 words, keeping 8374765 word types\n",
      "[2022-09-12 11:43:46,168] PROGRESS: at sentence #3450000, processed 1486898803 words, keeping 8389273 word types\n",
      "[2022-09-12 11:44:03,530] PROGRESS: at sentence #3460000, processed 1489965306 words, keeping 8402230 word types\n",
      "[2022-09-12 11:44:20,306] PROGRESS: at sentence #3470000, processed 1493125977 words, keeping 8416619 word types\n",
      "[2022-09-12 11:44:38,249] PROGRESS: at sentence #3480000, processed 1496355926 words, keeping 8430222 word types\n",
      "[2022-09-12 11:44:57,064] PROGRESS: at sentence #3490000, processed 1499708220 words, keeping 8451890 word types\n",
      "[2022-09-12 11:45:16,101] PROGRESS: at sentence #3500000, processed 1503076599 words, keeping 8463549 word types\n",
      "[2022-09-12 11:45:34,432] PROGRESS: at sentence #3510000, processed 1506254809 words, keeping 8475488 word types\n",
      "[2022-09-12 11:45:53,794] PROGRESS: at sentence #3520000, processed 1509602779 words, keeping 8489757 word types\n",
      "[2022-09-12 11:46:11,207] PROGRESS: at sentence #3530000, processed 1512881680 words, keeping 8501360 word types\n",
      "[2022-09-12 11:46:28,112] PROGRESS: at sentence #3540000, processed 1516236439 words, keeping 8512994 word types\n",
      "[2022-09-12 11:46:45,285] PROGRESS: at sentence #3550000, processed 1519489414 words, keeping 8524730 word types\n",
      "[2022-09-12 11:47:04,601] PROGRESS: at sentence #3560000, processed 1522815519 words, keeping 8538356 word types\n",
      "[2022-09-12 11:47:29,052] PROGRESS: at sentence #3570000, processed 1526182754 words, keeping 8550487 word types\n",
      "[2022-09-12 11:47:46,974] PROGRESS: at sentence #3580000, processed 1529483081 words, keeping 8561055 word types\n",
      "[2022-09-12 11:48:06,785] PROGRESS: at sentence #3590000, processed 1532829715 words, keeping 8572621 word types\n",
      "[2022-09-12 11:48:25,699] PROGRESS: at sentence #3600000, processed 1536048844 words, keeping 8584055 word types\n",
      "[2022-09-12 11:48:43,961] PROGRESS: at sentence #3610000, processed 1539079409 words, keeping 8597810 word types\n",
      "[2022-09-12 11:49:02,304] PROGRESS: at sentence #3620000, processed 1542060044 words, keeping 8608152 word types\n",
      "[2022-09-12 11:49:19,244] PROGRESS: at sentence #3630000, processed 1544961814 words, keeping 8619576 word types\n",
      "[2022-09-12 11:49:36,478] PROGRESS: at sentence #3640000, processed 1547890338 words, keeping 8631327 word types\n",
      "[2022-09-12 11:49:52,058] PROGRESS: at sentence #3650000, processed 1550751529 words, keeping 8642525 word types\n",
      "[2022-09-12 11:50:11,008] PROGRESS: at sentence #3660000, processed 1554160040 words, keeping 8656754 word types\n",
      "[2022-09-12 11:50:29,630] PROGRESS: at sentence #3670000, processed 1557367178 words, keeping 8668642 word types\n",
      "[2022-09-12 11:50:48,145] PROGRESS: at sentence #3680000, processed 1560585332 words, keeping 8679615 word types\n",
      "[2022-09-12 11:51:06,086] PROGRESS: at sentence #3690000, processed 1563806094 words, keeping 8691429 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 11:51:23,414] PROGRESS: at sentence #3700000, processed 1566972771 words, keeping 8703372 word types\n",
      "[2022-09-12 11:51:42,146] PROGRESS: at sentence #3710000, processed 1570239115 words, keeping 8716057 word types\n",
      "[2022-09-12 11:52:03,889] PROGRESS: at sentence #3720000, processed 1573609688 words, keeping 8728913 word types\n",
      "[2022-09-12 11:52:23,013] PROGRESS: at sentence #3730000, processed 1576872170 words, keeping 8742052 word types\n",
      "[2022-09-12 11:52:41,498] PROGRESS: at sentence #3740000, processed 1580136745 words, keeping 8755194 word types\n",
      "[2022-09-12 11:52:58,968] PROGRESS: at sentence #3750000, processed 1583338301 words, keeping 8770738 word types\n",
      "[2022-09-12 11:53:24,914] PROGRESS: at sentence #3760000, processed 1586423727 words, keeping 8782733 word types\n",
      "[2022-09-12 11:53:42,550] PROGRESS: at sentence #3770000, processed 1589534557 words, keeping 8794554 word types\n",
      "[2022-09-12 11:54:02,535] PROGRESS: at sentence #3780000, processed 1592628083 words, keeping 8806999 word types\n",
      "[2022-09-12 11:54:18,144] PROGRESS: at sentence #3790000, processed 1595535009 words, keeping 8817732 word types\n",
      "[2022-09-12 11:54:35,421] PROGRESS: at sentence #3800000, processed 1598581649 words, keeping 8830655 word types\n",
      "[2022-09-12 11:54:56,592] PROGRESS: at sentence #3810000, processed 1601850178 words, keeping 8844685 word types\n",
      "[2022-09-12 11:55:14,292] PROGRESS: at sentence #3820000, processed 1605047677 words, keeping 8858306 word types\n",
      "[2022-09-12 11:55:31,530] PROGRESS: at sentence #3830000, processed 1608096835 words, keeping 8871495 word types\n",
      "[2022-09-12 11:55:49,091] PROGRESS: at sentence #3840000, processed 1611019348 words, keeping 8882821 word types\n",
      "[2022-09-12 11:56:07,171] PROGRESS: at sentence #3850000, processed 1614127267 words, keeping 8895821 word types\n",
      "[2022-09-12 11:56:24,734] PROGRESS: at sentence #3860000, processed 1617201513 words, keeping 8908213 word types\n",
      "[2022-09-12 11:56:44,316] PROGRESS: at sentence #3870000, processed 1620222328 words, keeping 8920622 word types\n",
      "[2022-09-12 11:57:03,720] PROGRESS: at sentence #3880000, processed 1623372605 words, keeping 8932750 word types\n",
      "[2022-09-12 11:57:24,504] PROGRESS: at sentence #3890000, processed 1626746786 words, keeping 8945825 word types\n",
      "[2022-09-12 11:57:42,934] PROGRESS: at sentence #3900000, processed 1629880818 words, keeping 8958180 word types\n",
      "[2022-09-12 11:58:01,189] PROGRESS: at sentence #3910000, processed 1632851292 words, keeping 8971061 word types\n",
      "[2022-09-12 11:58:19,550] PROGRESS: at sentence #3920000, processed 1635963263 words, keeping 8984004 word types\n",
      "[2022-09-12 11:58:40,590] PROGRESS: at sentence #3930000, processed 1639259426 words, keeping 8996376 word types\n",
      "[2022-09-12 11:58:59,956] PROGRESS: at sentence #3940000, processed 1642364605 words, keeping 9009144 word types\n",
      "[2022-09-12 11:59:18,874] PROGRESS: at sentence #3950000, processed 1645401874 words, keeping 9020346 word types\n",
      "[2022-09-12 11:59:39,190] PROGRESS: at sentence #3960000, processed 1648581379 words, keeping 9033801 word types\n",
      "[2022-09-12 11:59:57,541] PROGRESS: at sentence #3970000, processed 1651666014 words, keeping 9044587 word types\n",
      "[2022-09-12 12:00:16,901] PROGRESS: at sentence #3980000, processed 1654832838 words, keeping 9057268 word types\n",
      "[2022-09-12 12:00:36,401] PROGRESS: at sentence #3990000, processed 1658144226 words, keeping 9069273 word types\n",
      "[2022-09-12 12:00:56,476] PROGRESS: at sentence #4000000, processed 1661329264 words, keeping 9082612 word types\n",
      "[2022-09-12 12:01:20,468] PROGRESS: at sentence #4010000, processed 1664317195 words, keeping 9095106 word types\n",
      "[2022-09-12 12:01:38,981] PROGRESS: at sentence #4020000, processed 1667322315 words, keeping 9108214 word types\n",
      "[2022-09-12 12:01:59,366] PROGRESS: at sentence #4030000, processed 1670471362 words, keeping 9121288 word types\n",
      "[2022-09-12 12:02:20,243] PROGRESS: at sentence #4040000, processed 1673608278 words, keeping 9134744 word types\n",
      "[2022-09-12 12:02:45,729] PROGRESS: at sentence #4050000, processed 1676871279 words, keeping 9155373 word types\n",
      "[2022-09-12 12:03:06,431] PROGRESS: at sentence #4060000, processed 1680045808 words, keeping 9167882 word types\n",
      "[2022-09-12 12:03:24,510] PROGRESS: at sentence #4070000, processed 1683082221 words, keeping 9179386 word types\n",
      "[2022-09-12 12:03:43,466] PROGRESS: at sentence #4080000, processed 1686185847 words, keeping 9192547 word types\n",
      "[2022-09-12 12:04:05,040] PROGRESS: at sentence #4090000, processed 1689523753 words, keeping 9206351 word types\n",
      "[2022-09-12 12:04:24,614] PROGRESS: at sentence #4100000, processed 1692669368 words, keeping 9218250 word types\n",
      "[2022-09-12 12:04:43,523] PROGRESS: at sentence #4110000, processed 1695723881 words, keeping 9233324 word types\n",
      "[2022-09-12 12:05:03,662] PROGRESS: at sentence #4120000, processed 1698847932 words, keeping 9246548 word types\n",
      "[2022-09-12 12:05:22,444] PROGRESS: at sentence #4130000, processed 1701864941 words, keeping 9261376 word types\n",
      "[2022-09-12 12:05:42,107] PROGRESS: at sentence #4140000, processed 1705016082 words, keeping 9276231 word types\n",
      "[2022-09-12 12:06:03,066] PROGRESS: at sentence #4150000, processed 1708097297 words, keeping 9297842 word types\n",
      "[2022-09-12 12:06:22,974] PROGRESS: at sentence #4160000, processed 1711200529 words, keeping 9315649 word types\n",
      "[2022-09-12 12:06:45,815] PROGRESS: at sentence #4170000, processed 1714540693 words, keeping 9327979 word types\n",
      "[2022-09-12 12:07:07,213] PROGRESS: at sentence #4180000, processed 1717949439 words, keeping 9340184 word types\n",
      "[2022-09-12 12:07:26,409] PROGRESS: at sentence #4190000, processed 1721177769 words, keeping 9352105 word types\n",
      "[2022-09-12 12:07:46,014] PROGRESS: at sentence #4200000, processed 1724318719 words, keeping 9363925 word types\n",
      "[2022-09-12 12:08:05,393] PROGRESS: at sentence #4210000, processed 1727469115 words, keeping 9376600 word types\n",
      "[2022-09-12 12:08:24,986] PROGRESS: at sentence #4220000, processed 1730876569 words, keeping 9391030 word types\n",
      "[2022-09-12 12:08:48,003] PROGRESS: at sentence #4230000, processed 1734081265 words, keeping 9401173 word types\n",
      "[2022-09-12 12:09:09,276] PROGRESS: at sentence #4240000, processed 1737300017 words, keeping 9413030 word types\n",
      "[2022-09-12 12:09:36,884] PROGRESS: at sentence #4250000, processed 1740702627 words, keeping 9424897 word types\n",
      "[2022-09-12 12:10:00,581] PROGRESS: at sentence #4260000, processed 1744004165 words, keeping 9437048 word types\n",
      "[2022-09-12 12:10:20,928] PROGRESS: at sentence #4270000, processed 1747101433 words, keeping 9447828 word types\n",
      "[2022-09-12 12:10:40,872] PROGRESS: at sentence #4280000, processed 1750218596 words, keeping 9458731 word types\n",
      "[2022-09-12 12:10:58,923] PROGRESS: at sentence #4290000, processed 1753311723 words, keeping 9469053 word types\n",
      "[2022-09-12 12:11:18,792] PROGRESS: at sentence #4300000, processed 1756467950 words, keeping 9479439 word types\n",
      "[2022-09-12 12:11:37,658] PROGRESS: at sentence #4310000, processed 1759655008 words, keeping 9491583 word types\n",
      "[2022-09-12 12:11:57,984] PROGRESS: at sentence #4320000, processed 1762984480 words, keeping 9503798 word types\n",
      "[2022-09-12 12:12:19,687] PROGRESS: at sentence #4330000, processed 1766173950 words, keeping 9516043 word types\n",
      "[2022-09-12 12:12:40,897] PROGRESS: at sentence #4340000, processed 1769340254 words, keeping 9527362 word types\n",
      "[2022-09-12 12:13:00,615] PROGRESS: at sentence #4350000, processed 1772441861 words, keeping 9538977 word types\n",
      "[2022-09-12 12:13:19,739] PROGRESS: at sentence #4360000, processed 1775501487 words, keeping 9549264 word types\n",
      "[2022-09-12 12:13:39,789] PROGRESS: at sentence #4370000, processed 1778606204 words, keeping 9561598 word types\n",
      "[2022-09-12 12:14:01,185] PROGRESS: at sentence #4380000, processed 1781787823 words, keeping 9574441 word types\n",
      "[2022-09-12 12:14:23,404] PROGRESS: at sentence #4390000, processed 1784931891 words, keeping 9586959 word types\n",
      "[2022-09-12 12:14:43,834] PROGRESS: at sentence #4400000, processed 1788058422 words, keeping 9599354 word types\n",
      "[2022-09-12 12:15:05,332] PROGRESS: at sentence #4410000, processed 1791309743 words, keeping 9634578 word types\n",
      "[2022-09-12 12:15:28,034] PROGRESS: at sentence #4420000, processed 1794573017 words, keeping 9647092 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 12:15:49,888] PROGRESS: at sentence #4430000, processed 1797738840 words, keeping 9658590 word types\n",
      "[2022-09-12 12:16:12,184] PROGRESS: at sentence #4440000, processed 1800984255 words, keeping 9669487 word types\n",
      "[2022-09-12 12:16:37,168] PROGRESS: at sentence #4450000, processed 1804347281 words, keeping 9680408 word types\n",
      "[2022-09-12 12:16:57,572] PROGRESS: at sentence #4460000, processed 1807495059 words, keeping 9691870 word types\n",
      "[2022-09-12 12:17:17,556] PROGRESS: at sentence #4470000, processed 1810553367 words, keeping 9703974 word types\n",
      "[2022-09-12 12:17:43,266] PROGRESS: at sentence #4480000, processed 1813742988 words, keeping 9715122 word types\n",
      "[2022-09-12 12:18:05,202] PROGRESS: at sentence #4490000, processed 1817065513 words, keeping 9727067 word types\n",
      "[2022-09-12 12:18:26,350] PROGRESS: at sentence #4500000, processed 1820320812 words, keeping 9739649 word types\n",
      "[2022-09-12 12:18:46,305] PROGRESS: at sentence #4510000, processed 1823390867 words, keeping 9752024 word types\n",
      "[2022-09-12 12:19:05,648] PROGRESS: at sentence #4520000, processed 1826398420 words, keeping 9762801 word types\n",
      "[2022-09-12 12:19:25,546] PROGRESS: at sentence #4530000, processed 1829323984 words, keeping 9774236 word types\n",
      "[2022-09-12 12:19:45,175] PROGRESS: at sentence #4540000, processed 1832365310 words, keeping 9787742 word types\n",
      "[2022-09-12 12:20:05,121] PROGRESS: at sentence #4550000, processed 1835484510 words, keeping 9800327 word types\n",
      "[2022-09-12 12:20:25,025] PROGRESS: at sentence #4560000, processed 1838576778 words, keeping 9813666 word types\n",
      "[2022-09-12 12:20:45,969] PROGRESS: at sentence #4570000, processed 1841807054 words, keeping 9825387 word types\n",
      "[2022-09-12 12:21:05,114] PROGRESS: at sentence #4580000, processed 1844824332 words, keeping 9836395 word types\n",
      "[2022-09-12 12:21:27,932] PROGRESS: at sentence #4590000, processed 1848035608 words, keeping 9850536 word types\n",
      "[2022-09-12 12:21:48,802] PROGRESS: at sentence #4600000, processed 1851126620 words, keeping 9863761 word types\n",
      "[2022-09-12 12:22:10,210] PROGRESS: at sentence #4610000, processed 1854198798 words, keeping 9876715 word types\n",
      "[2022-09-12 12:22:31,166] PROGRESS: at sentence #4620000, processed 1857274213 words, keeping 9890431 word types\n",
      "[2022-09-12 12:22:52,619] PROGRESS: at sentence #4630000, processed 1860345906 words, keeping 9904062 word types\n",
      "[2022-09-12 12:23:13,010] PROGRESS: at sentence #4640000, processed 1863398520 words, keeping 9917265 word types\n",
      "[2022-09-12 12:23:32,017] PROGRESS: at sentence #4650000, processed 1866386498 words, keeping 9928197 word types\n",
      "[2022-09-12 12:23:51,314] PROGRESS: at sentence #4660000, processed 1869298303 words, keeping 9940906 word types\n",
      "[2022-09-12 12:24:10,430] PROGRESS: at sentence #4670000, processed 1872173063 words, keeping 9952306 word types\n",
      "[2022-09-12 12:24:35,251] PROGRESS: at sentence #4680000, processed 1874965635 words, keeping 9964270 word types\n",
      "[2022-09-12 12:24:54,009] PROGRESS: at sentence #4690000, processed 1877653991 words, keeping 9976877 word types\n",
      "[2022-09-12 12:25:11,463] PROGRESS: at sentence #4700000, processed 1880348179 words, keeping 9990996 word types\n",
      "[2022-09-12 12:25:30,928] PROGRESS: at sentence #4710000, processed 1883230760 words, keeping 10003601 word types\n",
      "[2022-09-12 12:25:49,676] PROGRESS: at sentence #4720000, processed 1885931677 words, keeping 10016576 word types\n",
      "[2022-09-12 12:26:08,846] PROGRESS: at sentence #4730000, processed 1888631054 words, keeping 10028586 word types\n",
      "[2022-09-12 12:26:11,321] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-12 12:26:11,963] collected 10029189 word types from a corpus of 1888752251 raw words and 4730463 sentences\n",
      "[2022-09-12 12:26:11,964] Loading a fresh vocabulary\n",
      "[2022-09-12 12:26:23,623] effective_min_count=5 retains 2487755 unique words (24% of original 10029189, drops 7541434)\n",
      "[2022-09-12 12:26:23,624] effective_min_count=5 leaves 1877310257 word corpus (99% of original 1888752251, drops 11441994)\n",
      "[2022-09-12 12:26:31,431] deleting the raw counts dictionary of 10029189 items\n",
      "[2022-09-12 12:26:31,921] sample=0.001 downsamples 5 most-common words\n",
      "[2022-09-12 12:26:31,921] downsampling leaves estimated 1852679833 word corpus (98.7% of prior 1877310257)\n",
      "[2022-09-12 12:26:41,952] estimated required memory for 2487755 words and 300 dimensions: 7214489500 bytes\n",
      "[2022-09-12 12:26:41,954] resetting layer weights\n",
      "[2022-09-12 12:35:39,008] training model with 40 workers on 2487755 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "[2022-09-12 12:42:43,099] EPOCHs No. 1 - PROGRESS: at 1.00% examples, 147418 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-12 12:46:06,742] EPOCHs No. 1 - PROGRESS: at 2.00% examples, 151080 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 12:51:03,265] EPOCHs No. 1 - PROGRESS: at 3.00% examples, 153911 words/s, in_qsize 51, out_qsize 0\n",
      "[2022-09-12 12:55:13,002] EPOCHs No. 1 - PROGRESS: at 4.00% examples, 155771 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 12:58:59,101] EPOCHs No. 1 - PROGRESS: at 5.00% examples, 156806 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:02:36,076] EPOCHs No. 1 - PROGRESS: at 6.00% examples, 156785 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:05:51,402] EPOCHs No. 1 - PROGRESS: at 7.00% examples, 156987 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:08:55,358] EPOCHs No. 1 - PROGRESS: at 8.00% examples, 157288 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:11:51,024] EPOCHs No. 1 - PROGRESS: at 9.00% examples, 157588 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-12 13:14:40,491] EPOCHs No. 1 - PROGRESS: at 10.00% examples, 157714 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:17:31,986] EPOCHs No. 1 - PROGRESS: at 11.00% examples, 157520 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:20:10,089] EPOCHs No. 1 - PROGRESS: at 12.00% examples, 157558 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-12 13:22:40,376] EPOCHs No. 1 - PROGRESS: at 13.00% examples, 157650 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 13:25:04,972] EPOCHs No. 1 - PROGRESS: at 14.00% examples, 157591 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:27:27,076] EPOCHs No. 1 - PROGRESS: at 15.00% examples, 157552 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:29:49,905] EPOCHs No. 1 - PROGRESS: at 16.00% examples, 157437 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:32:09,560] EPOCHs No. 1 - PROGRESS: at 17.00% examples, 157202 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:34:26,133] EPOCHs No. 1 - PROGRESS: at 18.00% examples, 157056 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:36:39,715] EPOCHs No. 1 - PROGRESS: at 19.00% examples, 156973 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:38:51,783] EPOCHs No. 1 - PROGRESS: at 20.00% examples, 156839 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-12 13:40:58,429] EPOCHs No. 1 - PROGRESS: at 21.00% examples, 156698 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:43:02,407] EPOCHs No. 1 - PROGRESS: at 22.00% examples, 156533 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:45:01,220] EPOCHs No. 1 - PROGRESS: at 23.00% examples, 156421 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:47:06,704] EPOCHs No. 1 - PROGRESS: at 24.00% examples, 156157 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:49:05,318] EPOCHs No. 1 - PROGRESS: at 25.00% examples, 156010 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:51:07,043] EPOCHs No. 1 - PROGRESS: at 26.00% examples, 155802 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:53:10,514] EPOCHs No. 1 - PROGRESS: at 27.00% examples, 155590 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:55:08,107] EPOCHs No. 1 - PROGRESS: at 28.00% examples, 155441 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:57:08,798] EPOCHs No. 1 - PROGRESS: at 29.00% examples, 155182 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 13:59:11,812] EPOCHs No. 1 - PROGRESS: at 30.00% examples, 155010 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:01:12,301] EPOCHs No. 1 - PROGRESS: at 31.00% examples, 154693 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-12 14:03:11,840] EPOCHs No. 1 - PROGRESS: at 32.00% examples, 154363 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 14:05:02,711] EPOCHs No. 1 - PROGRESS: at 33.00% examples, 154047 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:07:05,535] EPOCHs No. 1 - PROGRESS: at 34.00% examples, 153611 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:09:02,204] EPOCHs No. 1 - PROGRESS: at 35.00% examples, 153327 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:11:03,241] EPOCHs No. 1 - PROGRESS: at 36.00% examples, 152869 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:13:06,980] EPOCHs No. 1 - PROGRESS: at 37.00% examples, 152342 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:15:24,223] EPOCHs No. 1 - PROGRESS: at 38.00% examples, 151590 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:17:23,216] EPOCHs No. 1 - PROGRESS: at 39.00% examples, 151301 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:19:18,768] EPOCHs No. 1 - PROGRESS: at 40.00% examples, 151045 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:21:15,799] EPOCHs No. 1 - PROGRESS: at 41.00% examples, 150715 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:23:25,139] EPOCHs No. 1 - PROGRESS: at 42.00% examples, 150326 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:25:31,008] EPOCHs No. 1 - PROGRESS: at 43.00% examples, 149873 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:27:32,841] EPOCHs No. 1 - PROGRESS: at 44.00% examples, 149592 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:29:32,052] EPOCHs No. 1 - PROGRESS: at 45.00% examples, 149288 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:31:30,196] EPOCHs No. 1 - PROGRESS: at 46.00% examples, 148968 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:33:33,057] EPOCHs No. 1 - PROGRESS: at 47.00% examples, 148640 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:35:39,690] EPOCHs No. 1 - PROGRESS: at 48.00% examples, 148227 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:37:41,003] EPOCHs No. 1 - PROGRESS: at 49.00% examples, 147975 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:39:41,698] EPOCHs No. 1 - PROGRESS: at 50.00% examples, 147729 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:41:44,992] EPOCHs No. 1 - PROGRESS: at 51.00% examples, 147407 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:43:40,786] EPOCHs No. 1 - PROGRESS: at 52.00% examples, 147204 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:45:43,841] EPOCHs No. 1 - PROGRESS: at 53.00% examples, 146881 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:47:41,971] EPOCHs No. 1 - PROGRESS: at 54.00% examples, 146638 words/s, in_qsize 0, out_qsize 2\n",
      "[2022-09-12 14:49:51,477] EPOCHs No. 1 - PROGRESS: at 55.00% examples, 146208 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:51:57,688] EPOCHs No. 1 - PROGRESS: at 56.00% examples, 145762 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:54:08,736] EPOCHs No. 1 - PROGRESS: at 57.00% examples, 145544 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 14:56:12,287] EPOCHs No. 1 - PROGRESS: at 58.00% examples, 145344 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 14:58:23,389] EPOCHs No. 1 - PROGRESS: at 59.00% examples, 144981 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:00:28,742] EPOCHs No. 1 - PROGRESS: at 60.00% examples, 144642 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:02:32,328] EPOCHs No. 1 - PROGRESS: at 61.00% examples, 144403 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:04:40,785] EPOCHs No. 1 - PROGRESS: at 62.00% examples, 144096 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:06:46,809] EPOCHs No. 1 - PROGRESS: at 63.00% examples, 143839 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:08:52,428] EPOCHs No. 1 - PROGRESS: at 64.00% examples, 143526 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:10:59,590] EPOCHs No. 1 - PROGRESS: at 65.00% examples, 143217 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:13:13,537] EPOCHs No. 1 - PROGRESS: at 66.00% examples, 142897 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:15:24,591] EPOCHs No. 1 - PROGRESS: at 67.00% examples, 142589 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:17:42,630] EPOCHs No. 1 - PROGRESS: at 68.00% examples, 142252 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:19:48,104] EPOCHs No. 1 - PROGRESS: at 69.00% examples, 142002 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:21:54,814] EPOCHs No. 1 - PROGRESS: at 70.00% examples, 141774 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:23:57,694] EPOCHs No. 1 - PROGRESS: at 71.00% examples, 141565 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:26:01,774] EPOCHs No. 1 - PROGRESS: at 72.00% examples, 141367 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:28:07,151] EPOCHs No. 1 - PROGRESS: at 73.00% examples, 141079 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:30:07,977] EPOCHs No. 1 - PROGRESS: at 74.00% examples, 140892 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:32:10,000] EPOCHs No. 1 - PROGRESS: at 75.00% examples, 140708 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:34:17,136] EPOCHs No. 1 - PROGRESS: at 76.00% examples, 140481 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:36:11,986] EPOCHs No. 1 - PROGRESS: at 77.00% examples, 140261 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:38:13,672] EPOCHs No. 1 - PROGRESS: at 78.00% examples, 140062 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 15:40:16,920] EPOCHs No. 1 - PROGRESS: at 79.00% examples, 139866 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:42:31,656] EPOCHs No. 1 - PROGRESS: at 80.00% examples, 139482 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:44:29,945] EPOCHs No. 1 - PROGRESS: at 81.00% examples, 139289 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:46:33,443] EPOCHs No. 1 - PROGRESS: at 82.00% examples, 139025 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:48:39,138] EPOCHs No. 1 - PROGRESS: at 83.00% examples, 138787 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:50:45,293] EPOCHs No. 1 - PROGRESS: at 84.00% examples, 138525 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 15:52:51,561] EPOCHs No. 1 - PROGRESS: at 85.00% examples, 138275 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:55:06,458] EPOCHs No. 1 - PROGRESS: at 86.00% examples, 137932 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:57:13,918] EPOCHs No. 1 - PROGRESS: at 87.00% examples, 137692 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 15:59:24,639] EPOCHs No. 1 - PROGRESS: at 88.00% examples, 137391 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:01:35,776] EPOCHs No. 1 - PROGRESS: at 89.00% examples, 137155 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:04:10,007] EPOCHs No. 1 - PROGRESS: at 90.00% examples, 136692 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:06:17,531] EPOCHs No. 1 - PROGRESS: at 91.00% examples, 136460 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:08:30,027] EPOCHs No. 1 - PROGRESS: at 92.00% examples, 136205 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:10:44,276] EPOCHs No. 1 - PROGRESS: at 93.00% examples, 135910 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:13:10,209] EPOCHs No. 1 - PROGRESS: at 94.00% examples, 135551 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:15:24,044] EPOCHs No. 1 - PROGRESS: at 95.00% examples, 135287 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:17:31,800] EPOCHs No. 1 - PROGRESS: at 96.00% examples, 135050 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:19:43,590] EPOCHs No. 1 - PROGRESS: at 97.00% examples, 134808 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:21:58,228] EPOCHs No. 1 - PROGRESS: at 98.00% examples, 134518 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:24:09,452] EPOCHs No. 1 - PROGRESS: at 99.00% examples, 134205 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:26:10,490] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-12 16:26:11,021] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-12 16:26:11,030] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-12 16:26:11,030] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-12 16:26:11,031] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-12 16:26:11,032] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-12 16:26:11,032] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-12 16:26:11,033] worker thread finished; awaiting finish of 33 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 16:26:11,033] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-12 16:26:11,033] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-12 16:26:11,034] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-12 16:26:11,034] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-12 16:26:11,035] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-12 16:26:11,035] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-12 16:26:11,036] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-12 16:26:11,036] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-12 16:26:11,037] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-12 16:26:11,037] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-12 16:26:11,038] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-12 16:26:11,038] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-12 16:26:11,038] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-12 16:26:11,039] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-12 16:26:11,039] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-12 16:26:11,040] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-12 16:26:11,040] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-12 16:26:11,040] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-12 16:26:11,041] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-12 16:26:11,041] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-12 16:26:11,041] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-12 16:26:11,042] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-12 16:26:11,042] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-12 16:26:11,043] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-12 16:26:11,043] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-12 16:26:11,043] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-12 16:26:11,044] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-12 16:26:11,044] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-12 16:26:11,045] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-12 16:26:11,045] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-12 16:26:11,045] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-12 16:26:11,046] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-12 16:26:11,252] EPOCHs No. 1 - PROGRESS: at 100.00% examples, 133939 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 16:26:11,253] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-12 16:26:11,254] EPOCH - 1 : training on 1888752251 raw words (1852678239 effective words) took 13832.2s, 133939 effective words/s\n",
      "[2022-09-12 16:33:18,052] EPOCHs No. 2 - PROGRESS: at 1.00% examples, 146495 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 16:36:42,236] EPOCHs No. 2 - PROGRESS: at 2.00% examples, 150293 words/s, in_qsize 43, out_qsize 0\n",
      "[2022-09-12 16:41:33,641] EPOCHs No. 2 - PROGRESS: at 3.00% examples, 154218 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-12 16:45:42,910] EPOCHs No. 2 - PROGRESS: at 4.00% examples, 156088 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:49:28,830] EPOCHs No. 2 - PROGRESS: at 5.00% examples, 157085 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:53:00,932] EPOCHs No. 2 - PROGRESS: at 6.00% examples, 157505 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:56:12,613] EPOCHs No. 2 - PROGRESS: at 7.00% examples, 157947 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 16:59:16,481] EPOCHs No. 2 - PROGRESS: at 8.00% examples, 158168 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:02:11,913] EPOCHs No. 2 - PROGRESS: at 9.00% examples, 158415 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-12 17:05:01,691] EPOCHs No. 2 - PROGRESS: at 10.00% examples, 158467 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:07:51,001] EPOCHs No. 2 - PROGRESS: at 11.00% examples, 158352 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:10:28,185] EPOCHs No. 2 - PROGRESS: at 12.00% examples, 158397 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:12:58,151] EPOCHs No. 2 - PROGRESS: at 13.00% examples, 158464 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:15:23,717] EPOCHs No. 2 - PROGRESS: at 14.00% examples, 158311 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:17:43,936] EPOCHs No. 2 - PROGRESS: at 15.00% examples, 158339 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:20:06,924] EPOCHs No. 2 - PROGRESS: at 16.00% examples, 158180 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:22:23,407] EPOCHs No. 2 - PROGRESS: at 17.00% examples, 158059 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:24:38,096] EPOCHs No. 2 - PROGRESS: at 18.00% examples, 157965 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 17:26:50,974] EPOCHs No. 2 - PROGRESS: at 19.00% examples, 157876 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:29:02,656] EPOCHs No. 2 - PROGRESS: at 20.00% examples, 157726 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:31:08,947] EPOCHs No. 2 - PROGRESS: at 21.00% examples, 157571 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 17:33:12,492] EPOCHs No. 2 - PROGRESS: at 22.00% examples, 157394 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:35:14,348] EPOCHs No. 2 - PROGRESS: at 23.00% examples, 157142 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:37:18,080] EPOCHs No. 2 - PROGRESS: at 24.00% examples, 156920 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:39:16,825] EPOCHs No. 2 - PROGRESS: at 25.00% examples, 156749 words/s, in_qsize 0, out_qsize 2\n",
      "[2022-09-12 17:41:18,273] EPOCHs No. 2 - PROGRESS: at 26.00% examples, 156528 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:43:21,849] EPOCHs No. 2 - PROGRESS: at 27.00% examples, 156292 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:45:20,584] EPOCHs No. 2 - PROGRESS: at 28.00% examples, 156088 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 17:47:20,444] EPOCHs No. 2 - PROGRESS: at 29.00% examples, 155837 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:49:25,547] EPOCHs No. 2 - PROGRESS: at 30.00% examples, 155584 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:51:22,882] EPOCHs No. 2 - PROGRESS: at 31.00% examples, 155349 words/s, in_qsize 4, out_qsize 0\n",
      "[2022-09-12 17:53:22,373] EPOCHs No. 2 - PROGRESS: at 32.00% examples, 155004 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:55:12,479] EPOCHs No. 2 - PROGRESS: at 33.00% examples, 154695 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:57:15,075] EPOCHs No. 2 - PROGRESS: at 34.00% examples, 154249 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 17:59:13,976] EPOCHs No. 2 - PROGRESS: at 35.00% examples, 153891 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:01:16,710] EPOCHs No. 2 - PROGRESS: at 36.00% examples, 153372 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:03:18,395] EPOCHs No. 2 - PROGRESS: at 37.00% examples, 152887 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:05:35,279] EPOCHs No. 2 - PROGRESS: at 38.00% examples, 152128 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:07:33,359] EPOCHs No. 2 - PROGRESS: at 39.00% examples, 151851 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:09:28,460] EPOCHs No. 2 - PROGRESS: at 40.00% examples, 151594 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:11:25,680] EPOCHs No. 2 - PROGRESS: at 41.00% examples, 151248 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:13:38,831] EPOCHs No. 2 - PROGRESS: at 42.00% examples, 150758 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:15:41,685] EPOCHs No. 2 - PROGRESS: at 43.00% examples, 150364 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:17:43,952] EPOCHs No. 2 - PROGRESS: at 44.00% examples, 150064 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:19:43,358] EPOCHs No. 2 - PROGRESS: at 45.00% examples, 149746 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:21:42,148] EPOCHs No. 2 - PROGRESS: at 46.00% examples, 149405 words/s, in_qsize 1, out_qsize 1\n",
      "[2022-09-12 18:23:46,183] EPOCHs No. 2 - PROGRESS: at 47.00% examples, 149043 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 18:25:51,826] EPOCHs No. 2 - PROGRESS: at 48.00% examples, 148642 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:27:52,598] EPOCHs No. 2 - PROGRESS: at 49.00% examples, 148393 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:29:56,974] EPOCHs No. 2 - PROGRESS: at 50.00% examples, 148066 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:31:57,226] EPOCHs No. 2 - PROGRESS: at 51.00% examples, 147798 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:33:52,214] EPOCHs No. 2 - PROGRESS: at 52.00% examples, 147603 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:35:56,601] EPOCHs No. 2 - PROGRESS: at 53.00% examples, 147247 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:37:54,950] EPOCHs No. 2 - PROGRESS: at 54.00% examples, 146995 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:40:03,532] EPOCHs No. 2 - PROGRESS: at 55.00% examples, 146575 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:42:10,961] EPOCHs No. 2 - PROGRESS: at 56.00% examples, 146101 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:44:22,134] EPOCHs No. 2 - PROGRESS: at 57.00% examples, 145875 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:46:29,743] EPOCHs No. 2 - PROGRESS: at 58.00% examples, 145599 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:48:35,536] EPOCHs No. 2 - PROGRESS: at 59.00% examples, 145320 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:50:41,976] EPOCHs No. 2 - PROGRESS: at 60.00% examples, 144959 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 18:52:46,396] EPOCHs No. 2 - PROGRESS: at 61.00% examples, 144701 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:54:53,833] EPOCHs No. 2 - PROGRESS: at 62.00% examples, 144405 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:56:59,126] EPOCHs No. 2 - PROGRESS: at 63.00% examples, 144155 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 18:59:03,848] EPOCHs No. 2 - PROGRESS: at 64.00% examples, 143851 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:01:11,568] EPOCHs No. 2 - PROGRESS: at 65.00% examples, 143529 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:03:26,267] EPOCHs No. 2 - PROGRESS: at 66.00% examples, 143192 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:05:44,105] EPOCHs No. 2 - PROGRESS: at 67.00% examples, 142779 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:07:56,839] EPOCHs No. 2 - PROGRESS: at 68.00% examples, 142515 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:10:02,212] EPOCHs No. 2 - PROGRESS: at 69.00% examples, 142264 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:12:08,684] EPOCHs No. 2 - PROGRESS: at 70.00% examples, 142035 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:14:12,204] EPOCHs No. 2 - PROGRESS: at 71.00% examples, 141814 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:16:18,365] EPOCHs No. 2 - PROGRESS: at 72.00% examples, 141583 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:18:25,289] EPOCHs No. 2 - PROGRESS: at 73.00% examples, 141272 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:20:25,857] EPOCHs No. 2 - PROGRESS: at 74.00% examples, 141086 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:22:27,718] EPOCHs No. 2 - PROGRESS: at 75.00% examples, 140902 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:24:35,711] EPOCHs No. 2 - PROGRESS: at 76.00% examples, 140660 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:26:31,721] EPOCHs No. 2 - PROGRESS: at 77.00% examples, 140422 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:28:39,955] EPOCHs No. 2 - PROGRESS: at 78.00% examples, 140137 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:30:46,145] EPOCHs No. 2 - PROGRESS: at 79.00% examples, 139904 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:32:54,925] EPOCHs No. 2 - PROGRESS: at 80.00% examples, 139593 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:34:53,811] EPOCHs No. 2 - PROGRESS: at 81.00% examples, 139392 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:36:57,246] EPOCHs No. 2 - PROGRESS: at 82.00% examples, 139127 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:39:03,767] EPOCHs No. 2 - PROGRESS: at 83.00% examples, 138878 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:41:08,541] EPOCHs No. 2 - PROGRESS: at 84.00% examples, 138632 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:43:13,842] EPOCHs No. 2 - PROGRESS: at 85.00% examples, 138392 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:45:28,613] EPOCHs No. 2 - PROGRESS: at 86.00% examples, 138048 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:47:36,181] EPOCHs No. 2 - PROGRESS: at 87.00% examples, 137806 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:49:44,921] EPOCHs No. 2 - PROGRESS: at 88.00% examples, 137525 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:52:06,292] EPOCHs No. 2 - PROGRESS: at 89.00% examples, 137174 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:54:31,904] EPOCHs No. 2 - PROGRESS: at 90.00% examples, 136805 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:56:40,663] EPOCHs No. 2 - PROGRESS: at 91.00% examples, 136559 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 19:58:54,033] EPOCHs No. 2 - PROGRESS: at 92.00% examples, 136293 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:01:08,223] EPOCHs No. 2 - PROGRESS: at 93.00% examples, 135998 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:03:33,538] EPOCHs No. 2 - PROGRESS: at 94.00% examples, 135644 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:05:47,475] EPOCHs No. 2 - PROGRESS: at 95.00% examples, 135378 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:07:56,725] EPOCHs No. 2 - PROGRESS: at 96.00% examples, 135124 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:10:09,456] EPOCHs No. 2 - PROGRESS: at 97.00% examples, 134872 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:12:24,842] EPOCHs No. 2 - PROGRESS: at 98.00% examples, 134574 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:14:35,523] EPOCHs No. 2 - PROGRESS: at 99.00% examples, 134266 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:16:48,445] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-12 20:16:49,461] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-12 20:16:49,472] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-12 20:16:49,472] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-12 20:16:49,473] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-12 20:16:49,473] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-12 20:16:49,474] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-12 20:16:49,474] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-12 20:16:49,475] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-12 20:16:49,475] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-12 20:16:49,476] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-12 20:16:49,476] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-12 20:16:49,477] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-12 20:16:49,477] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-12 20:16:49,478] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-12 20:16:49,478] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-12 20:16:49,479] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-12 20:16:49,479] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-12 20:16:49,480] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-12 20:16:49,480] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-12 20:16:49,481] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-12 20:16:49,481] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-12 20:16:49,482] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-12 20:16:49,482] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-12 20:16:49,482] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-12 20:16:49,483] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-12 20:16:49,483] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-12 20:16:49,484] worker thread finished; awaiting finish of 13 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 20:16:49,484] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-12 20:16:49,485] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-12 20:16:49,485] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-12 20:16:49,486] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-12 20:16:49,486] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-12 20:16:49,487] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-12 20:16:49,487] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-12 20:16:49,488] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-12 20:16:49,492] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-12 20:16:49,493] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-12 20:16:49,493] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-12 20:16:49,494] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-12 20:16:49,648] EPOCHs No. 2 - PROGRESS: at 100.00% examples, 133880 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 20:16:49,649] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-12 20:16:49,649] EPOCH - 2 : training on 1888752251 raw words (1852678080 effective words) took 13838.4s, 133880 effective words/s\n",
      "[2022-09-12 20:23:52,129] EPOCHs No. 3 - PROGRESS: at 1.00% examples, 147979 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:27:16,656] EPOCHs No. 3 - PROGRESS: at 2.00% examples, 151236 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:32:10,448] EPOCHs No. 3 - PROGRESS: at 3.00% examples, 154488 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:36:18,170] EPOCHs No. 3 - PROGRESS: at 4.00% examples, 156503 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:40:04,562] EPOCHs No. 3 - PROGRESS: at 5.00% examples, 157390 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:43:38,174] EPOCHs No. 3 - PROGRESS: at 6.00% examples, 157615 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:46:50,496] EPOCHs No. 3 - PROGRESS: at 7.00% examples, 157994 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:49:52,636] EPOCHs No. 3 - PROGRESS: at 8.00% examples, 158348 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 20:52:49,733] EPOCHs No. 3 - PROGRESS: at 9.00% examples, 158462 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:55:40,126] EPOCHs No. 3 - PROGRESS: at 10.00% examples, 158461 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 20:58:28,828] EPOCHs No. 3 - PROGRESS: at 11.00% examples, 158388 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 21:01:06,352] EPOCHs No. 3 - PROGRESS: at 12.00% examples, 158412 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:03:38,705] EPOCHs No. 3 - PROGRESS: at 13.00% examples, 158344 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:06:03,608] EPOCHs No. 3 - PROGRESS: at 14.00% examples, 158232 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:08:28,592] EPOCHs No. 3 - PROGRESS: at 15.00% examples, 158019 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:10:54,103] EPOCHs No. 3 - PROGRESS: at 16.00% examples, 157751 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:13:10,280] EPOCHs No. 3 - PROGRESS: at 17.00% examples, 157664 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 21:15:26,121] EPOCHs No. 3 - PROGRESS: at 18.00% examples, 157535 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:17:39,824] EPOCHs No. 3 - PROGRESS: at 19.00% examples, 157426 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 21:19:51,481] EPOCHs No. 3 - PROGRESS: at 20.00% examples, 157292 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 21:21:59,370] EPOCHs No. 3 - PROGRESS: at 21.00% examples, 157087 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:24:05,043] EPOCHs No. 3 - PROGRESS: at 22.00% examples, 156844 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 21:26:05,295] EPOCHs No. 3 - PROGRESS: at 23.00% examples, 156668 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-12 21:28:10,381] EPOCHs No. 3 - PROGRESS: at 24.00% examples, 156411 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:30:08,110] EPOCHs No. 3 - PROGRESS: at 25.00% examples, 156290 words/s, in_qsize 0, out_qsize 4\n",
      "[2022-09-12 21:32:10,378] EPOCHs No. 3 - PROGRESS: at 26.00% examples, 156054 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:34:14,571] EPOCHs No. 3 - PROGRESS: at 27.00% examples, 155811 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:36:13,380] EPOCHs No. 3 - PROGRESS: at 28.00% examples, 155617 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:38:15,725] EPOCHs No. 3 - PROGRESS: at 29.00% examples, 155300 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:40:19,262] EPOCHs No. 3 - PROGRESS: at 30.00% examples, 155110 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:42:17,541] EPOCHs No. 3 - PROGRESS: at 31.00% examples, 154855 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:44:16,824] EPOCHs No. 3 - PROGRESS: at 32.00% examples, 154531 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:46:07,189] EPOCHs No. 3 - PROGRESS: at 33.00% examples, 154225 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:48:09,174] EPOCHs No. 3 - PROGRESS: at 34.00% examples, 153808 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:50:08,675] EPOCHs No. 3 - PROGRESS: at 35.00% examples, 153443 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:52:11,864] EPOCHs No. 3 - PROGRESS: at 36.00% examples, 152923 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:54:12,120] EPOCHs No. 3 - PROGRESS: at 37.00% examples, 152486 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:56:30,465] EPOCHs No. 3 - PROGRESS: at 38.00% examples, 151702 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 21:58:28,118] EPOCHs No. 3 - PROGRESS: at 39.00% examples, 151444 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:00:23,879] EPOCHs No. 3 - PROGRESS: at 40.00% examples, 151179 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:02:24,618] EPOCHs No. 3 - PROGRESS: at 41.00% examples, 150759 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:04:34,486] EPOCHs No. 3 - PROGRESS: at 42.00% examples, 150357 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:06:37,787] EPOCHs No. 3 - PROGRESS: at 43.00% examples, 149961 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:08:40,303] EPOCHs No. 3 - PROGRESS: at 44.00% examples, 149663 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:10:39,290] EPOCHs No. 3 - PROGRESS: at 45.00% examples, 149362 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:12:37,280] EPOCHs No. 3 - PROGRESS: at 46.00% examples, 149046 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:14:40,079] EPOCHs No. 3 - PROGRESS: at 47.00% examples, 148716 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:16:46,680] EPOCHs No. 3 - PROGRESS: at 48.00% examples, 148302 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:18:53,577] EPOCHs No. 3 - PROGRESS: at 49.00% examples, 147936 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:20:54,965] EPOCHs No. 3 - PROGRESS: at 50.00% examples, 147677 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:22:54,980] EPOCHs No. 3 - PROGRESS: at 51.00% examples, 147422 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:24:51,280] EPOCHs No. 3 - PROGRESS: at 52.00% examples, 147208 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:26:55,560] EPOCHs No. 3 - PROGRESS: at 53.00% examples, 146860 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:28:53,852] EPOCHs No. 3 - PROGRESS: at 54.00% examples, 146615 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 22:31:03,336] EPOCHs No. 3 - PROGRESS: at 55.00% examples, 146186 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:33:16,248] EPOCHs No. 3 - PROGRESS: at 56.00% examples, 145622 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:35:27,321] EPOCHs No. 3 - PROGRESS: at 57.00% examples, 145406 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 22:37:31,092] EPOCHs No. 3 - PROGRESS: at 58.00% examples, 145204 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:39:37,699] EPOCHs No. 3 - PROGRESS: at 59.00% examples, 144918 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:41:44,383] EPOCHs No. 3 - PROGRESS: at 60.00% examples, 144559 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:43:49,260] EPOCHs No. 3 - PROGRESS: at 61.00% examples, 144300 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:45:58,582] EPOCHs No. 3 - PROGRESS: at 62.00% examples, 143980 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 22:48:04,593] EPOCHs No. 3 - PROGRESS: at 63.00% examples, 143726 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:50:10,408] EPOCHs No. 3 - PROGRESS: at 64.00% examples, 143411 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-12 22:52:18,688] EPOCHs No. 3 - PROGRESS: at 65.00% examples, 143087 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:54:37,687] EPOCHs No. 3 - PROGRESS: at 66.00% examples, 142693 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:56:49,699] EPOCHs No. 3 - PROGRESS: at 67.00% examples, 142375 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 22:59:02,583] EPOCHs No. 3 - PROGRESS: at 68.00% examples, 142116 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:01:09,314] EPOCHs No. 3 - PROGRESS: at 69.00% examples, 141850 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:03:16,920] EPOCHs No. 3 - PROGRESS: at 70.00% examples, 141611 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:05:20,181] EPOCHs No. 3 - PROGRESS: at 71.00% examples, 141399 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:07:24,572] EPOCHs No. 3 - PROGRESS: at 72.00% examples, 141198 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:09:31,030] EPOCHs No. 3 - PROGRESS: at 73.00% examples, 140899 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:11:31,339] EPOCHs No. 3 - PROGRESS: at 74.00% examples, 140721 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:13:33,512] EPOCHs No. 3 - PROGRESS: at 75.00% examples, 140538 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:15:47,558] EPOCHs No. 3 - PROGRESS: at 76.00% examples, 140223 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:17:44,643] EPOCHs No. 3 - PROGRESS: at 77.00% examples, 139977 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:19:45,862] EPOCHs No. 3 - PROGRESS: at 78.00% examples, 139787 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:21:50,044] EPOCHs No. 3 - PROGRESS: at 79.00% examples, 139583 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:23:58,376] EPOCHs No. 3 - PROGRESS: at 80.00% examples, 139283 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:25:58,592] EPOCHs No. 3 - PROGRESS: at 81.00% examples, 139069 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:28:00,992] EPOCHs No. 3 - PROGRESS: at 82.00% examples, 138821 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:30:08,113] EPOCHs No. 3 - PROGRESS: at 83.00% examples, 138568 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:32:13,503] EPOCHs No. 3 - PROGRESS: at 84.00% examples, 138318 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:34:19,025] EPOCHs No. 3 - PROGRESS: at 85.00% examples, 138079 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:36:33,575] EPOCHs No. 3 - PROGRESS: at 86.00% examples, 137743 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:38:49,137] EPOCHs No. 3 - PROGRESS: at 87.00% examples, 137413 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:40:58,740] EPOCHs No. 3 - PROGRESS: at 88.00% examples, 137128 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:43:10,744] EPOCHs No. 3 - PROGRESS: at 89.00% examples, 136886 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:45:37,262] EPOCHs No. 3 - PROGRESS: at 90.00% examples, 136511 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:47:46,758] EPOCHs No. 3 - PROGRESS: at 91.00% examples, 136260 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:50:00,130] EPOCHs No. 3 - PROGRESS: at 92.00% examples, 135998 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-12 23:52:13,184] EPOCHs No. 3 - PROGRESS: at 93.00% examples, 135719 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:54:39,413] EPOCHs No. 3 - PROGRESS: at 94.00% examples, 135359 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:56:53,531] EPOCHs No. 3 - PROGRESS: at 95.00% examples, 135095 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-12 23:59:03,950] EPOCHs No. 3 - PROGRESS: at 96.00% examples, 134832 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:01:16,925] EPOCHs No. 3 - PROGRESS: at 97.00% examples, 134581 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:03:30,845] EPOCHs No. 3 - PROGRESS: at 98.00% examples, 134301 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:05:50,912] EPOCHs No. 3 - PROGRESS: at 99.00% examples, 133905 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:07:53,621] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-13 00:07:54,618] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-13 00:07:54,629] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-13 00:07:54,629] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-13 00:07:54,630] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-13 00:07:54,631] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-13 00:07:54,631] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-13 00:07:54,632] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-13 00:07:54,632] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-13 00:07:54,633] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-13 00:07:54,633] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-13 00:07:54,634] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-13 00:07:54,634] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-13 00:07:54,635] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-13 00:07:54,635] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-13 00:07:54,636] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-13 00:07:54,636] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-13 00:07:54,637] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-13 00:07:54,637] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-13 00:07:54,638] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-13 00:07:54,638] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-13 00:07:54,639] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-13 00:07:54,639] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-13 00:07:54,640] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-13 00:07:54,640] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-13 00:07:54,640] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-13 00:07:54,641] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-13 00:07:54,641] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-13 00:07:54,641] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-13 00:07:54,642] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-13 00:07:54,642] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-13 00:07:54,643] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-13 00:07:54,643] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-13 00:07:54,643] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-13 00:07:54,644] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-13 00:07:54,644] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-13 00:07:54,644] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-13 00:07:54,645] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-13 00:07:54,645] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-13 00:07:54,645] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-13 00:07:54,804] EPOCHs No. 3 - PROGRESS: at 100.00% examples, 133622 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 00:07:54,805] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-13 00:07:54,806] EPOCH - 3 : training on 1888752251 raw words (1852681566 effective words) took 13865.1s, 133622 effective words/s\n",
      "[2022-09-13 00:15:04,017] EPOCHs No. 4 - PROGRESS: at 1.00% examples, 145658 words/s, in_qsize 52, out_qsize 5\n",
      "[2022-09-13 00:18:25,792] EPOCHs No. 4 - PROGRESS: at 2.00% examples, 150311 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:23:19,324] EPOCHs No. 4 - PROGRESS: at 3.00% examples, 153858 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-13 00:27:29,937] EPOCHs No. 4 - PROGRESS: at 4.00% examples, 155626 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:31:16,928] EPOCHs No. 4 - PROGRESS: at 5.00% examples, 156587 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:34:50,779] EPOCHs No. 4 - PROGRESS: at 6.00% examples, 156884 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:38:02,892] EPOCHs No. 4 - PROGRESS: at 7.00% examples, 157360 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:41:06,493] EPOCHs No. 4 - PROGRESS: at 8.00% examples, 157653 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 00:44:06,908] EPOCHs No. 4 - PROGRESS: at 9.00% examples, 157587 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:46:56,226] EPOCHs No. 4 - PROGRESS: at 10.00% examples, 157723 words/s, in_qsize 0, out_qsize 3\n",
      "[2022-09-13 00:49:45,051] EPOCHs No. 4 - PROGRESS: at 11.00% examples, 157692 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:52:23,942] EPOCHs No. 4 - PROGRESS: at 12.00% examples, 157673 words/s, in_qsize 6, out_qsize 0\n",
      "[2022-09-13 00:54:55,416] EPOCHs No. 4 - PROGRESS: at 13.00% examples, 157696 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:57:21,343] EPOCHs No. 4 - PROGRESS: at 14.00% examples, 157564 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 00:59:45,371] EPOCHs No. 4 - PROGRESS: at 15.00% examples, 157428 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:02:08,008] EPOCHs No. 4 - PROGRESS: at 16.00% examples, 157326 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:04:24,898] EPOCHs No. 4 - PROGRESS: at 17.00% examples, 157222 words/s, in_qsize 4, out_qsize 0\n",
      "[2022-09-13 01:06:40,084] EPOCHs No. 4 - PROGRESS: at 18.00% examples, 157139 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:08:53,441] EPOCHs No. 4 - PROGRESS: at 19.00% examples, 157063 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:11:03,956] EPOCHs No. 4 - PROGRESS: at 20.00% examples, 156988 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 01:13:13,332] EPOCHs No. 4 - PROGRESS: at 21.00% examples, 156733 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:15:17,261] EPOCHs No. 4 - PROGRESS: at 22.00% examples, 156571 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:17:16,836] EPOCHs No. 4 - PROGRESS: at 23.00% examples, 156428 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:19:21,568] EPOCHs No. 4 - PROGRESS: at 24.00% examples, 156191 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:21:21,212] EPOCHs No. 4 - PROGRESS: at 25.00% examples, 156007 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 01:23:24,094] EPOCHs No. 4 - PROGRESS: at 26.00% examples, 155760 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:25:26,608] EPOCHs No. 4 - PROGRESS: at 27.00% examples, 155580 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:27:26,855] EPOCHs No. 4 - PROGRESS: at 28.00% examples, 155346 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:29:28,434] EPOCHs No. 4 - PROGRESS: at 29.00% examples, 155061 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:31:32,146] EPOCHs No. 4 - PROGRESS: at 30.00% examples, 154869 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:33:29,961] EPOCHs No. 4 - PROGRESS: at 31.00% examples, 154637 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:35:31,530] EPOCHs No. 4 - PROGRESS: at 32.00% examples, 154250 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:37:22,822] EPOCHs No. 4 - PROGRESS: at 33.00% examples, 153924 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:39:26,166] EPOCHs No. 4 - PROGRESS: at 34.00% examples, 153476 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:41:28,202] EPOCHs No. 4 - PROGRESS: at 35.00% examples, 153050 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:43:29,967] EPOCHs No. 4 - PROGRESS: at 36.00% examples, 152578 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:45:31,198] EPOCHs No. 4 - PROGRESS: at 37.00% examples, 152125 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:47:49,261] EPOCHs No. 4 - PROGRESS: at 38.00% examples, 151357 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:49:48,463] EPOCHs No. 4 - PROGRESS: at 39.00% examples, 151068 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:51:47,511] EPOCHs No. 4 - PROGRESS: at 40.00% examples, 150731 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:53:44,163] EPOCHs No. 4 - PROGRESS: at 41.00% examples, 150418 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:55:53,775] EPOCHs No. 4 - PROGRESS: at 42.00% examples, 150029 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 01:57:56,685] EPOCHs No. 4 - PROGRESS: at 43.00% examples, 149649 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 01:59:59,462] EPOCHs No. 4 - PROGRESS: at 44.00% examples, 149352 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-13 02:01:59,000] EPOCHs No. 4 - PROGRESS: at 45.00% examples, 149045 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:03:57,593] EPOCHs No. 4 - PROGRESS: at 46.00% examples, 148721 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:06:02,314] EPOCHs No. 4 - PROGRESS: at 47.00% examples, 148358 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:08:13,468] EPOCHs No. 4 - PROGRESS: at 48.00% examples, 147858 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:10:14,026] EPOCHs No. 4 - PROGRESS: at 49.00% examples, 147628 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:12:15,184] EPOCHs No. 4 - PROGRESS: at 50.00% examples, 147379 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:14:14,032] EPOCHs No. 4 - PROGRESS: at 51.00% examples, 147150 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:16:09,474] EPOCHs No. 4 - PROGRESS: at 52.00% examples, 146958 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:18:14,178] EPOCHs No. 4 - PROGRESS: at 53.00% examples, 146608 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:20:13,072] EPOCHs No. 4 - PROGRESS: at 54.00% examples, 146355 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-13 02:22:21,372] EPOCHs No. 4 - PROGRESS: at 55.00% examples, 145952 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:24:34,620] EPOCHs No. 4 - PROGRESS: at 56.00% examples, 145388 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:26:45,279] EPOCHs No. 4 - PROGRESS: at 57.00% examples, 145182 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-13 02:28:49,129] EPOCHs No. 4 - PROGRESS: at 58.00% examples, 144982 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:30:56,540] EPOCHs No. 4 - PROGRESS: at 59.00% examples, 144687 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:33:03,560] EPOCHs No. 4 - PROGRESS: at 60.00% examples, 144326 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:35:08,222] EPOCHs No. 4 - PROGRESS: at 61.00% examples, 144074 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:37:16,487] EPOCHs No. 4 - PROGRESS: at 62.00% examples, 143775 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:39:21,971] EPOCHs No. 4 - PROGRESS: at 63.00% examples, 143533 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:41:27,214] EPOCHs No. 4 - PROGRESS: at 64.00% examples, 143230 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:43:41,830] EPOCHs No. 4 - PROGRESS: at 65.00% examples, 142812 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:45:55,907] EPOCHs No. 4 - PROGRESS: at 66.00% examples, 142497 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:48:08,666] EPOCHs No. 4 - PROGRESS: at 67.00% examples, 142170 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:50:23,277] EPOCHs No. 4 - PROGRESS: at 68.00% examples, 141889 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:52:29,246] EPOCHs No. 4 - PROGRESS: at 69.00% examples, 141638 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:54:36,931] EPOCHs No. 4 - PROGRESS: at 70.00% examples, 141401 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:56:40,143] EPOCHs No. 4 - PROGRESS: at 71.00% examples, 141194 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 02:58:45,820] EPOCHs No. 4 - PROGRESS: at 72.00% examples, 140977 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:00:51,282] EPOCHs No. 4 - PROGRESS: at 73.00% examples, 140694 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:02:52,308] EPOCHs No. 4 - PROGRESS: at 74.00% examples, 140510 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:05:00,694] EPOCHs No. 4 - PROGRESS: at 75.00% examples, 140247 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:07:07,998] EPOCHs No. 4 - PROGRESS: at 76.00% examples, 140023 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:09:05,411] EPOCHs No. 4 - PROGRESS: at 77.00% examples, 139776 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-13 03:11:07,361] EPOCHs No. 4 - PROGRESS: at 78.00% examples, 139580 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:13:12,064] EPOCHs No. 4 - PROGRESS: at 79.00% examples, 139371 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:15:20,934] EPOCHs No. 4 - PROGRESS: at 80.00% examples, 139067 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:17:20,008] EPOCHs No. 4 - PROGRESS: at 81.00% examples, 138870 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:19:22,396] EPOCHs No. 4 - PROGRESS: at 82.00% examples, 138624 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 03:21:28,776] EPOCHs No. 4 - PROGRESS: at 83.00% examples, 138383 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:23:34,136] EPOCHs No. 4 - PROGRESS: at 84.00% examples, 138136 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:25:40,186] EPOCHs No. 4 - PROGRESS: at 85.00% examples, 137893 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:28:02,522] EPOCHs No. 4 - PROGRESS: at 86.00% examples, 137470 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:30:10,681] EPOCHs No. 4 - PROGRESS: at 87.00% examples, 137228 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:32:22,302] EPOCHs No. 4 - PROGRESS: at 88.00% examples, 136922 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:34:35,854] EPOCHs No. 4 - PROGRESS: at 89.00% examples, 136665 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:37:04,570] EPOCHs No. 4 - PROGRESS: at 90.00% examples, 136270 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:39:12,906] EPOCHs No. 4 - PROGRESS: at 91.00% examples, 136035 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:41:26,952] EPOCHs No. 4 - PROGRESS: at 92.00% examples, 135768 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:43:42,899] EPOCHs No. 4 - PROGRESS: at 93.00% examples, 135462 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:46:09,808] EPOCHs No. 4 - PROGRESS: at 94.00% examples, 135098 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:48:24,139] EPOCHs No. 4 - PROGRESS: at 95.00% examples, 134835 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:50:33,016] EPOCHs No. 4 - PROGRESS: at 96.00% examples, 134591 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:52:56,696] EPOCHs No. 4 - PROGRESS: at 97.00% examples, 134236 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:55:12,958] EPOCHs No. 4 - PROGRESS: at 98.00% examples, 133937 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:57:25,002] EPOCHs No. 4 - PROGRESS: at 99.00% examples, 133623 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 03:59:28,909] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-13 03:59:30,204] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-13 03:59:30,214] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-13 03:59:30,215] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-13 03:59:30,216] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-13 03:59:30,216] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-13 03:59:30,217] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-13 03:59:30,217] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-13 03:59:30,218] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-13 03:59:30,219] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-13 03:59:30,219] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-13 03:59:30,220] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-13 03:59:30,220] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-13 03:59:30,220] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-13 03:59:30,221] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-13 03:59:30,221] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-13 03:59:30,222] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-13 03:59:30,222] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-13 03:59:30,222] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-13 03:59:30,223] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-13 03:59:30,223] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-13 03:59:30,224] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-13 03:59:30,224] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-13 03:59:30,224] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-13 03:59:30,225] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-13 03:59:30,225] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-13 03:59:30,226] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-13 03:59:30,226] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-13 03:59:30,226] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-13 03:59:30,227] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-13 03:59:30,227] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-13 03:59:30,228] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-13 03:59:30,228] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-13 03:59:30,228] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-13 03:59:30,229] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-13 03:59:30,229] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-13 03:59:30,230] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-13 03:59:30,230] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-13 03:59:30,230] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-13 03:59:30,231] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-13 03:59:30,427] EPOCHs No. 4 - PROGRESS: at 100.00% examples, 133329 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 03:59:30,428] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-13 03:59:30,428] EPOCH - 4 : training on 1888752251 raw words (1852683725 effective words) took 13895.6s, 133329 effective words/s\n",
      "[2022-09-13 04:06:48,463] EPOCHs No. 5 - PROGRESS: at 1.00% examples, 142746 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:10:10,622] EPOCHs No. 5 - PROGRESS: at 2.00% examples, 148151 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:15:06,422] EPOCHs No. 5 - PROGRESS: at 3.00% examples, 151977 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:19:19,315] EPOCHs No. 5 - PROGRESS: at 4.00% examples, 153829 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 04:23:06,696] EPOCHs No. 5 - PROGRESS: at 5.00% examples, 155020 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:26:42,656] EPOCHs No. 5 - PROGRESS: at 6.00% examples, 155333 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:29:55,035] EPOCHs No. 5 - PROGRESS: at 7.00% examples, 155934 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:33:00,898] EPOCHs No. 5 - PROGRESS: at 8.00% examples, 156180 words/s, in_qsize 2, out_qsize 0\n",
      "[2022-09-13 04:35:58,237] EPOCHs No. 5 - PROGRESS: at 9.00% examples, 156455 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 04:38:47,249] EPOCHs No. 5 - PROGRESS: at 10.00% examples, 156691 words/s, in_qsize 0, out_qsize 2\n",
      "[2022-09-13 04:41:36,716] EPOCHs No. 5 - PROGRESS: at 11.00% examples, 156693 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:44:14,981] EPOCHs No. 5 - PROGRESS: at 12.00% examples, 156768 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:46:47,315] EPOCHs No. 5 - PROGRESS: at 13.00% examples, 156791 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:49:14,534] EPOCHs No. 5 - PROGRESS: at 14.00% examples, 156637 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:51:35,986] EPOCHs No. 5 - PROGRESS: at 15.00% examples, 156672 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:53:58,930] EPOCHs No. 5 - PROGRESS: at 16.00% examples, 156590 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:56:16,035] EPOCHs No. 5 - PROGRESS: at 17.00% examples, 156508 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 04:58:30,952] EPOCHs No. 5 - PROGRESS: at 18.00% examples, 156461 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-13 05:00:45,806] EPOCHs No. 5 - PROGRESS: at 19.00% examples, 156347 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:02:59,997] EPOCHs No. 5 - PROGRESS: at 20.00% examples, 156146 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:05:07,472] EPOCHs No. 5 - PROGRESS: at 21.00% examples, 155995 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:07:12,795] EPOCHs No. 5 - PROGRESS: at 22.00% examples, 155802 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:09:13,753] EPOCHs No. 5 - PROGRESS: at 23.00% examples, 155632 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:11:19,812] EPOCHs No. 5 - PROGRESS: at 24.00% examples, 155371 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:13:18,385] EPOCHs No. 5 - PROGRESS: at 25.00% examples, 155248 words/s, in_qsize 0, out_qsize 2\n",
      "[2022-09-13 05:15:21,508] EPOCHs No. 5 - PROGRESS: at 26.00% examples, 155014 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:17:27,906] EPOCHs No. 5 - PROGRESS: at 27.00% examples, 154726 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 05:19:26,440] EPOCHs No. 5 - PROGRESS: at 28.00% examples, 154569 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:21:29,042] EPOCHs No. 5 - PROGRESS: at 29.00% examples, 154273 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:23:33,943] EPOCHs No. 5 - PROGRESS: at 30.00% examples, 154066 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:25:31,784] EPOCHs No. 5 - PROGRESS: at 31.00% examples, 153851 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:27:32,535] EPOCHs No. 5 - PROGRESS: at 32.00% examples, 153508 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:29:22,749] EPOCHs No. 5 - PROGRESS: at 33.00% examples, 153230 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:31:28,145] EPOCHs No. 5 - PROGRESS: at 34.00% examples, 152744 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:33:27,729] EPOCHs No. 5 - PROGRESS: at 35.00% examples, 152401 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:35:28,317] EPOCHs No. 5 - PROGRESS: at 36.00% examples, 151975 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:37:30,277] EPOCHs No. 5 - PROGRESS: at 37.00% examples, 151517 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:39:49,746] EPOCHs No. 5 - PROGRESS: at 38.00% examples, 150731 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:41:50,965] EPOCHs No. 5 - PROGRESS: at 39.00% examples, 150406 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:43:47,982] EPOCHs No. 5 - PROGRESS: at 40.00% examples, 150132 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:45:45,163] EPOCHs No. 5 - PROGRESS: at 41.00% examples, 149818 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:47:54,979] EPOCHs No. 5 - PROGRESS: at 42.00% examples, 149439 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:49:58,438] EPOCHs No. 5 - PROGRESS: at 43.00% examples, 149059 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:52:00,485] EPOCHs No. 5 - PROGRESS: at 44.00% examples, 148789 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:54:00,350] EPOCHs No. 5 - PROGRESS: at 45.00% examples, 148486 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:56:00,545] EPOCHs No. 5 - PROGRESS: at 46.00% examples, 148140 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 05:58:07,533] EPOCHs No. 5 - PROGRESS: at 47.00% examples, 147741 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:00:13,137] EPOCHs No. 5 - PROGRESS: at 48.00% examples, 147367 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:02:16,055] EPOCHs No. 5 - PROGRESS: at 49.00% examples, 147099 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:04:18,122] EPOCHs No. 5 - PROGRESS: at 50.00% examples, 146841 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:06:18,459] EPOCHs No. 5 - PROGRESS: at 51.00% examples, 146593 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:08:16,164] EPOCHs No. 5 - PROGRESS: at 52.00% examples, 146365 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:10:22,069] EPOCHs No. 5 - PROGRESS: at 53.00% examples, 146004 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:12:21,771] EPOCHs No. 5 - PROGRESS: at 54.00% examples, 145747 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:14:36,276] EPOCHs No. 5 - PROGRESS: at 55.00% examples, 145245 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:16:43,718] EPOCHs No. 5 - PROGRESS: at 56.00% examples, 144796 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:18:54,911] EPOCHs No. 5 - PROGRESS: at 57.00% examples, 144591 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:20:59,864] EPOCHs No. 5 - PROGRESS: at 58.00% examples, 144383 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:23:06,282] EPOCHs No. 5 - PROGRESS: at 59.00% examples, 144113 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:25:14,251] EPOCHs No. 5 - PROGRESS: at 60.00% examples, 143747 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:27:18,821] EPOCHs No. 5 - PROGRESS: at 61.00% examples, 143506 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:29:27,263] EPOCHs No. 5 - PROGRESS: at 62.00% examples, 143213 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 06:31:39,863] EPOCHs No. 5 - PROGRESS: at 63.00% examples, 142868 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:33:46,809] EPOCHs No. 5 - PROGRESS: at 64.00% examples, 142549 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:35:55,522] EPOCHs No. 5 - PROGRESS: at 65.00% examples, 142232 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:38:09,721] EPOCHs No. 5 - PROGRESS: at 66.00% examples, 141925 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:40:21,838] EPOCHs No. 5 - PROGRESS: at 67.00% examples, 141617 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:42:35,141] EPOCHs No. 5 - PROGRESS: at 68.00% examples, 141363 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:44:42,368] EPOCHs No. 5 - PROGRESS: at 69.00% examples, 141102 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:46:50,628] EPOCHs No. 5 - PROGRESS: at 70.00% examples, 140864 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:48:53,798] EPOCHs No. 5 - PROGRESS: at 71.00% examples, 140665 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:50:59,987] EPOCHs No. 5 - PROGRESS: at 72.00% examples, 140448 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:53:13,203] EPOCHs No. 5 - PROGRESS: at 73.00% examples, 140068 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:55:15,671] EPOCHs No. 5 - PROGRESS: at 74.00% examples, 139873 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:57:19,214] EPOCHs No. 5 - PROGRESS: at 75.00% examples, 139682 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 06:59:28,187] EPOCHs No. 5 - PROGRESS: at 76.00% examples, 139445 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:01:27,386] EPOCHs No. 5 - PROGRESS: at 77.00% examples, 139182 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:03:28,725] EPOCHs No. 5 - PROGRESS: at 78.00% examples, 139001 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:05:34,028] EPOCHs No. 5 - PROGRESS: at 79.00% examples, 138792 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:07:43,707] EPOCHs No. 5 - PROGRESS: at 80.00% examples, 138486 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:09:45,138] EPOCHs No. 5 - PROGRESS: at 81.00% examples, 138267 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:11:48,128] EPOCHs No. 5 - PROGRESS: at 82.00% examples, 138021 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:13:54,720] EPOCHs No. 5 - PROGRESS: at 83.00% examples, 137785 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:16:07,815] EPOCHs No. 5 - PROGRESS: at 84.00% examples, 137455 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:18:15,253] EPOCHs No. 5 - PROGRESS: at 85.00% examples, 137205 words/s, in_qsize 1, out_qsize 0\n",
      "[2022-09-13 07:20:30,455] EPOCHs No. 5 - PROGRESS: at 86.00% examples, 136873 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:22:37,674] EPOCHs No. 5 - PROGRESS: at 87.00% examples, 136649 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:24:47,634] EPOCHs No. 5 - PROGRESS: at 88.00% examples, 136369 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:27:01,130] EPOCHs No. 5 - PROGRESS: at 89.00% examples, 136120 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:29:28,789] EPOCHs No. 5 - PROGRESS: at 90.00% examples, 135744 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:31:37,878] EPOCHs No. 5 - PROGRESS: at 91.00% examples, 135507 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:33:52,750] EPOCHs No. 5 - PROGRESS: at 92.00% examples, 135238 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-13 07:36:07,145] EPOCHs No. 5 - PROGRESS: at 93.00% examples, 134954 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:38:33,159] EPOCHs No. 5 - PROGRESS: at 94.00% examples, 134607 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:40:53,881] EPOCHs No. 5 - PROGRESS: at 95.00% examples, 134285 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 07:43:04,135] EPOCHs No. 5 - PROGRESS: at 96.00% examples, 134034 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:45:17,748] EPOCHs No. 5 - PROGRESS: at 97.00% examples, 133786 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 07:47:33,531] EPOCHs No. 5 - PROGRESS: at 98.00% examples, 133497 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:49:45,850] EPOCHs No. 5 - PROGRESS: at 99.00% examples, 133186 words/s, in_qsize 0, out_qsize 0\n",
      "[2022-09-13 07:51:49,937] finished iterating over Wikipedia corpus of 4730463 documents with 1888752251 positions (total 21639335 articles, 1969858045 positions before pruning articles shorter than 50 words)\n",
      "[2022-09-13 07:51:51,258] worker thread finished; awaiting finish of 39 more threads\n",
      "[2022-09-13 07:51:51,270] worker thread finished; awaiting finish of 38 more threads\n",
      "[2022-09-13 07:51:51,271] worker thread finished; awaiting finish of 37 more threads\n",
      "[2022-09-13 07:51:51,271] worker thread finished; awaiting finish of 36 more threads\n",
      "[2022-09-13 07:51:51,272] worker thread finished; awaiting finish of 35 more threads\n",
      "[2022-09-13 07:51:51,272] worker thread finished; awaiting finish of 34 more threads\n",
      "[2022-09-13 07:51:51,273] worker thread finished; awaiting finish of 33 more threads\n",
      "[2022-09-13 07:51:51,273] worker thread finished; awaiting finish of 32 more threads\n",
      "[2022-09-13 07:51:51,274] worker thread finished; awaiting finish of 31 more threads\n",
      "[2022-09-13 07:51:51,274] worker thread finished; awaiting finish of 30 more threads\n",
      "[2022-09-13 07:51:51,274] worker thread finished; awaiting finish of 29 more threads\n",
      "[2022-09-13 07:51:51,275] worker thread finished; awaiting finish of 28 more threads\n",
      "[2022-09-13 07:51:51,275] worker thread finished; awaiting finish of 27 more threads\n",
      "[2022-09-13 07:51:51,276] worker thread finished; awaiting finish of 26 more threads\n",
      "[2022-09-13 07:51:51,276] worker thread finished; awaiting finish of 25 more threads\n",
      "[2022-09-13 07:51:51,277] worker thread finished; awaiting finish of 24 more threads\n",
      "[2022-09-13 07:51:51,277] worker thread finished; awaiting finish of 23 more threads\n",
      "[2022-09-13 07:51:51,278] worker thread finished; awaiting finish of 22 more threads\n",
      "[2022-09-13 07:51:51,278] worker thread finished; awaiting finish of 21 more threads\n",
      "[2022-09-13 07:51:51,279] worker thread finished; awaiting finish of 20 more threads\n",
      "[2022-09-13 07:51:51,279] worker thread finished; awaiting finish of 19 more threads\n",
      "[2022-09-13 07:51:51,280] worker thread finished; awaiting finish of 18 more threads\n",
      "[2022-09-13 07:51:51,282] worker thread finished; awaiting finish of 17 more threads\n",
      "[2022-09-13 07:51:51,283] worker thread finished; awaiting finish of 16 more threads\n",
      "[2022-09-13 07:51:51,283] worker thread finished; awaiting finish of 15 more threads\n",
      "[2022-09-13 07:51:51,284] worker thread finished; awaiting finish of 14 more threads\n",
      "[2022-09-13 07:51:51,284] worker thread finished; awaiting finish of 13 more threads\n",
      "[2022-09-13 07:51:51,285] worker thread finished; awaiting finish of 12 more threads\n",
      "[2022-09-13 07:51:51,285] worker thread finished; awaiting finish of 11 more threads\n",
      "[2022-09-13 07:51:51,287] worker thread finished; awaiting finish of 10 more threads\n",
      "[2022-09-13 07:51:51,287] worker thread finished; awaiting finish of 9 more threads\n",
      "[2022-09-13 07:51:51,288] worker thread finished; awaiting finish of 8 more threads\n",
      "[2022-09-13 07:51:51,288] worker thread finished; awaiting finish of 7 more threads\n",
      "[2022-09-13 07:51:51,289] worker thread finished; awaiting finish of 6 more threads\n",
      "[2022-09-13 07:51:51,289] worker thread finished; awaiting finish of 5 more threads\n",
      "[2022-09-13 07:51:51,289] worker thread finished; awaiting finish of 4 more threads\n",
      "[2022-09-13 07:51:51,290] worker thread finished; awaiting finish of 3 more threads\n",
      "[2022-09-13 07:51:51,290] worker thread finished; awaiting finish of 2 more threads\n",
      "[2022-09-13 07:51:51,291] worker thread finished; awaiting finish of 1 more threads\n",
      "[2022-09-13 07:51:51,500] EPOCHs No. 5 - PROGRESS: at 100.00% examples, 132894 words/s, in_qsize 0, out_qsize 1\n",
      "[2022-09-13 07:51:51,501] worker thread finished; awaiting finish of 0 more threads\n",
      "[2022-09-13 07:51:51,501] EPOCH - 5 : training on 1888752251 raw words (1852676827 effective words) took 13941.0s, 132894 effective words/s\n",
      "[2022-09-13 07:51:51,502] training on a 9443761255 raw words (9263398437 effective words) took 69372.5s, 133531 effective words/s\n",
      "[2022-09-13 07:51:51,504] Training done.\n"
     ]
    }
   ],
   "source": [
    "logging.info('Training model %s', 'wordnet')\n",
    "model = word2vec.Word2Vec(sentences, window=10, sg=1, hs=0, negative=5, size=300, workers=40, iter=5)\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:14.273Z"
    }
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_filtered_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc1_epoch5_300_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2R_mc1_epoch5_300_filtered.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2w2v_mc1_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_con1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2S_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2B_mc100_epoch5_300_sub3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2TB_mc100_epoch5_300_LR.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2POS_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2DEP_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LRM3_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LOC_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_loc.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_pos.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_ent_w10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_dep2_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_ent_static_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_unsem.txt'\n",
    "#emb_file = '/home/manni/embs/en_wiki_spx_mc100_epoch5_300_em.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns_w1.txt'\n",
    "emb_file = '/home/manni/embs/en_wiki_wnet_epoch5_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:15.305Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:15.784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2487755"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:16.191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2487755"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.pop('[', None)\n",
    "vocab.pop(']', None)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-13T05:49:16.590Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-13 07:51:51,659] Save trained word vectors\n",
      " 37%|████████████████████████████████████████████████████████▏                                                                                             | 932086/2487755 [05:22<08:30, 3047.34it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2487755/2487755 [14:14<00:00, 2912.73it/s]\n",
      "[2022-09-13 08:06:08,453] Done\n"
     ]
    }
   ],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab, position=0):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
