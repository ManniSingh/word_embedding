{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T19:09:25.112633Z",
     "start_time": "2022-03-07T19:09:22.374580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing word2vec_inner...\n",
      "CYTHON IMPORTED SUCCESSFULLY!! (Normal)\n",
      "cbound : False\n",
      "tbound : False\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "#import wiki_old as w # old wiki\n",
    "import wiki as w # changed wiki to include '[]'\n",
    " \n",
    "#from gensim.models import word2vec # for orignal w2v\n",
    "from localgensim.gensim2.models import word2vec #remmember to change flags in word2vec.py  161-162\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models.fasttext import FastText\n",
    "#from gensim.models.word2vec import Word2Vec # not in use\n",
    "#from localgensim.gensim2.models.word2vec import Word2Vec # not in use\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream.xml.bz2'\n",
    "#WIKIXML = '/home/manni/data/wiki/enwiki-20211120-pages-articles-multistream1.xml-p1p41242.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T19:09:25.120930Z",
     "start_time": "2022-03-07T19:09:25.115923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manni/ner-s2s/word_embedding/wiki.py\n",
      "/home/manni/ner-s2s/word_embedding/localgensim/gensim2/models/word2vec.py\n"
     ]
    }
   ],
   "source": [
    "print(w.__file__)\n",
    "print(word2vec.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T19:09:25.203961Z",
     "start_time": "2022-03-07T19:09:25.123817Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T19:09:25.278014Z",
     "start_time": "2022-03-07T19:09:25.207124Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO)\n",
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:25:05.774307Z",
     "start_time": "2020-10-25T13:25:05.608721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Training model %s', 'word2vec')\n",
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=10, size=300, sample=0, \n",
    "                 workers=1, iter=1, min_count=1)\n",
    "\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:48:45.834994Z",
     "start_time": "2020-10-25T13:47:14.780675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conll corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"conll_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T13:52:12.732111Z",
     "start_time": "2020-10-25T13:48:45.838043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=1, iter=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_spx2g.txt'\n",
    "emb_file = '/mnt/nfs/resdata0/manni/wiki/conll_w2v.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip if sentence made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:32.868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-07 19:09:32,986] Parsing wiki corpus Altered...\n",
      "[2022-03-07 19:09:32,988] utils.py line 55, Pattern altered to include [ and ] ...\n",
      "[2022-03-07 19:09:34,761] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "[2022-03-07 19:10:36,051] adding document #10000 to Dictionary(612371 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:11:32,624] adding document #20000 to Dictionary(879423 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:12:19,499] adding document #30000 to Dictionary(1073861 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:13:02,325] adding document #40000 to Dictionary(1240266 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:13:37,048] adding document #50000 to Dictionary(1351778 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:13:55,208] adding document #60000 to Dictionary(1377046 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:14:11,209] adding document #70000 to Dictionary(1403071 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:14:26,778] adding document #80000 to Dictionary(1424339 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:15:03,668] adding document #90000 to Dictionary(1530944 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:15:47,143] adding document #100000 to Dictionary(1671225 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:16:23,993] adding document #110000 to Dictionary(1787566 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:17:00,074] adding document #120000 to Dictionary(1894031 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:17:34,599] adding document #130000 to Dictionary(1988802 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:18:13,658] discarding 92116 tokens: [('anteayer', 1), ('antié', 1), ('babucha', 1), ('bailaor', 1), ('bailarín', 1), ('bofetada', 1), ('callaos', 1), ('callaros', 1), ('callarse', 1), ('ceceante', 1)]...\n",
      "[2022-03-07 19:18:13,661] keeping 2000000 tokens which were in no less than 0 and no more than 140000 (=100.0%) documents\n",
      "[2022-03-07 19:18:17,893] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:18:17,929] adding document #140000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:18:54,027] discarding 114125 tokens: [('vrodado', 1), ('vuccino#E', 1), ('σῦρος', 1), ('aarrejärvi#E', 1), ('sadeoja', 1), ('sadeoja#E', 1), ('countermobility', 1), ('oilstones', 1), ('beirget', 1), ('dripsy', 1)]...\n",
      "[2022-03-07 19:18:54,029] keeping 2000000 tokens which were in no less than 0 and no more than 150000 (=100.0%) documents\n",
      "[2022-03-07 19:18:57,286] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:18:57,338] adding document #150000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:19:34,476] discarding 107703 tokens: [('charbonnage', 1), ('deliverd', 1), ('ilsede', 1), ('kinkepois', 1), ('kleiputter', 1), ('meuneries', 1), ('métallurgie', 1), ('stoomlocomotief#E', 1), ('stoomspoorlijn', 1), ('stoomvereniging', 1)]...\n",
      "[2022-03-07 19:19:34,478] keeping 2000000 tokens which were in no less than 0 and no more than 160000 (=100.0%) documents\n",
      "[2022-03-07 19:19:37,702] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:19:37,753] adding document #160000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:20:11,413] discarding 96671 tokens: [('montgolfières', 1), ('rmcsj', 1), ('scarpellino#E', 1), ('adve', 1), ('ahamad', 1), ('conit', 1), ('conits', 1), ('gharachorloo', 1), ('ledgered', 1), ('membar', 1)]...\n",
      "[2022-03-07 19:20:11,416] keeping 2000000 tokens which were in no less than 0 and no more than 170000 (=100.0%) documents\n",
      "[2022-03-07 19:20:14,713] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:20:14,763] adding document #170000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:20:50,506] discarding 77192 tokens: [('phragmacia#E', 1), ('robertsi', 1), ('scepomycter#E', 1), ('schistolais#E', 1), ('spiloptila', 1), ('spiloptila#E', 1), ('subcinnamomea', 1), ('substriata', 1), ('tailorbirds', 1), ('urolais', 1)]...\n",
      "[2022-03-07 19:20:50,508] keeping 2000000 tokens which were in no less than 0 and no more than 180000 (=100.0%) documents\n",
      "[2022-03-07 19:20:53,763] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:20:53,813] adding document #180000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:21:24,824] discarding 89699 tokens: [('slsk', 1), ('solarseek', 1), ('soulseek', 1), ('soulseekqt', 1), ('soulseex', 1), ('abbotsham#E', 1), ('cambeak', 1), ('carnsew', 1), ('carvannel', 1), ('chiselbury#E', 1)]...\n",
      "[2022-03-07 19:21:24,827] keeping 2000000 tokens which were in no less than 0 and no more than 190000 (=100.0%) documents\n",
      "[2022-03-07 19:21:28,065] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:21:28,115] adding document #190000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:21:58,056] discarding 85540 tokens: [('노동자의', 1), ('노예의', 1), ('높으신', 1), ('다만', 1), ('단김에', 1), ('대지의', 1), ('던져라', 1), ('덩어리는', 1), ('도살자에게는', 1), ('되리', 1)]...\n",
      "[2022-03-07 19:21:58,059] keeping 2000000 tokens which were in no less than 0 and no more than 200000 (=100.0%) documents\n",
      "[2022-03-07 19:22:01,374] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:22:01,424] adding document #200000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:22:32,842] discarding 77375 tokens: [('bethausen#E', 1), ('birda#E', 1), ('boldur#E', 1), ('bârna#E', 1), ('chevereșu#E', 1), ('comloșu#E', 1), ('comtim#E', 1), ('coșteiu#E', 1), ('cărpiniș#E', 1), ('darova#E', 1)]...\n",
      "[2022-03-07 19:22:32,847] keeping 2000000 tokens which were in no less than 0 and no more than 210000 (=100.0%) documents\n",
      "[2022-03-07 19:22:36,610] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:22:36,691] adding document #210000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:23:07,746] discarding 74808 tokens: [('encliandra#E', 1), ('eufuchsia', 1), ('excorticata#E', 1), ('fuchsiae', 1), ('fuchsiavrienden', 1), ('fuchsietum', 1), ('fushia', 1), ('garleppiana#E', 1), ('gehrigeri#E', 1), ('glazioviana#E', 1)]...\n",
      "[2022-03-07 19:23:07,758] keeping 2000000 tokens which were in no less than 0 and no more than 220000 (=100.0%) documents\n",
      "[2022-03-07 19:23:11,026] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:23:11,077] adding document #220000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:23:42,006] discarding 80208 tokens: [('hantik', 1), ('hantique', 1), ('hantík', 1), ('igpasungaw', 1), ('iraynun', 1), ('kairawan', 1), ('kangaranan', 1), ('kapaligiran', 1), ('lublub#E', 1), ('lumboyan', 1)]...\n",
      "[2022-03-07 19:23:42,010] keeping 2000000 tokens which were in no less than 0 and no more than 230000 (=100.0%) documents\n",
      "[2022-03-07 19:23:45,275] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:23:45,362] adding document #230000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:24:14,275] discarding 79220 tokens: [('partay#E', 1), ('smiirnoff', 1), ('империалъ', 1), ('смирновъ', 1), ('титулъ', 1), ('althusserian#E', 1), ('overaccumulated', 1), ('galic#E', 1), ('gaëlic', 1), ('hearson#E', 1)]...\n",
      "[2022-03-07 19:24:14,279] keeping 2000000 tokens which were in no less than 0 and no more than 240000 (=100.0%) documents\n",
      "[2022-03-07 19:24:17,917] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-03-07 19:24:18,013] adding document #240000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:24:47,433] discarding 71867 tokens: [('caeserion', 1), ('paliama', 1), ('volcatius', 1), ('gbordzoe', 1), ('pappoe', 1), ('ptglive', 1), ('uniiq#E', 1), ('araquel', 1), ('flitling', 1), ('leanen', 1)]...\n",
      "[2022-03-07 19:24:47,437] keeping 2000000 tokens which were in no less than 0 and no more than 250000 (=100.0%) documents\n",
      "[2022-03-07 19:24:51,052] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:24:51,105] adding document #250000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:25:20,583] discarding 77267 tokens: [('mirologhia', 1), ('nicodorus#E', 1), ('pandouris#E', 1), ('patinadha', 1), ('simiades', 1), ('syrto#E', 1), ('privatizacion', 1), ('bubbins', 1), ('cdbgpd', 1), ('jasmcd', 1)]...\n",
      "[2022-03-07 19:25:20,587] keeping 2000000 tokens which were in no less than 0 and no more than 260000 (=100.0%) documents\n",
      "[2022-03-07 19:25:25,314] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:25:25,410] adding document #260000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:25:55,575] discarding 71276 tokens: [('ipotesi#E', 1), ('megagamma', 1), ('rcshift', 1), ('spicup', 1), ('blipped', 1), ('hughesairwest', 1), ('sundowe#E', 1), ('hashpa', 1), ('mashpia', 1), ('strassfeld#E', 1)]...\n",
      "[2022-03-07 19:25:55,578] keeping 2000000 tokens which were in no less than 0 and no more than 270000 (=100.0%) documents\n",
      "[2022-03-07 19:25:59,238] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:25:59,307] adding document #270000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:26:32,110] discarding 79565 tokens: [('muhammedanische', 1), ('sprachgelehrsamkeit', 1), ('amidostomum#E', 1), ('aramidae', 1), ('campeloma#E', 1), ('carau#E', 1), ('courlan', 1), ('cowperiana#E', 1), ('dolosus', 1), ('elucus', 1)]...\n",
      "[2022-03-07 19:26:32,114] keeping 2000000 tokens which were in no less than 0 and no more than 280000 (=100.0%) documents\n",
      "[2022-03-07 19:26:35,695] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:26:35,789] adding document #280000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:27:03,989] discarding 78485 tokens: [('libona', 1), ('limbay', 1), ('lumuluyaw', 1), ('lunhaw', 1), ('lunocan', 1), ('maagnaw', 1), ('maapag', 1), ('madagway', 1), ('magubo', 1), ('makupal', 1)]...\n",
      "[2022-03-07 19:27:03,992] keeping 2000000 tokens which were in no less than 0 and no more than 290000 (=100.0%) documents\n",
      "[2022-03-07 19:27:07,537] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:27:07,626] adding document #290000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:27:33,256] discarding 73399 tokens: [('jalalia', 1), ('qasmia', 1), ('walshale', 1), ('olindavirtual', 1), ('paratibe#E', 1), ('jamesson#E', 1), ('bowleggedness#E', 1), ('bronca', 1), ('kabuscorp', 1), ('kabuscorp#E', 1)]...\n",
      "[2022-03-07 19:27:33,259] keeping 2000000 tokens which were in no less than 0 and no more than 300000 (=100.0%) documents\n",
      "[2022-03-07 19:27:37,084] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:27:37,165] adding document #300000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:28:04,392] discarding 75768 tokens: [('rīšat', 1), ('sadarnunna', 1), ('sargonids#E', 1), ('weierhäuser', 1), ('weiershäuser', 1), ('weiherhäuser', 1), ('yatribu', 1), ('bistricë#E', 1), ('devolli', 1), ('erseka#E', 1)]...\n",
      "[2022-03-07 19:28:04,393] keeping 2000000 tokens which were in no less than 0 and no more than 310000 (=100.0%) documents\n",
      "[2022-03-07 19:28:07,864] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:28:07,955] adding document #310000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:28:35,729] discarding 80637 tokens: [('antithamnion#E', 1), ('bluebills', 1), ('callithamnion#E', 1), ('catbriar#E', 1), ('clotbur#E', 1), ('cunner', 1), ('fraudwater', 1), ('grinnellia#E', 1), ('neosiphonia#E', 1), ('phymatolithon#E', 1)]...\n",
      "[2022-03-07 19:28:35,732] keeping 2000000 tokens which were in no less than 0 and no more than 320000 (=100.0%) documents\n",
      "[2022-03-07 19:28:39,770] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:28:39,855] adding document #320000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:29:05,805] discarding 68776 tokens: [('yanushkevich#E', 1), ('zhigarev#E', 1), ('ставка', 1), ('aardbei', 1), ('allambique', 1), ('artisanaal#E', 1), ('beerstyle', 1), ('bezomerd', 1), ('coigneau#E', 1), ('druif', 1)]...\n",
      "[2022-03-07 19:29:05,808] keeping 2000000 tokens which were in no less than 0 and no more than 330000 (=100.0%) documents\n",
      "[2022-03-07 19:29:09,932] resulting dictionary: Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n",
      "[2022-03-07 19:29:10,015] adding document #330000 to Dictionary(2000000 unique tokens: ['[', ']', 'a', 'a#E', 'ability']...)\n"
     ]
    }
   ],
   "source": [
    "# loc = 'num'|'lr'|'ent'\n",
    "# pos = True|False\n",
    "# download latest wiki dump\n",
    "#w.download_wiki_dump('en', WIKIXML)\n",
    "\n",
    "# parse wiki dump\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',lower=True) # Orignal\n",
    "wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='EM',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='DEP',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNS',lower=True,pos=False,loc=False)\n",
    "#wiki_sentences = w.WikiSentences(WIKIXML, 'en',tokenizer_func='UNSEM',lower=True,pos=False,loc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:34.083Z"
    }
   },
   "outputs": [],
   "source": [
    "#sv.save(wiki_sentences,\"wiki_sentences_pos_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_pos\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_loc\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_sp_ent_sample\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences\") # orignal\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_dep2\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_uns\")\n",
    "#sv.save(wiki_sentences,\"wiki_sentences_unsem\")\n",
    "sv.save(wiki_sentences,\"wiki_sentences_em\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phrase mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:50:46.815932Z",
     "start_time": "2021-05-29T18:50:46.810808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T22:24:31.858045Z",
     "start_time": "2021-05-29T18:51:21.445049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences, min_count=100, threshold=1)\n",
    "frozen_phrases = phrases.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T11:41:54.697897Z",
     "start_time": "2021-05-30T11:41:38.608475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(phrases,\"gensim_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T22:06:13.397797Z",
     "start_time": "2020-03-16T22:06:13.394080Z"
    }
   },
   "source": [
    "# Train procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:38.481Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentences = sv.load(\"wiki_sentences_no\")\n",
    "#temp_sens are cased!!\n",
    "#sentences = sv.load(\"temp_sens\")\n",
    " \n",
    "#sentences = sv.load(\"wiki_sentences\") #Normal sentences using wiki_old.py\n",
    "\n",
    "#Wiki_Sentences_SP are cased\n",
    "#sentences = sv.load(\"Wiki_Sentences_SP\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_loc\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_sp\") #New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_pos\") # not to be used\n",
    "#sentences = sv.load(\"Wiki_sentences_pos_sample\")\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent\") # New\n",
    "#sentences = sv.load(\"wiki_sentences_sp_ent_sample\") # New\n",
    "\n",
    "#sentences = sv.load(\"wiki_sentences_dep\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_dep2\") #New\n",
    "\n",
    "#wiki english sample Cased \n",
    "#sentences = sv.load(\"Wiki_sentences_sp_sample\")\n",
    "#sentences = sv.load(\"wiki_sentences_uns\") #New\n",
    "#sentences = sv.load(\"wiki_sentences_unsem\") #New\n",
    "sentences = sv.load(\"wiki_sentences_em\") #New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:38.952Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:40.670Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Training model %s', 'spxM100EMw5')\n",
    "model = word2vec.Word2Vec(sentences, cbound=False, tbound=False, bound_type='lr', window=5, sg=1, hs=0, negative=5, size=300, sample=1e-3, workers=40, iter=5, min_count=100)\n",
    "#model = word2vec.Word2Vec(sentences, window=1, sg=1, hs=0, negative=5, size=300, sample=1e-3, workers=40, iter=5, min_count=100)\n",
    "logging.info('Training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:42.557Z"
    }
   },
   "outputs": [],
   "source": [
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_filtered_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc1_epoch5_300_sample.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2R_mc1_epoch5_300_filtered.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2w2v_mc1_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_con1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc1_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_mc100_epoch5_300_neg10_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2S_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2B_mc100_epoch5_300_sub3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2TB_mc100_epoch5_300_LR.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2POS_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2DEP_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LRM3_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2LOC_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_reversed.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_loc.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_w2v_mc100_epoch5_300_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_pos.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx_mc100_epoch5_300_ent_w10.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_dep2_w1.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_ent_static_w3.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_unsem.txt'\n",
    "emb_file = '/home/manni/embs/en_wiki_spx_mc100_epoch5_300_em.txt'\n",
    "#emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_spx2_mc100_epoch5_300_uns_w1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:43.544Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:43.986Z"
    }
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:44.500Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab.pop('[', None)\n",
    "vocab.pop(']', None)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:09:45.796Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab, position=0):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ = dict()\n",
    "for word,obj in model.wv.vocab.items():\n",
    "    word_ = word.split('#')\n",
    "    if len(word_) < 2:\n",
    "        vocab_[word] = obj\n",
    "        continue \n",
    "    if int(word_[-1]) > 5:\n",
    "        continue\n",
    "    vocab_[word]=obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-15T16:04:50.575Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-15T16:05:20.903Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vocab), 300))\n",
    "    for word in tqdm(vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T05:27:29.780749Z",
     "start_time": "2021-06-15T05:27:29.698465Z"
    }
   },
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:47.172484Z",
     "start_time": "2021-02-25T06:15:45.664705Z"
    }
   },
   "outputs": [],
   "source": [
    "#Wiki_Phrase_Vocab are cased phrases => length 2\n",
    "phrase_vocab = sv.load(\"Wiki_Phrase_Vocab\")\n",
    "#wiki_phrase_vocab are lower cased phrases => length 2\n",
    "#phrase_vocab = sv.load(\"wiki_phrase_vocab\")\n",
    "#bi_phrase are only bi-grams\n",
    "#phrase_vocab = sv.load(\"bi_phrase\")\n",
    "#Phrase vocab filtered with first letter capital from Wikipedia\n",
    "#phrase_vocab = sv.load(\"filtered_phrase_vocab_freq\")\n",
    "#phrase_vocab = sv.load(\"filtered_phrase_vocab_freq_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:47.193022Z",
     "start_time": "2021-02-25T06:15:47.177032Z"
    }
   },
   "outputs": [],
   "source": [
    "len(phrase_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:51.532986Z",
     "start_time": "2021-02-25T06:15:47.194688Z"
    }
   },
   "outputs": [],
   "source": [
    "common_vocab = set(model.wv.vocab) & set(phrase_vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:15:51.541444Z",
     "start_time": "2021-02-25T06:15:51.536009Z"
    }
   },
   "outputs": [],
   "source": [
    "len(common_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:26:24.941776Z",
     "start_time": "2021-02-25T06:15:51.543819Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save X-trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(common_vocab), 300))\n",
    "    for word in tqdm(common_vocab, position=0, leave=True):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-12T12:25:27.820Z"
    }
   },
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T14:10:48.114Z"
    }
   },
   "outputs": [],
   "source": [
    "len(phrase_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T15:45:36.718717Z",
     "start_time": "2020-11-22T15:45:36.714446Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T15:46:25.556163Z",
     "start_time": "2020-11-22T15:45:37.403827Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_phrase_vocab_freq = defaultdict(int)\n",
    "for sent in tqdm(sentences, position=0, leave=True):\n",
    "    lensen = len(sent)\n",
    "    i = 0\n",
    "    try:\n",
    "        while i<lensen:\n",
    "            if sent[i]=='[':\n",
    "                i+=1\n",
    "                while sent[i]!=']':\n",
    "                    if sent[i][0].isupper() and sent[i][1:].islower():\n",
    "                        filtered_phrase_vocab_freq[sent[i]]+=1\n",
    "                    i+=1\n",
    "            else:\n",
    "                i+=1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T15:46:25.662163Z",
     "start_time": "2020-11-22T15:46:25.560551Z"
    }
   },
   "outputs": [],
   "source": [
    "sv.save(filtered_phrase_vocab_freq,\"filtered_phrase_vocab_freq_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save(filtered_phrase_vocab_freq,\"filtered_phrase_vocab_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_phrase_vocab_10 = set()\n",
    "for word,freq in filtered_phrase_vocab_freq.items():\n",
    "    if freq >=10:\n",
    "        filtered_phrase_vocab_10.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_phrase_vocab_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_emb_file_10 = '/mnt/nfs/resdata0/manni/wiki/en_wiki_SPX2_r_mc10_epoch5_300_filtered.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(filt_emb_file_10, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(filtered_phrase_vocab_10), 300))\n",
    "    for word in tqdm(filtered_phrase_vocab_10,position=0, leave=True):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:24:15.811039Z",
     "start_time": "2020-07-31T14:16:15.627353Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:24:15.816736Z",
     "start_time": "2020-07-31T14:24:15.812984Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total trainting time:\",model.total_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:24:40.090027Z",
     "start_time": "2020-07-31T14:24:15.818151Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"/mnt/nfs/resdata0/manni/wiki/no_sp_r_m1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:17:22.908996Z",
     "start_time": "2020-06-19T10:35:51.695Z"
    }
   },
   "outputs": [],
   "source": [
    "min100vocab = sv.load(\"en_vocab_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:17:22.910337Z",
     "start_time": "2020-06-19T10:35:55.889Z"
    }
   },
   "outputs": [],
   "source": [
    "new_emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_skip2_r_mc100_epoch5_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T09:17:22.911527Z",
     "start_time": "2020-06-19T10:35:57.226Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(new_emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(min100vocab), 300))\n",
    "    for word in tqdm(min100vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ngam testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T20:10:55.241574Z",
     "start_time": "2021-02-27T20:08:21.513741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams = sv.load(\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T21:46:11.984024Z",
     "start_time": "2021-02-27T21:44:40.578809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngram_counts = dict()\n",
    "for k,ngs in ngrams.items():\n",
    "    new_dict = dict()\n",
    "    for p in ngs:\n",
    "        for w in p:\n",
    "            w = w.lower()\n",
    "            if w in new_dict:\n",
    "                new_dict[w] += 1\n",
    "            else:\n",
    "                new_dict[w] = 1\n",
    "    ngram_counts[k]=new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T22:09:21.874134Z",
     "start_time": "2021-02-27T22:09:18.533627Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# merging on lowercase\n",
    "ng_counts = dict()\n",
    "for _k,ngs in ngram_counts.items():\n",
    "    k = _k.lower()\n",
    "    if k in ng_counts:\n",
    "        for word,count in ngs.items():\n",
    "            if word in ng_counts[k]:\n",
    "                ng_counts[k][word]+=count\n",
    "            else:\n",
    "                ng_counts[k][word]=count\n",
    "    else:\n",
    "        ng_counts[k] = ngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T22:09:33.055414Z",
     "start_time": "2021-02-27T22:09:25.213606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sv.save(ng_counts,\"ngram_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T22:10:00.242755Z",
     "start_time": "2021-02-27T22:10:00.236981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(ngram_counts))\n",
    "print(len(ng_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T10:38:08.600385Z",
     "start_time": "2020-06-21T10:38:04.780273Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "min10vocab = sv.load(\"en_vocab_min10_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T20:38:35.678667Z",
     "start_time": "2020-07-05T20:37:39.162620Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"/mnt/nfs/resdata0/manni/wiki/skip4_m1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T10:39:54.872501Z",
     "start_time": "2020-06-21T10:39:54.867737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_emb_file = '/mnt/nfs/resdata0/manni/wiki/en_wiki_sp_r_mc10_epoch5_300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T10:45:21.624290Z",
     "start_time": "2020-06-21T10:39:54.875449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open(new_emb_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(min10vocab), 300))\n",
    "    for word in tqdm(min10vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SPhrase test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    ">>>\n",
    ">>> texts = [['human', 'interface', 'computer']]\n",
    ">>> dct = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = w.WikiSentences(WIKIXML, \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T21:36:19.157874Z",
     "start_time": "2020-06-13T21:36:18.682474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = sv.load(\"wiki_sentences_sp_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T21:36:19.660635Z",
     "start_time": "2020-06-13T21:36:19.655291Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T09:27:49.908959Z",
     "start_time": "2020-06-14T09:18:46.079637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, window=5, sg=1, hs=0, negative=5, size=300, sample=0, workers=40, iter=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T09:29:54.494935Z",
     "start_time": "2020-06-14T09:27:49.911300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logging.info('Save trained word vectors')\n",
    "with open('/mnt/nfs/resdata0/manni/wiki/en_wiki_sp_test_300.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(model.wv.vocab), 300))\n",
    "    for word in tqdm(model.wv.vocab):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in model.wv[word]])))\n",
    "logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
