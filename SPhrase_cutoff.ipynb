{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "sys.path.append(\"../\")\n",
    "import saver as sv\n",
    "import data_utils_conv as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO)\n",
    "os.makedirs('data/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sv.load(\"wiki_sentences_sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum length of token:\",sentences.wiki.token_min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence[:15])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from localgensim.gensim2 import utils\n",
    "from collections import defaultdict \n",
    "from six import string_types,iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vocab = None\n",
    "max_vocab_size = None\n",
    "\n",
    "def _scan_vocab(sentences, progress_per, trim_rule):\n",
    "        global raw_vocab, max_vocab_size\n",
    "        sentence_no = -1\n",
    "        total_words = 0\n",
    "        min_reduce = 1\n",
    "        vocab = defaultdict(int)\n",
    "        checked_string_types = 0\n",
    "        for sentence_no, sentence in enumerate(sentences):\n",
    "            if not checked_string_types:\n",
    "                if isinstance(sentence, string_types):\n",
    "                    logging.warning(\n",
    "                        \"Each 'sentences' item should be a list of words (usually unicode strings). \"\n",
    "                        \"First item here is instead plain %s.\",\n",
    "                        type(sentence)\n",
    "                    )\n",
    "                checked_string_types += 1\n",
    "            if sentence_no % progress_per == 0:\n",
    "                logging.info(\n",
    "                    \"PROGRESS: at sentence #%i, processed %i words, keeping %i word types\",\n",
    "                    sentence_no, total_words, len(vocab)\n",
    "                )\n",
    "            for word in sentence:\n",
    "                vocab[word] += 1\n",
    "            total_words += len(sentence)\n",
    "\n",
    "            if max_vocab_size and len(vocab) > max_vocab_size:\n",
    "                utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)\n",
    "                min_reduce += 1\n",
    "\n",
    "        corpus_count = sentence_no + 1\n",
    "        raw_vocab = vocab\n",
    "        return total_words, corpus_count\n",
    "\n",
    "def scan_vocab(sentences=None, progress_per=100000, trim_rule=None):\n",
    "        logging.info(\"collecting all words and their counts\")\n",
    "\n",
    "        total_words, corpus_count = _scan_vocab(sentences, progress_per, trim_rule)\n",
    "\n",
    "        logging.info(\n",
    "            \"collected %i word types from a corpus of %i raw words and %i sentences\",\n",
    "            len(raw_vocab), total_words, corpus_count\n",
    "        )\n",
    "\n",
    "        return total_words, corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words, corpus_count = scan_vocab(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save(raw_vocab,\"en_raw_vocab_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from localgensim.gensim2.models.keyedvectors import Vocab\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "sample = False\n",
    "drop_total = drop_unique = 0\n",
    "update = False\n",
    "trim_rule = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not update:\n",
    "    logging.info(\"Loading a fresh vocabulary\")\n",
    "    retain_total, retain_words = 0, []\n",
    "    # Discard words less-frequent than min_count\n",
    "    index2word = []\n",
    "    # make stored settings match these applied settings\n",
    "    vocab = {}\n",
    "    effective_min_count = min_count\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        if utils.keep_vocab_item(word, v, effective_min_count, trim_rule=trim_rule):\n",
    "            retain_words.append(word)\n",
    "            retain_total += v\n",
    "            vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "            index2word.append(word)\n",
    "        else:\n",
    "            drop_unique += 1\n",
    "            drop_total += v\n",
    "    original_unique_total = len(retain_words) + drop_unique\n",
    "    retain_unique_pct = len(retain_words) * 100 / max(original_unique_total, 1)\n",
    "    logging.info(\"effective_min_count=%d retains %i unique words (%i%% of original %i, drops %i)\",\n",
    "                 effective_min_count, len(retain_words), retain_unique_pct, original_unique_total, drop_unique)\n",
    "    original_total = retain_total + drop_total\n",
    "    retain_pct = retain_total * 100 / max(original_total, 1)\n",
    "    logging.info(\"effective_min_count=%d leaves %i word corpus (%i%% of original %i, drops %i)\",\n",
    "                 effective_min_count, retain_total, retain_pct, original_total, drop_total)\n",
    "else:\n",
    "    logging.info(\"Updating model with new vocabulary\")\n",
    "    new_total = pre_exist_total = 0\n",
    "    new_words = pre_exist_words = []\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        if utils.keep_vocab_item(word, v, effective_min_count, trim_rule=trim_rule):\n",
    "            if word in vocab:\n",
    "                pre_exist_words.append(word)\n",
    "                pre_exist_total += v\n",
    "                vocab[word].count += v\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "                new_total += v\n",
    "                vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "                index2word.append(word)\n",
    "        else:\n",
    "            drop_unique += 1\n",
    "            drop_total += v\n",
    "    original_unique_total = len(pre_exist_words) + len(new_words) + drop_unique\n",
    "    pre_exist_unique_pct = len(pre_exist_words) * 100 / max(original_unique_total, 1)\n",
    "    new_unique_pct = len(new_words) * 100 / max(original_unique_total, 1)\n",
    "    logging.info(\"New added %i unique words (%i%% of original %i) \"\n",
    "                 \"and increased the count of %i pre-existing words (%i%% of original %i)\",\n",
    "                 len(new_words), new_unique_pct, original_unique_total, len(pre_exist_words),\n",
    "                 pre_exist_unique_pct, original_unique_total)\n",
    "    retain_words = new_words + pre_exist_words\n",
    "    retain_total = new_total + pre_exist_total\n",
    "\n",
    "# Precalculate each vocabulary item's threshold for sampling\n",
    "if not sample:\n",
    "    # no words downsampled\n",
    "    logging.info(\"NO DOWNSAMPLING\")\n",
    "    threshold_count = retain_total\n",
    "elif sample < 1.0:\n",
    "    # traditional meaning: set parameter as proportion of total\n",
    "    threshold_count = sample * retain_total\n",
    "else:\n",
    "    # new shorthand: sample >= 1 means downsample all words with higher count than sample\n",
    "    threshold_count = int(sample * (3 + sqrt(5)) / 2)\n",
    "\n",
    "downsample_total, downsample_unique = 0, 0\n",
    "for w in retain_words:\n",
    "    v = raw_vocab[w]\n",
    "    word_probability = (sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "    if word_probability < 1.0:\n",
    "        downsample_unique += 1\n",
    "        downsample_total += word_probability * v\n",
    "    else:\n",
    "        word_probability = 1.0\n",
    "        downsample_total += v\n",
    "    vocab[w].sample_int = int(round(word_probability * 2**32))\n",
    "\n",
    "\n",
    "logging.info(\"deleting the raw counts dictionary of %i items\", len(raw_vocab))\n",
    "raw_vocab = defaultdict(int)\n",
    "\n",
    "logging.info(\"sample=%g downsamples %i most-common words\", sample, downsample_unique)\n",
    "logging.info(\"downsampling leaves estimated %i word corpus (%.1f%% of prior %i)\",\n",
    "             downsample_total, downsample_total * 100.0 / max(retain_total, 1), retain_total)\n",
    "\n",
    "# return from each step: words-affected, resulting-corpus-size, extra memory estimates\n",
    "report_values = {\n",
    "    'drop_unique': drop_unique, 'retain_total': retain_total, 'downsample_unique': downsample_unique,\n",
    "    'downsample_total': int(downsample_total), 'num_retained_words': len(retain_words)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save(vocab,\"en_vocab_min10_gensim\")\n",
    "sv.save(index2word,\"en_index2word_min10_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict()\n",
    "with open (\"../hp.json\", \"r\") as jfile:\n",
    "    hparams=eval(jfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 26869 tokens\n",
      "Total Singulars are: 18350\n",
      "Words found in pre-training are: 18094\n",
      "Words NOT found in pre-training are: 8775\n",
      "Singulars not in pre-training are: 6946\n"
     ]
    }
   ],
   "source": [
    "vocab_pre =  set(sv.load(\"en_vocab_gensim\").keys())\n",
    "#vocab_pre =  set(sv.load(\"en_vocab_min10_gensim\").keys())\n",
    "\n",
    "\n",
    "processing_word = du.get_processing_word(lowercase=hparams['lowercase'])\n",
    "\n",
    "dev   = du.CoNLLDataset(hparams['dev_filename'], processing_word)\n",
    "test  = du.CoNLLDataset(hparams['test_filename'], processing_word)\n",
    "train = du.CoNLLDataset(hparams['train_filename'], processing_word)\n",
    "    \n",
    "\n",
    "# Build Word and Tag vocab\n",
    "vocab_words, vocab_tags, singulars = du.get_vocabs([train,test,dev])\n",
    "print(\"Total Singulars are: \" + str(len(singulars)))\n",
    "\n",
    "in_pre = vocab_pre & vocab_words\n",
    "print(\"Words found in pre-training are: \"+str(len(in_pre)))\n",
    "not_in_pre = vocab_words - in_pre\n",
    "print(\"Words NOT found in pre-training are: \"+str(len(not_in_pre)))\n",
    "\n",
    "sni = set()\n",
    "sni.update(singulars)\n",
    "for sing in singulars:\n",
    "    if sing in in_pre:\n",
    "        sni.remove(sing)\n",
    "print(\"Singulars not in pre-training are: \"+str(len(sni)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
