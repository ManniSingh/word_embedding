{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05757692",
   "metadata": {},
   "source": [
    "# Babelnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab33266d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T10:45:15.066297Z",
     "start_time": "2023-01-28T10:45:15.034097Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../imports/\")\n",
    "import saver as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "8fa0e75b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T10:24:54.078950Z",
     "start_time": "2023-02-01T10:24:54.072562Z"
    }
   },
   "outputs": [],
   "source": [
    "import babelnet as bn\n",
    "from babelnet import Language\n",
    "from babelnet.data.source import BabelSenseSource\n",
    "from babelnet.resources import BabelSynsetID\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1faf7e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "56f017dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:14:12.406673Z",
     "start_time": "2023-02-05T09:14:12.402522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ws353A = '/home/manni/data/wordsim/EN-WS353.out'\n",
    "ws353R = '/home/manni/data/wordsim/EN-WSR353.out'\n",
    "ws353S = '/home/manni/data/wordsim/EN-WSS353.out'\n",
    "rw = '/home/manni/data/wordsim/rw.out'\n",
    "sim999 = '/home/manni/data/wordsim/EN-SIM999.out'\n",
    "turk = '/home/manni/data/wordsim/EN_TRUK.txt'\n",
    "mturk = '/home/manni/data/wordsim/MTURK-771.out'\n",
    "rg = '/home/manni/data/wordsim/EN-RG-65.txt'\n",
    "men = '/home/manni/data/wordsim/EN-MEN-LEM.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "9f227080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:14:12.499908Z",
     "start_time": "2023-02-05T09:14:12.496916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "datasets = [ws353A,sim999,rg,men]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "27970230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:14:24.959562Z",
     "start_time": "2023-02-05T09:14:14.377961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for ds in datasets:\n",
    "    with open(ds) as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in lines:\n",
    "            line=line.split()\n",
    "            if len(line)!=3:\n",
    "                continue\n",
    "            vocab.add(line[0].lower())\n",
    "            vocab.add(line[1].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865dfa5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# extract synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac15572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T08:09:06.635712Z",
     "start_time": "2023-01-25T08:09:06.627098Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_syn_data(synset):\n",
    "    '''\n",
    "    Returns a dict conainting,\n",
    "    0. wn offset (str)\n",
    "    1. translations (set)\n",
    "    2. gloss (str)\n",
    "    3. wiki title (str)\n",
    "    4. languages (list of str)\n",
    "    '''\n",
    "    dat = list()\n",
    "    if synset.wordnet_offsets:\n",
    "        wt = ''\n",
    "        trans = set()\n",
    "        for sense in synset:\n",
    "            if 'WIKI' in str(sense) and 'EN' in str(sense):\n",
    "                wt = str(sense.full_lemma)\n",
    "            trans.add(str(sense.full_lemma))\n",
    "        langs = [str(l) for l in synset.languages]\n",
    "        offset = [str(l).split(':')[1] for l in synset.wordnet_offsets if 'wn' in str(l)][0]\n",
    "        gloss = str(synset.main_gloss())\n",
    "        dat.append(offset)\n",
    "        dat.append(trans)\n",
    "        dat.append(gloss)\n",
    "        dat.append(wt)\n",
    "        dat.append(langs)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9776a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = dict()\n",
    "retrieved = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "380d6c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:14:28.776283Z",
     "start_time": "2023-02-05T09:14:28.772594Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = list(vocab) #testset vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7868fe43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T13:59:38.472165Z",
     "start_time": "2023-01-27T10:50:21.993333Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1807/1807 [3:09:16<00:00,  6.28s/it]\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm(vocab[20:],position=0,leave=True):\n",
    "    synsets_a = bn.get_synsets(word, from_langs=[Language.EN,Language.FR,Language.IT])\n",
    "    synsets_b = bn.get_synsets(word, from_langs=[Language.DE,Language.FA,Language.ES])\n",
    "    synsets_c = bn.get_synsets(word, from_langs=[Language.PT,Language.EU,Language.RU])\n",
    "    synsets = synsets_a + synsets_b + synsets_c\n",
    "    for synset in synsets:\n",
    "        dat = get_syn_data(synset)\n",
    "        if dat:\n",
    "            if dat[0] in data:\n",
    "                data[dat[0]].append(dat[1:])\n",
    "            else:\n",
    "                data[dat[0]] = [dat[1:]]\n",
    "    retrieved.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88106892",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = load('syndata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b556197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T03:59:58.677754Z",
     "start_time": "2023-01-29T03:59:58.024028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "syn_dat = dict()\n",
    "for k,v in data.items():\n",
    "    words = set()\n",
    "    gloss = set()\n",
    "    title = set()\n",
    "    langs = set()\n",
    "    for dat in v:\n",
    "        words.update(dat[0])\n",
    "        gloss.update({dat[1]})\n",
    "        title.update({dat[2]})\n",
    "        langs.update(set(dat[3]))\n",
    "    syn_dat[k]=[words,gloss,title,langs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9564e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T04:01:04.338319Z",
     "start_time": "2023-01-29T04:01:04.333643Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'avaliar',\n",
       "  'balioztatu',\n",
       "  'baloratu',\n",
       "  'cotar',\n",
       "  'cotizar',\n",
       "  'fissare_il_prezzo_di',\n",
       "  'fixer_le_prix',\n",
       "  'prezzare',\n",
       "  'price',\n",
       "  'valorar',\n",
       "  'valorizar',\n",
       "  'قیمت_گذاشتن'},\n",
       " {'Determine the price of'},\n",
       " {''},\n",
       " {'EN', 'ES', 'EU', 'FA', 'FR', 'IT', 'PT'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_dat['02351010v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b82e567d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T04:04:43.652055Z",
     "start_time": "2023-01-29T04:04:43.321220Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the data\n"
     ]
    }
   ],
   "source": [
    "sv.save(syn_dat,'syn_dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e87bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T11:06:57.102244Z",
     "start_time": "2023-01-24T11:04:59.849618Z"
    }
   },
   "source": [
    "# filter sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc650d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T08:31:22.435644Z",
     "start_time": "2023-01-29T08:31:22.432038Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "6340c085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:50:17.727359Z",
     "start_time": "2023-02-08T09:50:17.724312Z"
    }
   },
   "outputs": [],
   "source": [
    "#text =\"learn php from guru99 and make study easy\".split()\n",
    "#text =\"to be star\".split()\n",
    "#text =\"black , wide or narrow\".split()\n",
    "text = \"relating to a recently developed fashion or style\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "b8727085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:50:24.502452Z",
     "start_time": "2023-02-08T09:50:24.498814Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "WORD_TAGS = ['to', 'be', 'relate', 'relating', 'related','or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "90a13c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:43:07.575351Z",
     "start_time": "2023-02-08T09:43:07.572147Z"
    }
   },
   "outputs": [],
   "source": [
    "#chunk_a: {<TO> <VB> <NN+>}\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  nn: {<TO> <BE> <NN>+}        \n",
    "  nn: {<NN><CC><NN>}\n",
    "  nn: {<DT>+<.*>*<NN>+}\n",
    "  nn: {(<RELATE>|<RELATING>|<RELATED>) <TO> <.*>* <NN>}\n",
    "  vb: {<TO> <BE> <VB>}\n",
    "  vb: {^<VB>}\n",
    "  vb: {<VB> ((<CC>|,) <VB>)+}\n",
    "  jj: {<JJ>}\n",
    "  jj: {<JJ> <CC> <JJ>}\n",
    "  jj: {<JJ><,><JJ><OR><JJ>}\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c637c9fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:43:07.801450Z",
     "start_time": "2023-02-08T09:43:07.796906Z"
    }
   },
   "outputs": [],
   "source": [
    "chunker = RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "2b5bbdfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T10:06:22.307356Z",
     "start_time": "2023-02-08T10:06:22.300555Z"
    }
   },
   "outputs": [],
   "source": [
    "def gloss_clean(words):\n",
    "    tokens_tag = pos_tag(words)\n",
    "    pos_tags = [(w, w.upper()) if w in WORD_TAGS else (w, t) for w, t in tokens_tag]\n",
    "    parsed = chunker.parse(pos_tags)\n",
    "    out = list()\n",
    "    for tree in parsed:\n",
    "        if type(tree) is tuple:\n",
    "            continue\n",
    "        if tree.label()=='nn':\n",
    "            for word in tree:\n",
    "                if word[-1]=='NN':\n",
    "                    out.append(word[0])\n",
    "        if tree.label()=='vb':\n",
    "            for word in tree:\n",
    "                if word[-1]=='VB':\n",
    "                    out.append(word[0])\n",
    "        if tree.label()=='jj':\n",
    "            for word in tree:\n",
    "                if word[-1]=='JJ':\n",
    "                    out.append(word[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7699b917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-31T06:49:43.330421Z",
     "start_time": "2023-01-31T06:49:43.326668Z"
    }
   },
   "outputs": [],
   "source": [
    "def wt_clean(word):\n",
    "    word = word.replace('(','')\n",
    "    word = word.replace(')','')\n",
    "    word = word.replace(',','')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa610d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "6c01188e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-31T08:51:07.559460Z",
     "start_time": "2023-01-31T07:18:44.873776Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎         | 12632/13487 [1:17:04<22:34,  1.58s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmates_list = list() #counter\n",
    "lemma_bias = dict()\n",
    "for _,v in tqdm(syn_dat.items()):\n",
    "    lemmas = v[0]\n",
    "    gloss = wt_clean(list(v[1])[0].lower())\n",
    "    wt = wt_clean(list(v[2])[0].lower())\n",
    "    for lemma in lemmas:\n",
    "        lemmas_list.append(lemma)\n",
    "        lemma_bias[lemma+'#'+str(lemmas_list.count(lemma))]=[lemmas,gloss,wt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc4f5b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T07:00:01.699954Z",
     "start_time": "2023-02-01T07:00:01.696707Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "98592fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T08:07:17.980942Z",
     "start_time": "2023-02-01T08:07:02.014616Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 08:07:17,971 [gensim.summarization.textcleaner] INFO: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e9fd2b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T07:28:00.564224Z",
     "start_time": "2023-02-01T07:28:00.561014Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = '/home/manni/embs/numberbatch-19.08.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ab7b205b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T08:46:36.630071Z",
     "start_time": "2023-02-01T08:09:44.703409Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 08:09:44,704 [gensim.models.utils_any2vec] INFO: loading projection weights from /home/manni/embs/numberbatch-19.08.txt.gz\n",
      "2023-02-01 08:46:36,624 [gensim.models.utils_any2vec] INFO: loaded (9161912, 300) matrix from /home/manni/embs/numberbatch-19.08.txt.gz\n"
     ]
    }
   ],
   "source": [
    "vecs = KeyedVectors.load_word2vec_format(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2ec12",
   "metadata": {},
   "source": [
    "# embeddings computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c7d45f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:01:54.133333Z",
     "start_time": "2023-02-05T09:01:13.212592Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_map = dict()\n",
    "for word in vecs.vocab:\n",
    "    _word = word.split('/')[-1]\n",
    "    if _word in filtered_map:\n",
    "        filtered_map[_word].append(word)\n",
    "    else:\n",
    "        filtered_map[_word]=[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "90a23278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:18:51.093775Z",
     "start_time": "2023-02-05T09:18:51.002109Z"
    }
   },
   "outputs": [],
   "source": [
    "l_vocab = set(lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "621d4823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T09:18:51.446851Z",
     "start_time": "2023-02-05T09:18:51.307159Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand l_vocab with parts of WT\n",
    "for v in lemma_bias.values():\n",
    "    if v[2]:\n",
    "        continue\n",
    "    wt = v[2].replace(' ','_')\n",
    "    l_vocab.add(wt)\n",
    "    for word in v[2].split():\n",
    "        l_vocab.add(word)\n",
    "    words = v[2].split()\n",
    "    for i in range(len(words)-1):\n",
    "        word = '_'.join(word[i+1:])\n",
    "        l_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "43801426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T10:27:20.159596Z",
     "start_time": "2023-02-08T10:10:02.146955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 463308/463308 [17:17<00:00, 446.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# get vectors \n",
    "vectors = dict()\n",
    "for k,v in tqdm(lemma_bias.items()):\n",
    "    unfound = True\n",
    "    words = list() # for WT processing (else case.)\n",
    "    vector = np.zeros(vecs.vector_size)\n",
    "    for word in v[0]: # lemmas\n",
    "        if word in filtered_map:\n",
    "            for _word in filtered_map[word]:\n",
    "                vector = np.add(vecs.get_vector(_word),vector)\n",
    "    filtered_gloss = gloss_clean(v[1].split()) #grammar rules applied\n",
    "    for word in filtered_gloss:\n",
    "        if word in filtered_map:\n",
    "            for _word in filtered_map[word]:\n",
    "                vector = np.add(vecs.get_vector(_word),vector)\n",
    "    if not wt:\n",
    "        vectors[k]=vector\n",
    "        continue\n",
    "    wt = v[2].replace(' ','_')\n",
    "    if wt in filtered_map:\n",
    "        for _word in filtered_map[word]:\n",
    "                vector = np.add(vecs.get_vector(_word),vector)\n",
    "        vectors[k]=vector\n",
    "        continue\n",
    "    else:\n",
    "        words = v[2].split()\n",
    "        for i in range(len(words)-1):\n",
    "            word = '_'.join(word[i+1:])\n",
    "            if word in filtered_map:\n",
    "                for _word in filtered_map[word]:\n",
    "                    vector = np.add(vecs.get_vector(_word),vector)\n",
    "                vectors[k]=vector\n",
    "                unfound = False \n",
    "                break\n",
    "    if unfound:\n",
    "        word = words[-1]\n",
    "        if word in filtered_map:\n",
    "            for _word in filtered_map[word]:\n",
    "                vector = np.add(vecs.get_vector(_word),vector)\n",
    "            vectors[k]=vector        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "21dd6ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T10:31:31.086455Z",
     "start_time": "2023-02-08T10:27:20.162961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 463308/463308 [04:10<00:00, 1847.49it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_file = '/home/manni/embs/llx.txt.gz'\n",
    "with gzip.open(emb_file, 'wt', encoding='utf-8') as f:\n",
    "    f.write('%d %d\\n' % (len(vectors), 300))\n",
    "    for word,vector in tqdm(vectors.items(), position=0):\n",
    "        f.write('%s %s\\n' % (word, ' '.join([str(v) for v in vector])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a0e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
